https://docs.google.com/spreadsheets/d/1I6hcZlYZqGIQEsnKMoXMVsccITrSsbFdnLyJyAk55-Q/edit?pli=1#gid=119576526

https://github.com/joanby/tensorflow2

Machine Learning de la A a la Z (LIBRO)
https://joanby.github.io/bookdown-mlaz/clasificaci%C3%B3n.html#regresi%C3%B3n-log%C3%ADstica

Machine Learning A-Z: Download Codes and Datasets
https://www.superdatascience.com/pages/machine-learning

Repositorio del Curso Machine Learning de A a la Z: R y Python para Data Science
https://github.com/joanby/machinelearning-az

https://discord.com/invite/Gq5NX6a	

Bienvenido a la Parte 2: Regresión
Los modelos de regresión (tanto lineal como no lineal) se utilizan muchísimo para predecir valores numéricos como por ejemplo el sueldo. Si nuestra variable independiente es tiempo entonces podemos hacer predicciones de valores futuros, sin embargo nuestro modelo puede predecir también valores desconocidos del presente.  Las técnicas de Regresión son muy variadas, desde la regresión lineal hasta la SVR o la Regresión con Bosques Aleatorios.

En esta parte, vamos a entender e implementar los siguientes modelos de Regresión dentro del Machine Learning:

Simple Linear Regression

Multiple Linear Regression

Polynomial Regression

Support Vector for Regression (SVR)

Decision Tree Classification

Random Forest Classification

-----------
Regression
-----------

Modelos de regresion lineal

Modelos de regresion lineal tienen una series de restricciones que se deben comprobar antes de aplicar el modelo de regresion:
	-linealidad
	-homocedasticidad
	-Normalidad multivariable
	-Independencia de errores
	-ausencia de multicolinealidad
	
Si no se da alguna de las restricciones anteriores el modelo de regresion lineal no tiene sentido.

-	Simple Linear Regression (SLR)

	En el grafico las variables independientes van en el eje de las x y la dependiente en el eje de las Y.

	y = b0 + b1 * x1
		b0 constante (ordenada all origen)
		b1 coficiente
		x1 variable independiente
		y variable dependiente (a predecir)
		
		Para identificar la recta se usa el "metodo de los minimos cuadrados"
						  ^   2
			min	SUM (yi - yi)
				   i
		(SLR) no requiere scalado
		
		La recta de regresion es la misma para los datos entrenados como los datos de test. Se deja el conjunto de entrenamiento.
		

- Multiple Linear Regression	

		(MLR) no requiere scalado
		
		y = b0 + b1 * x1 + b2 * x2 + ... + bn *xn
			b0 constante (ordenada all origen)
			bi coficientes
			xi variables independientes
			y variable dependiente (a predecir)
			
			notar:
				bi coficientes positivos(+) aportan valor a la y 
				bi coficientes negativos(-) restan valor a la y 
			
			Dummy: propiedad  categorica (no ordinal). Ejemplo: Ciudades. 
				
			Es contra producente agregar todas las columnas dummy. Se deben añadir todas las columnas dummy salvo una. Evita el efecto de multicolinealidad.
			
			y = b0 + b1 * x1 + b2 * x2 +  b4 * d1
				d: variable dummy
			
			Trampa de variables dummy:
			
				Al incluir todas las variables dummy obtenemos una multicolinealidad. Esto debe evitarse. El modelo es incapaz de determinar los efectos de todas las variables dummy
					y = b0 + b1 * x1 + b2 * x2 +  b4 * d1 + b5 * d2
							El modelo no puede determinar los efectos o impacto de d1 y de d2.
							d2 = 1 - d1
				
				SIEMPRE DEBEMOS OMITIR UNA VARIABLE Dummy. Esto evita el efecto de multicolinealidad.
				
				Si tuvieramos dos variable dummy, deberia quitar un valor para cada variable dummy. Por ejemplo, pais y sector. Quito un valor (de todos los valores posibles) para pais y uno (de todos los valores posibles) para sector.	
				
				https://www.wikihow.com/Calculate-P-Value
				https://www.mathbootcamps.com/what-is-a-p-value/
				
				Entre varios modelos con variables independientes, preferimos el que tenga la menos cantidad de variables independientes que ayuden a predecir la variable dependiente.
				 Porque?
					1- Puede ser que añadir mas variables no aporte a predecir la variable dependiente.
					2- Mayor variables independientes hace que complejo el modelo. (dificulta la explicacion del modelo)
					
					SOLO MANTENER LAS VARIABLES INDEPENDIENTES IMPORTANTES CON CAPACIDADES DE PREDECIR EL VALOR DE LA VARIABLE DEPENDIENTE. SE debe seleccionar las variables importantes capaces de predecir algo:
						5 Modelos de seleccion:
						https://www.youtube.com/watch?v=tCXc2zl3dew
							1- Exhaustivo (all-in).
							2- Eliminacion hacia atras.   -------------
							3- Seleccion hacia adelante.			   --------	Regresion paso a paso los incluye 
							4- Eliminacion bidireccional.  ------------
							5- Comparacion de scores.
												
							1- Exhaustivo (all-in)
				
								La usamos por:
								
								1. Metemos todas la variables y vemos que sucede, ya que conocemos (por el conocimiento del negocio o por necesidad) que todas son predictoras.
								
								o
								
								2. Preparacion previa a la Eliminacion hacia atras. 	
									
			
							2- Eliminacion hacia atras. (Backward)
								Elimina cualquier variable independiente que no sea significativa.
									Paso 1. 
									Elegimos un nivel de significacion para que una variable permanezca en el modelo. (Standard 0.05)
									
									Paso 2.
										Se calcula el modelo usando todas las variables como predictoras.
										
									Paso 3.
										Se considera la variable predictora con el p-valor mas grande. Si p-valor > Standard vamos al paso 4 sino a fin.
										
									Paso 4.
										Se elimina la variable con p-valor > Standard
									
									Paso 5.
										Se ajusta el modelo sin la variable con p-valor > Standard
									
									FIN - El modelo esta listo!
									
									Repetimos del paso 3 al paso 5 hasta no tener variables con p-valor > Standard. Nos quedamos con ese modelo.
							
									WARNING: Para aplicar esta seleccion en Python, se agrega una columna de todos unos a las columnas de variables independientes. Esta representa la ordenada al origen. PAra analizar si tiene significado la ordenada al origen en la funcion a calcular.
							
							3- Seleccion hacia adelante. (Forward)						
									Paso 1. 
									Elegimos un nivel de significacion para que una variable pueda entrar en el modelo. (Standard 0.05)
									
									Paso 2.
										Ajustamos todos los modelos de regresion lineal simple y ~ Xn (hacemos todos los modelos de regresion linial simple para cada variable independiente con la dependiente). Elegimos el que tiene menor p-valor.
									
									Paso 3.
										Se conserva esta variable, y ajustamos todos los posibles modelos con una variable extra añadida a la(s) que ya tenga(s) el modelo hasta el momento.
										
									Paso 4.
										Se considera la variable predictora con el menor p-valor. Si p-valor < Standard vamos al paso 3 sino a fin.
										
									FIN	- Conservar  el modelo anterior.
									
									Repetimos el paso 3 hasta  que la variable añadida supere el p-valor.  Nos quedamos con el modelo anterior.
									
							4- Eliminacion bidireccional (stepwise)
							https://www.youtube.com/watch?v=6wpTEwaFbY0
									Combina: Seleccion hacia adelante y Eliminacion hacia atras.
									
									Paso 1. 
										- Elegimos un nivel de significacion para que una variable pueda entrar en el modelo. (Standard 0.05) SLENTER
										- Elegimos un nivel de significacion para que una variable permanezca en el modelo. (Standard 0.05) SLSTAY
										
									Paso 2.
										Inicia como forward.
										Llevar a cabo el Paso de selccion hacia adelante (con las nuevas variables con p < SLENTER para entrar)
										
									Paso 3. 
										Se lleva adelante la eliminacion hacia atras (variables antiguas con p-valor < SLSTAY para quedarse)
										
										Repetir de paso 2 hasta 3 que no queden variables para añadir.
										
									El mejor de los tres modelos.
									
									FIN- Modelo esta listo.
									
							5- Fuerza bruta: (Comparacion de scores.)
							
									Genera todos los modelos de regresion posibles. Todos los de una variable, Todos los de dos variables, Todos los de test variables, etc.
									
									Paso 1.
									Seleccionar un criterio de bondad de ajuste. Ej: Vallesiano, Akaike,etc.
									
									Paso 2.
									Construir todos los modelos de regresion posibles. (2 elevado a la N) - 1. N numero de variables. Por ejemplo: Con 10 variables -> (2 elevado a 10) - 1 = 1023 modelos.
									
									Paso 3.
									Seleccionamos el modelo basado el criterio elegido. El mejor modelo.
									
									FIN- Modelo esta listo.
									
									
							Conclusion:
									No hay un modelo mejor que otro. Hay que conocer todos los metodos.
									
									Si cualquiera de los coeficientes es cercano a cero no es necesario agregarlo a la formula de regresion.
									 OLS-> ordinary list square (minimo de cuadrados ordinarios)
								
									Cuanto mas cercano es el R-Cuadrado Ajustado al 1 (uno) mejor se explica el modelo de la regresion lineal (Es el mejor modelo)
									R-Cuadrado Ajustado > 0.7 se suele aceptar qu el modelo es lineal.

									WARNING: No Dividir el conjunto de datos entre entrenamiento y testing cuando son pocos casos o datos.
									
Regresión Lineal Múltiple en Python - Eliminación hacia atrás automática

Si estás interesado en implementaciones automáticas de la Eliminación hacia atrás en Python, aquí te presentamos dos de ellas. Se ha adaptado el código para que utilice la transformación .tolist() sobre el ndarray y así se adapte a Python 3.7.

Eliminación hacia atrás utilizando solamente p-valores:

import statsmodels.formula.api as sm
def backwardElimination(x, sl):    
    numVars = len(x[0])    
    for i in range(0, numVars):        
        regressor_OLS = sm.OLS(y, x.tolist()).fit()        
        maxVar = max(regressor_OLS.pvalues).astype(float)        
        if maxVar > sl:            
            for j in range(0, numVars - i):                
                if (regressor_OLS.pvalues[j].astype(float) == maxVar):                    
                    x = np.delete(x, j, 1)    
    regressor_OLS.summary()    
    return x 
 
SL = 0.05
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_Modeled = backwardElimination(X_opt, SL)

Eliminación hacia atrás utilizando  p-valores y el valor de  R Cuadrado Ajustado:

import statsmodels.formula.api as sm
def backwardElimination(x, SL):    
    numVars = len(x[0])    
    temp = np.zeros((50,6)).astype(int)    
    for i in range(0, numVars):        
        regressor_OLS = sm.OLS(y, x.tolist()).fit()        
        maxVar = max(regressor_OLS.pvalues).astype(float)        
        adjR_before = regressor_OLS.rsquared_adj.astype(float)        
        if maxVar > SL:            
            for j in range(0, numVars - i):                
                if (regressor_OLS.pvalues[j].astype(float) == maxVar):                    
                    temp[:,j] = x[:, j]                    
                    x = np.delete(x, j, 1)                    
                    tmp_regressor = sm.OLS(y, x.tolist()).fit()                    
                    adjR_after = tmp_regressor.rsquared_adj.astype(float)                    
                    if (adjR_before >= adjR_after):                        
                        x_rollback = np.hstack((x, temp[:,[0,j]]))                        
                        x_rollback = np.delete(x_rollback, j, 1)     
                        print (regressor_OLS.summary())                        
                        return x_rollback                    
                    else:                        
                        continue    
    regressor_OLS.summary()    
    return x 
 
SL = 0.05
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_Modeled = backwardElimination(X_opt, SL)


Instalar la nueva versión de ElemStatsLearn en R
Los que tengáis problemas al instalar ElemStatLearn desde R probad con esta línea de código

install.packages("https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/ElemStatLearn_2015.6.26.2.tar.gz",repos=NULL, type="source")
									
						
Regresión Lineal Múltiple en R - Eliminación hacia atrás automática
Si quieres tener una implementación automática de la eliminación hacia atrás en R, aquí te la dejo:

backwardElimination <- function(x, sl) {
  numVars = length(x)
  for (i in c(1:numVars)){
    regressor = lm(formula = Profit ~ ., data = x)
    maxVar = max(coef(summary(regressor))[c(2:numVars), "Pr(>|t|)"])
    if (maxVar > sl){
      j = which(coef(summary(regressor))[c(2:numVars), "Pr(>|t|)"] == maxVar)
      x = x[, -j]
    }
    numVars = numVars - 1
  }
  return(summary(regressor))
}
 
SL = 0.05
dataset = dataset[, c(1,2,3,4,5)]
backwardElimination(training_set, SL)


- Regresion Polinomica (RP)

	Esta regresion tiene una unica variable independiente.-

	Regresion lineal -> 	y = b0 + b1X1
	Regresion multiple -> 	y = b0 + b1X1 + b2X2 + ... + bnXn
	Regresion polinomica -> y = b0 + b1X1 + b2X1^2 + ... + bnX1^n (tenes una misma variable (x1) con potencias sucesivas) 
	
	Cuando usar:
	
	Cuando los datos relevados siguen una parabola o una curva (no una recta).
	
	Regresion polinomica de grado 2 -> y = b0 + b1X1 + b2X1^2
	
	En la RP debemos elegir hasta que grado queremos. Grado 2 o Grado 3 o ... o Grado n. El grado tiene que ver con el exponente o potencia.
	
	¿Porque se llama Regresion lineal Polinomica?
		Es lineal por los coeficientes. Los coeficientes tienen una relacion lineal con las variables. 
		En la regresion lineal las incognitas son los coeficientes.
		La variable y se predise como combinacion lineal de esos coeficientes con las variables.
		

	WARNING: No se escalan los datos porque la relacion no lineal podria perderse.
	
	poly_reg = PolynomialFeatures(degree = 4) - degree representa grado del polinomio . Hasta que grado deseo tener las caracteristicas polimonialies de mi matriz original. Se elige el deseado.
	
	# Training the Linear Regression model on the whole dataset
	from sklearn.linear_model import LinearRegression
	lin_reg = LinearRegression()
	lin_reg.fit(X, y)

	# Training the Polynomial Regression model on the whole dataset
	from sklearn.preprocessing import PolynomialFeatures
	poly_reg = PolynomialFeatures(degree = 2)
	X_poly = poly_reg.fit_transform(X) 
	lin_reg_2 = LinearRegression()
	lin_reg_2.fit(X_poly, y)    
	
	El codigo anterior genera una matriz llamada X_poly con tres columnas. La primera el termino independiente (ordenada al origen - todos unos), la segunda columna el valor de la variable independiente y la tercer columna el cuadrado de la variable independiente.
	
	WARNING: En un modelo polinomial nunca se representa una recta.- Es una Parabola o curva. 
	
	
-	Regresion con Maquinas de Soporte Vectorial (SVR)

	Estos métodos están propiamente relacionados con problemas de clasificación y regresión. 

	Sirven para Regresiones lineales (Kernel lineal )como no lineales (rbf, etc...). Dependen de un nucleo (Kernel). El Kernel determina el tipo de regresion que lleva a cabo la maquina de soporte vectorial.
		
	Para regresion hay que ajustar el mayor corredor (o calle) posible entre dos clases. Intentando que en el interior de ese pasillo o clase queden la mayoria de los puntos. Hay que limitar la anchura (margen maximo). 
	Imaginemos una recta o calle entorno a la misma dos pasillos (uno por arriba y otro por debajo) donde queden la mayoria de los puntos. 
	
	En el caso de clasificacion se intenta que la anchura del pasillo separe las dos clases lo mas posible. En la Regresion encontrar la anchura que incluya la mayoria de los puntos. 

	Como controlamos la anchura del pasillo. Se establece un parametro llamado epsilon. A mayor valor de epsilon mayor anchura.	
	
	epsilon: Distancia entre las lineas de soporte vectorial y la linea de hiperplano. 
	
	La calle se llama hiperplano. Hay que obtener la ecuacion del hiperplano. Los puntos que hacen cero a la ecuacion estan en el hiperplano. Si el son distintos de cero partenecen a una clase o a otra. Para que funcione hay que encontrar la ecuacion que represente al mejor hiperplano (aquel que permite que no haya sesgo entre una u otra categoria). Es decir que permita realizar la mejor separacion. Las maquinas de soporte vectorial ayudan a encontrar el mejor hiperplano.
	
	El algoritmo detecta los puntos mas cercanos entre una clase y otra. Luego, encuentra la linea que los conecta. Finalmente, traza una perpendicular que divide esta linea en dos. Esta linea es el hiperplano optimo. Se busca maximizar el margen. 
	
	Vocabularion:
	
		Hiperplano = recta de regresion.
		
		Los vectores de soporte: los puntos mas cercanos entre una clase y otra. 
		Margen: distancia entre los vectores de soporte.
		Hiperplano optimo: frontera de separacion que consige la mayor separacion entre una clase y otra. 
		
		Problema:
			Si aparece un dato excepcional o valor a tipico (outlier) que lleva a reducir el margen y por lo tanto a producir un overfitting (clasificacion incorrecta de un dato nuevo por el algoritmo). Para solucionar este problema al alogritmo tradicional (Hard margin) se le agrega un parametro C (Soft margin) para flexibilizar el margen. Este valor de C es elegido por el diseñador en el entrenamiento. A menor C  mayor margen o a mayor C menor margen. El valor se escoge de manera emperica analizando el error obtenido en la clasificacion comparado con diferentes valores de C. Se escoge el menor error posible. Es decir, menor C.
			
	La idea de las Maquinas de Soporte Vectorial es obtener el mayor margen posible entre una clase y otra.
	
	Problema: 
		Maquinas de Soporte Vectorial (SVR) permiten obtener fronteras de clasificacion o hiperplanos lineales. A veces es necesario en aplicacion reales obtener hiperplanos o fronteras no lineales. Como solucion una alternativa es agregar mas dimensiones a cada dato para separar las dos categorias. Es decir, usamos Maquinas de Soporte Vectorial para obtener hiperplanos en mas dimensiones. Como agregamos mas dimensiones a los datos para poder clasificarlos: Kernel.
		
		Podemos pensar que cada punto de datos de entrenamiento representara su propia dimension. 
		
		Kernel: Toma el dataset de datos original y lo mapea a un espacio de mayor dimensiones usando una funcion no lineal. Con esta tranformacion el dataset es linealmente separable. Luego, se aplica Maquinas de Soporte Vectorial (SVR) para obtener el hiperplano optimo. Luego, obtenido el hiperplano optimo se vuelve al espacio original y finalmente, se realiza la clasificacion. Se puede representar la funcion en el espacio original.
		
		Kernel: Mapea y realiza el calculo del hiperplano. Para el mapeo usa funciones polinomiales o de gaussianas.
		
		Las Maquinas de Soporte Vectorial sirven para aumentar la dimensionalidad del problema y calcular la regresion en ese espacio de dimension superior. Luego ese funcion se puede volver al espacio de dimension original y proyectar la linea. 
		
		Objetivo de las Maquinas de Soporte Vectorial es encontrar una funcion de regresion a partir de los puntos de entrenamientos y que los errores no superen el umbral establecido (el epsilon)
		
	Algoritmo SVR:
	
		Analizar si se escalan los datos de X (independientes) y Y (dependiente)
	
		1. Elegir el conjunto de entrenamiento. (variables independiente y dependiente)
		2. Elegir la funcion de nucleo y sus parametros. Tambien, analizar si hay que realizar una regulacion adicional que elimine el ruido en el conjunto de entrenamiento. 
		3. Crear la matriz de correlaciones (K).  
		4. Entrenamos el modelo en forma exacta o aproximada para obtener los coeficientes de contraccion para cada uno de los datos.
		5. Con los coeficientes de contraccion creamos un estimador.	
		
		Se debe elegir el nucleo:
				- Lineal
				- No lineal - (Gaussiano - Es el defecto) Es el la constante rbf en python.
				
		Regulacion para eliminar el ruido de los datos.
		
		IMPORTANTE: Considerar escalar ya que el algoritmo se basa en distancias euclideas.
			
- Regresion con Arboles de desicion

		Los saltos en los arboles son discretos. Hay saltos en el diagrama del arbol.
		
		Puede tener varias variables independientes.
		
		En los arboles de desicon no suele ser necesario ningun tipo de escalado.
		
		Escalar si el algoritmo usa distancias euclideas. EL ARbol de desicion no utiliza distancias euclideas.
		
		
		CART
			Classification & Regression Trees
				Hay dos Tipos:
					---> Arboles de Clasificacion
					---> Arboles de regresion para predecir valor.
					
		Arboles de Regresion:
				 Dos variables independientes x1 y x2. Objetivo predecir una tercer variable dependiente Y.
				 
				 Predice no necesariamente de forma lineal. 
				 
				 El algoritmo divide los puntos (nuestros datos) en un conjunto de secciones. 	
				 El algoritmo mira la entropia. Como de juntos o dispersos estan los puntos (similitudes entre los puntos)-
				 Agrupa los puntos en comunes basado la entropia. Hay que establecer los puntos que se quedan en un nodo hoja (Ejemplo 5% de datos).
				 Se puede usar otra regla para definir los puntos que quedan en la hoja.
				 El algoritmo encuentra las divisiones optimas del conjunto de datos.
					
				 Alg ejemplo:
				 
					1. desicion X1 < 20
							Si -> X2 < 200
									Si -> 
									No -> 
							No -> X2 < 170
									Si -> X1 < 40 
											Si -> La proyeccion o prediccion es el promedio de los puntos que conforman la hoja o seccion de division. Para un mismo nodo hoja retorna la misma prediccion.
											No -> 
									No -> 
		Python:
				Objeto en python para representar un arbol de desicion es DecisionTreeRegressor.
				Cuando se crea se establece el criterios de division de los datos. Se suele utilizar la medida del "error cuadrado medio" como forma de minimizar la diferencia entre la prediccion y el resultado. Este criterio busca cual de las formas de cortar minimiza los cuadrados de los errores. Este es el criterio por defecto.
				
				regressor = DecisionTreeRegressor(random_state = 0)
				
				Enfoques mas avanzados suelen definir enfoques que corten por mas de un rasgo a la vez. En el constructor del objeto DecisionTreeRegressor se definen las caracteristicas o parametros del arbol de desicion. Tambien se puede definir el numero maximo de nodos hojas, numeros maximos de elementos que forman el nodo hoja, etc. Elige que parametros pasar al objeto.
				
				
	WARNING: hay que escalar o no? Por defecto no escalar y ver que buena es la prediccion y luego, ver como es la prediccion con los datos escalados. Escalar si el algoritmo usa distancias euclideas. No se usa en Arboles de desicion. 
	
	Cuando tenes una linea horizontal (en grafico) que abarca todo el conjunto de datos podria pasar que los datos estan mal interpretados. Puede ser que el arbol tuviera unas restricciones a la hora de dividir una rama en nodos hojas. Es decir, que una rama no se divide en nodos hojas si no hay suficientes datos para formar parte del nodo hoja. Tambien, puede ocurrir que no se dividen las ramas a menos que se establezcan ciertas condiciones o las condiciones de division estan mal establecidas. Por lo tanto, se agrupan muchos datos en una sola hoja y se otorga el promedio de todos los datos de la hoja a las proyecciones que caen en ese nodo hoja. Con parametros se puede cambier los criterios de division de ramas. En R npar.control(minsplit=1) Establece que una rama puede tener un dato para en su hoja. 

	 		
- Regresion con Bosques aleatorios

	Se usan para regresion y para clasificacion.
	
	los saltos en los arboles son discretos.
	
	Los arboles cuando hacen la prediccion lo hacen sobre un conjunto discreto de datos.

	Este algoritmo entra en Aprendizaje en Conjunto. Basicamente, toma un algoritmo y lo replica n veces o toma varios algoritmos diferentes y los junta para obtener un mejor algotimo.  Aprendizaje en Conjunto permite tener algoritmos mas estables.
	
	Muchos arboles de desicion conforman un bosque aleatorio.
	
	En este caso toma un algorimto de arboles de desicion y los replica n veces.
	
	Pasos:
	
	Paso 1. Se elige un numero K (numero aleatorio). Representa un subconjunto de los datos del conjunto de datos de entrenamiento. En lugar de dividir los datos en conjunto de entrenamiento y conjunto de test. Elegimos un subconjunto de datos de los datos de entrenamiento.
	
	Paso 2. Construir un arbol de desicion para los K puntos de datos. Lo que hace que el arbol tenga una vision parcial de los datos.
	
	Paso 3. Se define el numero de arboles de desicion y se repite el paso 1 y 2. Cada nuevo arbol tiene un subconjunto de datos del total de datos de entrenamiento.
	
	Paso 4. Cuando llegamos a construir el numero de arboles deseados. Se puede usar todo el bosque para hacer la prediccion. A cada arbol se le indica que realice la prediccion de Y. Finalmente, la prediccion final es el promedio de las predicciones Y dadas por los arboles construidos.
	
	Este algoritmo mejor la prediccion de un solo arbol de desicion. 
	
	Cuando hay valores outlier (valores extremos) podemos tomar la mediana en lugar de la media en cada arbol. Tambien se puede usar la media recortada.
		
	Python:
		RandomForestRegressor(n_estimators = 10, random_state = 0)	
			Parametros de ForestRegressor
					n_estimators -> cantidad de arboles del bosque.
					criterion -> criterio para dividir una rama en dos subramas o nodos hojas.
								Criterio de division: MSE (error cuadrado medio) diferencia entre la prediccion y el valor real al cuadrado. Se intenta minimizar. Se divide la rama en dos subramas intentando que el MSE de cada rama sea menor al MSE de la rama que se divide. Tambien, hay otros criterios de division.
					max_features -> caracteristicas tenidas en cuenta a la hora de generar las divisiones.
					random_state -> reproductivilidad para obtener el mismo bosque.
					
			
	
	 IMPORTANTE: Cuando se conbinan diferentes modelos o algoritmo (Aprendizaje en Conjunto) en varias maquinas sale a relucir mejores predicciones.
	 

Generalidades:

	 R Cuadrado:
	 
		Buscamos el modelo que predise el menor error posible con respecto a los datos dados.
		
		R 2 = 1- SSE/SST 
		
		SSE es la suma de cuadrados de error o residuos - Se calcula sumando las diferencias al cuadrado entre los valores predichos y los valores reales de la variable dependiente.
						
		SST es la suma de cuadrados total - Es la suma de los cuadrados de las diferencias entre cada punto de datos (valores reales de la variable dependiente-valores observados) y la media de todo el conjunto de datos
		
		Cuanto mejor sea mi modelo mas cerca estara R Cuadrado del numero uno. Mide que tan bueno es el modelo que elijo con respecto a una prediccion de media para todos los valores del conjunto de datos.
		
		Si el R Cuadrado es negativo el resultado es un pesimo modelo de prediccion.
		
		Los algoritmos o tecnicas de regresion se enfocan en mejorar el R Cuadrado para indicar lo bueno que es mi modelo para predecir los datos.
		
		En R se llama "multiple R-squared"
		
		El valor mas grande que puede tomar es uno. 
		
		R cuadrado no es un estimador muy exacto de la bondad de ajuste para algoritmos de variables multiples. Siempre sera mejor o mayor cuanta mas variables agreguemos. Necesitamos usar un valor u otro indicador que no este sesgado. Usamos R Cuadrado Ajustado. (regresion lineal multiple)
	
	 R Cuadrado Ajustado:
	
		Regresion Lineal multiple: El R Cuadrado se sesga y no sirve para medir o evaluar modelos con numero de variables diferentes. Es decir, si agregar una variable mas al modelo lo mejora y en consecuencia, mejora la prediccion. (R2 nunca decrece, se incrementa). En ese caso hay que usar el R Cuadrado Ajustado para evaluar si agregar una variable mejora nuestro modelo.
		
		Adj R 2  = 1 - (1 - R2) * (n - 1) / (n - p - 1)
		
		R2 es el R cuadrado
		n tamaño de la muestra.
		p numero de variables independientes (o variables regresoras) en nuestro modelo.
		
		Al agregar mas variables independientes hace que el R 2 decrezca, ya que penaliza el cociente.	
		
		Adj R 2 Evalua si gana mas el valor del (1 - R2) o el cociente ((n - 1) / (n - p - 1)) por agregar esa nueva variable.
		
		
		A veces el umbral de rechazo de una variable para pertenecer al modelo (regresion multiple), el p valor, es muy cercano al nivel que tomemos como umbral. Si no señimos al algoritmo deberiamos rechazar la varible pero como su valor es muy cercano podriamos tener presente otros valores. Dar una segunda opinion si eliminamos o no ese variable.  
		
		Nos quedamos con el modelo de mayor R Cuadrado Ajustado cuando comparamos varios modelos en regresion multiple para determinar si agregar o eliminar una variable aporta al modelos. Cuando comenzas a sacar variables del modelo y cae el R Cuadrado Ajustado debemos para de eliminar variables (metodo de reduccion hacia atras).
		
     
	 Interpretar Coeficientes de Regresion Lineal
	 
		1. Signo del coeficiente. (Negativo) aporta negativamente a la prediccion.  (Positivo) aporta positivamente a la prediccion.  
		2. La magnitud del coeficiente. (Ojo con las unidades de los coeficientes) Analizar si son diferentes sus escalas. SIEWMPRE Analizar las unidades de los coeficientes. Hacer transformaciones para que todo cobre sentido.
		
		Interpretar el coeficiente como el incremento o decremento "por unidad de" ... (Dolar, Empleado, distancia, etc.). La unidad de la variable independiente que aporta positivamente o negativamente al modelo.

Fin de la Parte 2 - Regresión

¿Cómo sé qué modelo debo elegir para resolver mi problema?

Paso 1.

Averiguar si el problema es una prediccion (regresion), clasificacion o agrupamiento (clustering). Para ello analizamos la variable dependiente.

Si no existe la variable dependiente es un agrupamiento.
Si tiene la variable dependiente y la variable es continua o discreta es una regresion. Finalmente, si la variable dependiente es categorica es una clasificación.

Paso 2. Preguntarse si tu problema es o no es lineal. Se puede responder con la tecnica Grid Search.

Una vez lo sepas, Si tu problema es lineal, deberás intentar crear un modelo de Regresión Lineal Simple si solo tienes una variable independiente o un modelo de Regresión Lineal Múltiple en el caso de tener varias.

Si tu problema no es lineal, entonces tenemos varias técnicas donde elegir, como la Regresión Polinómica, SVR, Árboles de Decisión y Bosques Aleatorios. ¿Cuál te funcionará mejor? El método en este caso consiste en utilizar una técnica muy útil para evaluar modelos llamada k-Fold Cross Validation, y elegir el modelo que demuestre mejores resultados. 
 
¿Cómo puedo mejorar cada uno de estos modelos?

Existen dos tipos de parámetros en nuestros modelos:

- los parámetros que el modelo aprende, como los coeficientes de la Regresión Lineal,

- los hiper parámetros del algoritmo. En este último caso, los hiper parámetros son parámetros que el algoritmo no aprende, si no que son fijos y forman parte de las ecuaciones de los modelos. Por ejemplo, el parámetro lambda de regularización o el factor de penalización C son hiper parámetros.

---------------------
Seleccion de modelos.
---------------------

- - - 
K-Fold Cross Validation

Se utiliza para proporcionar una evaluacion relevante de la eficacia de nuestro modelo. 
Permite comparar diferentes algoritmos (Logistica, vectorial, KNN)y obtener una idea de cuan bien trabajaran.

Incrementa el rendimiento del modelo.

En la practica es comun dividir los datos en 10 bloques (Fold) - Python (cross_val_score (cv = nro. bloques)

Se muestrea varias veces nuestro dataset de partida. Incrementa el rendimiento del modelo porque todos los datos se usan para entrenar y evaluar.

Juzgar el rendimiento de nuestro modelo con un solo conjunto de datos de prueba no es el mejor enfoque (problema de varianza de los datos). No es la mejor forma de evaluar el rendimiento de un modelo.  K-Fold Cross Validation soluciona el problema de las varianzas entre un conjunto de datos de prueba.

Compensar el sesgo de la varianza

	Sesgo Bajo: diferencia baja entre la prediccion y el valor real observado. 
	Sesgo Alta: diferencia alat entre la prediccion y el valor real observado. 
		
	Varianza baja: Ejecutamos varias veces el modelo y la prediccion no varia demasiado.
	Varianza alta: Ejecutamos varias veces el modelo y la prediccion varia demasiado.
	
	En Python el resultado de aplicar este modelo es un array con la presicion de cada una de las corridas. Resultado final es la media de los resultados obtenidos y el desvio estandard.
	
- - - 
Grid search

Optimiza los hiperparametros. 

Nos permite identificar si es mejor elegir un modelo lineal o no lineal.

Trata de encontar los valores optimos de los hiperparametros. Hiperparametros son los que elige el usuario.

Realizamos primero la evaluacion deñ renmdimiento del algoritmo (K-Fold Cross Validation) y luego, ajustamos los hiperparametros para optimizarlos.

La optimizacion depende de los parametros del algoritmo utilizado. DEbe elegir los parametros que quiero optimizar y para ello debo conocerlos desde el algoritmo que voy a emplear. WARNING : No exagerar con la cantidad de parametros.


-------------
Clasificacion
-------------

Importante: Los algoritmos con distancia euclideas hay que escalarlos.

A diferencia de la regresión donde se predice un valor continuo, se utiliza la clasificación para predecir una categoría. Existen una gran amalgama de aplicaciones del proceso de clasificación desde medicina hasta marketing. Los modelos de clasificación incluyen desde modelos lineales como la Regresión Logística, SVM, así como otros no lineales como K-NN, Kernel SVM y Bosques Aleatorios.

Clasificar y catalogar.

- Regresion Logistica

	Es buena cuando los datos pueden ser separados por medio de una recta.
	
	Busca el mejor separador lineal recto (la mejor recta) posible en el proceso de clasificacion.
	
	En el grafico las variables independientes van en el eje de las x y la dependiente en el eje de las Y.
	
	Se usa para clasificacion. Es un clasificador lineal. Devuelve probabilidades.
	
	Se  predice una accion. Por ejemplo, compra o no compra. Abre o no abre.
	
	Accion: Comprar o no comprar un producto.
			Abrir o no abrir un correo electronico. 
			
			Es decir llevar a cabo una accion.
			
			La logistica utiliza la regresion lineal, en 2D (Sueldo y Edad por ejemplo), el separador es una recta. En 3D el separador es un plano. En multiple dimensiones el separador lineal es un hiper plano.
			
			Ejemplo, 
				Compra en funcion de la edad
				Abre el correo en funcion de la edad
				
			En lugar dee predecir lo que vaa suceder podriamos predecir que tan probable es un cliente acepte la oferta.
			Probabilidad entre cero y uno. Todo lo que esta por debajo de cero o cero sera no compra, todo lo que sea uno o supere a uno sera compra. Lo intermedio sera un probabilidad de que compre o no. La idea es encontrar esas franjas de edades y cuantificar la probabilidad que compre o no. 
			
			No se puede usar la regresion lineal porque los datos no siguen esa funcion. Si en lugar se le aplica una funcion sigmoide (por ejemplo para predecir la venta dada la edad) para despejar el valor de la variable Y, obtendriamos:

				funcion lineal 
			     		y = b0 + b1 * x
				
				funcion sigmoide 
							p = 1 /(1 + e^-y) 
				
					al despejar la y obtendriamos:
						 p = 1 /(1 + e^-y) => y = ln(p/1-p) 
						
				Finalmente, 
				
					ln(p/1-p) = b0 + b1 * x
								
			En resumen, a una regresion lineal se le aplica una funcion sigmoide para trasnformar el valor final de la prediccion en una probabilidad.
			
				Regresion logistica o sigmoide
						ln(p/1-p) = b0 + b1 * x
							
							Es un tuneo  de una regresion lineal. 
							
							El eje y se trasnforma en una probabilidad. (en la regresion lineal es un prediccion)
							Probabilidad entre 0 y 1. 
							
							Se busca la funcion sigmoide (o logistica) que mejor se ajusta con respecto al conjunto de funciones sigmoides. Objetivo final, si hay probabilidad de comprar o no (Si se realiza el suceso o no)
							
							En el grafico la recta X es la variable independiente y la recta Y la probabilidad de compar o no. (Si lleva a cabo la accion o no)
							
							Se toma valores aleatorios para la variable independiente X y se analiza la probabilidad del suceso dado ese valor de X en funcion de la regresion logistica. Luego, en funcion de un "valor central" se decide si ocurre o no el suceso (si compra o no). Ese valor central generalmente es 0,5. Define si ocurre o no el suceso dada la probabilidad obtenida. El valor central se puede elegir arbitrariamente.
							
							Si la probabilidad obtenida del modelo es menor o igual que 0,5 no ocurre el suceso, de lo contrario si ocurre.
							
							En resumenn, el calculo de probabilidades es necesario como herramienta intermedia para realizar la clasificacion (compra o no).
				
				Matriz de confusion
				
						Una matriz de confusión es una herramienta que permite la visualización del desempeño de un algoritmo que se emplea en aprendizaje supervisado. Cada columna de la matriz representa el número de predicciones de cada clase, mientras que cada fila representa a las instancias en la clase real. Uno de los beneficios de las matrices de confusión es que facilitan ver si el sistema está confundiendo dos clases.
		
			En R:
			
				# Ajustar el modelo de regresión logística con el conjunto de entrenamiento.
				classifier = glm(formula = Purchased ~ .,
							data = training_set, 
								family = binomial) Es (family = binomial) binomial porque compra o no compra es lo que queremos predecir.
				
					CUIDADO - Cambios en la sintaxis de las librerías

					En la clase instalo la librería ElemStatLearn. Si te sale un error debido a cambios en la librería, debes hacer la instalación indicada en los siguiente pasos:


					https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/


					1. Descarga la librería desde este link o utilizando el enlace anterior.

					2. A continuación abre R-Studio. Selecciona el menú de herramientas y haz click en instalar paquetes.

					3. Desde 'Install from:' elige el la opción package install file(.zip; .tar,etc)

					4. Selecciona el paquete descargado anteriormente.

					También puedes instalarlo haciendo uso de la linea de comandos con la instrucción : R CMD install <pkg>.

- K-Nearest neighbors (K-NN)

	Es usado para categorizacion.
	
	No es un clasificador lineal. Los datos pueden ser separados sin la existencia de una recta o linea.
	
	Como lo hace?
	
		Cuatro pasos:
		
				Paso 1. 
						Elegir numero k de vecinos mas cercanos que tenemos en cuenta para el proceso de clasificacion.
						Elegir K en impar para no tener empate. K = 5 es el mas utilizado generalmente.
						
				Paso 2.
						Tomar los k vecinos mas cercanos del nuevo dato utilizando la distancia Euclidea. Es decir, se seleccionan los k vecinos mas cercanos utilizando la distancia euclidea.
						No es necesario utilizar la distancia euclidea, se puede usar otra (ej, manhattan, metrica del infinito, etc..) pero euclidea es la mas utilizada.
						
						La distancia euclidea entre dos puntos es basicamente el teorema de pitagora:
								Distancia entre P1 y P2  = SQRT((x2-x1)^2 + (y2-y1)^2)
						
				Paso 3.
						Entre los k vecinos contamos la cantidad de puntos que pertenecen a cada una de las categorias.
				
				Paso 4.				
						Asignar el nuevo punto de dato a la categoria que tenga mas vecinos en ella. 
						
				Paso 5.
						El modelo ya esta elavorado.
											
	En Python:
	
		from sklearn.neighbors import KNeighborsClassifier
		classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
		classifier.fit(X_train, y_train)
		
		n_neighbors: nro de vecinos
		
		metric: metrica para calcular la distancia de los puntos. (minkowski)
						(minkowski) Familia de distancias.
									
		p: (1) manhattan (2) euclideana
		

- SVN (support vector machine)

		Detecta los puntos mas cercanos entre las categorias. Esto se llama el margen maximo. Distancia maxima para equivocarse. Luego,  traza una linea x distante de esos puntos. La suma de las distancias entre  los puntos mas cercanos y la linea se busca maximizarla.  Se busca crear ese margen lo mas grande posible. Margen la distancia entre los puntos (o vectores) mas cercanos. Los puntos mas cercanos entre las categorias se llaman vectores. La linea separadora se llama Hiperplano. El Hiperplano busca maximizar el margen. Las rectas asociadas a los vectores se llaman hiperplano positivo e hipleplano negativo.
		
		Python.
		
		from sklearn.svm import SVC
		classifier = SVC(kernel = "linear", random_state = 0)
		classifier.fit(X_train, y_train)
		
		kernel: define la forma de separacion. Hay varios: (rbf: Gausiano, linear: lineal, etc.)
		
		Kernel lineal: linear. Basicamente es como una regresion logistica.	 Es lineal. Se genera una linea de separacion.
		Kernel no lineales: rbf
		
		Parametro degree: se usa en un kernel polinomico.
		Parametro gamma: se usa en nucleo gauseano, polinomico o sigmoide.
		
		Kernel SVN:
		
			Idea de Kernel:
			
				Que paso con nucleos no lineales?
					
				No siempre el limite de separacion entre las categorias es una recta.
				Hay datos que no son linealmente separables.
				
				La SVN tiene una hipotesis la cual es, que el limite de separacion (o la forma de separacion) debe ser detectada o definida por el humano. La separacion puede ser por esferas, circulos, elipses, etc. 
				
				Cuando no son lineales puedo representarlos en otras dimensiones para ver que sucede. Por ejemplo, en 3D puede ser que los puntos esten al nivel del suelo o los otros puntos al nivel del techo. Alli puedo encontar un plano o una linea que los separe. 
				
				Datos no separables en forma lineal.
				
				Hay que tranformarlos en un espacio de nivel superior y obtener un conjunto de datos separables en forma lineal. Luego, se retorna al espacio original para obtener la separacion correcta.
				
				El algoritmo toma datos en una dimension que no son separables linealmente, los transforma en otra dimension, mediante una funcion, tratando de separarlos linealmente, intentando hallar un hiperplano, en la nueva dimension. Encontrada la linea de separacion lineal (o hiperplano) se retorna a la dimension original mediante una proyeccion y se representa, en la dimension original, el separador que es no lineal.
				Usar esta mecanismo de transformación tiene un precio. Al subir a una dimension el calculo computacional puede ser mas costoso.(puede requerirse un equipo con muchos recursos o puede tardarse en clasificar y dar una respuesta)
				
				Truco del Kernel:
				
					Kernel RBF (Gausiano) : No lineal. Se púede usar sin necesidad de entrar en un espacio de nivel superior.
				
				Tipos de Kernel:
				
						- Gausiano (RBF) Muy util cuando los datos deben ser separados como circulos 
						- Sigmoide 
						- Polinomico
						Hay muchos mas...
				
				NOTA: Cuando no quiero que una variable domine las demas variables aplico la estandarizacion. Tambien, en algoritmos que trabajan con distancia euclidea.
				
- Naive Bayes

		- Teorema de Bayes

				P(A|B) = P(B|A) * P(A) / P(B)
				
				A: condicionado
				B: Suceso condicionante
				P: 
				
				Maquina 1: 30 herramientas/hora - Las herramientas son marcadas indicando la maquina que la genero. Maquina 1
				Maquina 2: 20 herramientas/hora - Las herramientas son marcadas indicando la maquina que la genero. Maquina 2
				
				Total de herramientas/hora: 50 herramientas (30+20).
						
				De todas las herramientas producidas se observa que el 1% es defectuoso.
				
				De todas las herramientas defectuosas se observa que:
					El 50% de herramientas defectosas proviene de la maquina 1.
					El 50% de herramientas defectosas proviene de la maquina 2.
					
				Pregunta:

				¿Cual es la probabilidad que una herramienta producida por la maquina 2 sea defectuosa?
				La respuesta es el teorema de bayer.
				
				Probabilidad = Casos posibles / Total de casos.
				
				Si elijo una herramienta cualquiera la probabilidad que sea de una u otra maquina es:
				
				P(Match1) = 30/50 = 0,60 - 60% - Probabilidad de que la herramienta sea producida por la maquina 1 del total de herramientas
				P(Match2) = 20/50 = 0,40 - 40% - Probabilidad de que la herramienta sea producida por la maquina 2 del total de herramientas
				
				P(Defectuosa) = 1%
				
				P(Match1| Defectuosa) = 50% - Sabiendo que es defectuosa cual es la probabilidad de que la herramienta sea de la maquina 1
				P(Match2| Defectuosa) = 50% - Sabiendo que es defectuosa cual es la probabilidad que que la herramienta sea de la maquina 2
				
				Las anteriores son probabilidades condicionadas.
				
				¿Cual es la probabilidad que una herramienta producida por la maquina 2 sea defectuosa?
				
				P(Defectuosa|Match2) = ?
				
				La respuesta es el teorema de bayer.
				
				Teorema de bayer.
				
						P(A|B) = P(B|A) * P(A) / P(B)
				 
						P(Defectuosa|Match2) = P(Match2|Defectuosa) * P(Defectuosa) / P(Match2)
						
						P(Defectuosa|Match2) =  0,50 * 0,01 / 0,40 = 0,0125 (1,25%) De cada 1000 herramientas producidas de la maquina = 12,5 son defectuosas.
				
				En general, la probabilidad condicionada por un suceso se gira el condicionante y el condicionado, multiplicamos por la probabilidad del condicionado y dividimos por la probabilidad del condicionante.
				
				Veamos otro ejemplo:
				
					Calculo frecuentista:
						Total de herramientas: 1000 herramientas 
						Herramientas maquina 2: 400 herramientas 
						1% tienen defecto: 10 herramientas defectuosas.
						Herramientas defectuosas de producidas por la maquina 2: 5 herramientas 
						porcentaje de herramientas defectuosas provenientes de la maquina 2: 5 * 1 / 400 =  1,25%
						
					Porque no realizamos el calculo frecuentista(contamos las herramientas)?
							- Podria ser que tome mucho tiempo contar las herramientas por cada maquina e identificar las defectuosas.
							- Puede ser que no se tenga acceso a los datos o la informacion.
							
							
					Ejercicio rapido:
					
						P(Defectuosa|Match1) = P(Match1|Defectuosa) * P(Defectuosa) / P(Match1)
											 = 0,50 * 0,01 / 0,60 =
	
		- Naive Bayes.
		
				- Porque Naive?
					Naive( se traduce ingenuo)
					Supone una independencia entre los casos que aparencen dentro de la probabilidades. Que las variables independientes sean independientes es decir, no exista correlacion entre ellas. A veces, puede que no exista esa independencia. Es decir, el teorema de Bayes requiere que la Edad y el sueldo sean independientes entre si.
					Se puede aplicar igual, y se obtiene resultados bastantes buenos. Por eso se llama ingenuo.
					
				El ejemplo se trabajo con dos variables pero puede haber + variables.
				
				- Variables independientas: Sueldo y Edad. Vector: (Sueldo, Edad).
				- Categorias: "Camina al trabajo" o "Conduce al trabajo"
				
				Plan de Ataque:
					
					Se aplica dos veces. 
					
					(1) La primera es para conocer cual es la probabilidad de que la persona camine:
					 
							X: Es el vector de caracteristicas (Sueldo, Edad). Ej: (U$S30000, 25). (El dato lo observo o lo consulto).
							
							P(camine/X) = P(X/camine) * P(camine) / P(X)
							
								P(camine): Se llama Probabilidad Previa o a Priori. Conocer cuanta gente va caminando al trabajo. Probabilidad de que una persona camine. 
								Se cuenta todas las personas y se cuenta cuantas caminan.
								P(camine) = Cant. individuos que caminan/Total de individuos.
														
								P(X): Se llama Probabilidad Marginal con respecto a esas caracteristicas. Indivudos que presentan esas caracteristicas del total de individuos. Que probabilidad tiene una persona en presentar esas caracteristicas.
								Como se cualcula? La probabilidad de que un individuo seleccionado al azar presente esas caracteristicas.
								Para calcularla se selecciona un radio de variabilidad, se dibuja un circulo con ese radio cerca del individuo (punto) a clasificar. El individuo o punto se usa como centro. Luego, se cuenta la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. Es decir, cuantos individuos tienen caracteristicas similares al individuo a clasificar.
								El radio es un parametro de entrada del algoritmo. El radio define la cantidad de individuos o puntos forman parte de la muestra a ser usada para contar la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. 
								Todos los individuos o puntos que estan dentro del circulo se consideran similar al individuo a clasificar. Por lo tanto, todos los puntos dentro del circulo son similares. 
								Entra el concepto de distancia: No es lo mismo la distancia euclidea o manhattan. 
														
								P(X/camine): Se llama Probabilidad condicionada por un suceso.
								Del total de personas que camina cuantas presenta esas caracteristicas.
								Para realizar el calculo se usa el mismo circulo definido en el calculo de P(X). La diferencia es que dentro del circulo se cuenta el total de individuos que caminan.	
														
								P(camine/X): Se llama probabilidad a posteriori.

					(2) La segunda es para conocer cual es la probabilidad de que la persona conduce:
					 
							X: Es el vector de caracteristicas (Sueldo, Edad). Ej: (U$S30000, 25). (El dato lo observo o lo consulto).
							
							P(conducen/X) = P(X/conducen) * P(conducen) / P(X)
							
								P(conducen): Se llama Probabilidad Previa o a Priori. Conocer cuanta gente conduce al trabajo. Probabilidad de que una persona conduzca. 
								Se cuenta todas las personas y se cuenta cuantas personas conducen.
								P(conducen) = Cant. individuos que conducen/Total de individuos.
														
								P(X): Se llama Probabilidad Marginal con respecto a esas caracteristicas. Indivudos que presentan esas caracteristicas del total de individuos. Que probabilidad tiene una persona en presentar esas caracteristicas.
								Como se cualcula? La probabilidad de que un individuo seleccionado al azar presente esas caracteristicas.
								Para calcularla se selecciona un radio de variabilidad, se dibuja un circulo con ese radio cerca del individuo (punto) a clasificar. El individuo o punto se usa como centro. Luego, se cuenta la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. Es decir, cuantos individuos tienen caracteristicas similares al individuo a clasificar.
								El radio es un parametro de entrada del algoritmo. El radio define la cantidad de individuos o puntos forman parte de la muestra a ser usada para contar la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. 
								Todos los individuos o puntos que estan dentro del circulo se consideran similar al individuo a clasificar. Por lo tanto, todos los puntos dentro del circulo son similares. 
								Entra el concepto de distancia: No es lo mismo la distancia euclidea o manhattan. 
																				
								P(X/conducen): Se llama Probabilidad condicionada por un suceso.
								Del total de personas que conducen cuantas presenta esas caracteristicas.
								Para realizar el calculo se usa el mismo circulo definido en el calculo de P(X). La diferencia es que dentro del circulo se cuenta el total de individuos que conducen.	
								
								P(conducen/X): Se llama probabilidad a posteriori.
					
				   (3) - Paso 3. Compara las probabilidades. La mayor gana.
				   
							Naives Bayes compara las dos probabilidades y obtiene la mas probable.

							Compara P(camine/X) vs P(conducen/X) y obtiene la de mayor probabilidad.
							
					
					Ejemplo:
							
							Total de personas: 30
							Personas que conducen: 20 
							Personas que caminan: 10
							
							P(Camine): 10/30
							P(conducen): 20/30
							
							P(X): Como se cualcula? 
							Se selecciona un radio y luego se define un circulo tomando como centro el individuo a clasificar (sus caracteristicas.Ej: Edad, sueldo). Finalmente, se cuenta la cantidad de puntos o individuos dentro del circulo. En nuesto ejemplo son 4.
							
							Personas dentro del circulo: 4 (Individuos similares al individuo a clasificar)
							Personas dentro del circulo que caminan: 3
							Personas dentro del circulo que conducen: 1
							
							P(X): Cant. individuos dentro del circulo/Total de individuos.
							P(X): 4/30
							
							P(X/camine): Cant. individuos dentro del circulo que caminan/Total de individuos que caminan.
							P(X/camine): 3/10
							
							P(X/conducen): Cant. individuos dentro del circulo que conducen/Total de individuos que conducen.
							P(X/conducen): 1/20
												
							P(camine/X) = P(X/camine) * P(Camine) / P(X)
							P(camine/X) = (3/10 * 10/30) / (4/30) = 0,75
							
							P(conducen/X) = P(X/conducen) * P(conducen) / P(X)
							P(conducen/X) =	(1/20 * 20/30) / (4/30) = 0,25
							
							P(camine/X) vs  P(conducen/X) = 0,75 vs 0,25 => 0,75 > 0,25 => El individuo a catalogar tiene una mayor probabilidad de caminar. Finalmente, se categoriza como Camina. Es una probabilidad.
								Tiene el punto un 75% de que Camina
								Tiene el punto un 25% de que Conduce
							
					NOTA: Como P(X) es usado en ambas formulas, podemos eliminarlo en ambas formulas y el resultado es el mismo.
				
						P(camine/X) = P(X/camine) * P(Camine) 
						P(camine/X) = (3/10 * 10/30) = 0,1
						
						P(conducen/X) = P(X/conducen) * P(conducen) 
						P(conducen/X) =	(1/20 * 20/30) = 0,033
						
						P(camine/X) vs  P(conducen/X) = 0,1 > 0,033
						El punto a clasificar: Camina. No es una probabilidad. Solo me permite comparar cual es el valor mayor. Es una modificacion ligera. NOTA: No sirve para saber la probabilidad. La mas segura es realizar el calculo completo (usando el denominador P(X)).
					
					NOTA: Que pasa con mas de dos caracteriisticas independientes?
						
						Para el calculo de probabilidades se aplica Bayes a cada caracteristica independiente. 
						
						Recuerda que las probaabilidades no puede superar el 1. Con lo cual si tenemos una caracteristica con una probabilidad mayor al 0,5. El resto de las probabilidades sera menor y esa gana.
									
						Si son mas de tres caracteristicas independientes puedo calcular las probabilidades de dos caracteristicas y ver sus probabilidades viendo que si supera sus sumas el 0,5. La mayor de ellas es la que gana.

			Resumen: Se calculan las probabilidades condicionadas para cada una de las caracteristicas.
			
			NOTA: 
				En R hay que factorizar la variable dependiente si no es un factor. Hacerlo antes de aplicar el algoritmo de Naive Bayes.
				
				Un factor es una variable categórica con un número finito de valores o niveles (etiqueta). En R los factores se utilizan habitualmente para realizar clasificaciones de los datos, estableciendo su pertenencia a los grupos o categorías determinados por los niveles del factor.
	
	
- Arboles de desicion
		
		NOTA: Los arboles no utilizan el concepto de la distancia euclidea. No necesitan escalar.
		NOTA: Si el algotimo usa distancia euclidea, es necesario escalar. Si no, no es necesario.
				
		CART. (Classification and Regresion Trees)
			---> Arboles de Clasificacion. Basado en variables categoricas. Categorias ej, hombre o mujer, compra o no compra
			---> Arboles de Desicion. Basado en variables numericas. Predice un valor.
	
		Como sabe el algoritmo donde ir cortando los datos?
			Ejemplo:
				Rojo = compra
				Verde = no compra 
			Se hace de tal modo que se maximiza el numero de categorias (o puntos) dentro de cada una de las divisiones. Por ejmeplo, quremos que las categorias (o puntos) queden en un lado cuanto mas puntos rojos mejor y al otro lado cuanto mas puntos verdes.
			Concretamente, en la separacion se usa el concepto de entropia. La entropia mide el desorden de una serie de datos. Es un concepto matematico. 
			
		Despues de la separacion tendriamos nodos terminales u hojas con los puntos de separacion para la regresion o le asigna rojo o verde dependiendo el caso. 
		
		x1 y x2: variables independientes.
		
		Luego de la separacion:
		
				Si x2 > 60
						Si x1 < 50
								"Es Verde"
						else
								"Es Rojo"
				else 
					if x1 > 70
						si x2 < 20
							"Es Rojo"
						else
							"Es Verde" 
					else
						"Es Rojo"
						
				
		Un Arbol puede tener mas variables independientes.
		
		NOTA: La tecnica es muy vieja y no son muy utilizados. Son Simple. Se mejoran con otros algoritmos o tecnicas. Las nuevas tecnicas que los mejoran son varias por ejemplo, la tecnica de bosques aleatorios, Gradient Boosting, etc.
			  
		En python el objeto es DecisionTreeClassifier, sus parametros son:
		
				criterion: Criterio de division. Por defecto: "Gini". Tambien, esta la "entropy". Ambos son usados para la division. Generalmente se usa la entropia ("entropy"). Busca la homogenea en la separacion de los datos.
				Entropia 0 la rama es homogenea.


- Bosques aleatorios

		Aprendizaje en conjunto: Combinan la potencia de diferentes algoritmos de prediccion. Son varios algoritmos trabajandp en paralelo y luego juntan la informacion. Objetivo: Elevar el resultado.
	
		NOTA: En el ejemplo se escalo para que quede bonito el grafico. Los arboles no tienen necesidad de escalar. No usan distancia euclidea.

		NOTA: Puede ser que cuando agrego mas arboles al bosque la prediccion puede que no mejore por el overfitting.
		
		Pasos:
	
			Paso 1. Se elige un numero K (numero aleatorio). Representa un subconjunto de los datos del conjunto de datos de entrenamiento. En lugar de dividir los datos en conjunto de entrenamiento y conjunto de test. Elegimos un subconjunto de datos de los datos de entrenamiento.
			
			Paso 2. Construir un arbol de desicion para los K puntos de datos. Lo que hace que el arbol tenga una vision parcial de los datos.
			
			Paso 3. Se define el numero de arboles de desicion y se repite el paso 1 y 2. Cada nuevo arbol tiene un subconjunto de datos del total de datos de entrenamiento.
			
			Paso 4. Cuando llegamos a construir el numero de arboles deseados. Se puede usar todo el bosque para hacer la prediccion. A cada arbol se le indica que realice la prediccion de la categoria a la que pertenece el nuevo punto. Finalmente, la prediccion final es asignar al nuevo punto la categoria con mas votos. 
			
			Este algoritmo mejor la prediccion de un solo arbol de desicion. 
				
			Python:
				RandomForestRegressor(n_estimators = 10, random_state = 0)	
					Parametros de ForestRegressor
							n_estimators -> cantidad de arboles del bosque. Defecto 10 arboles.
							criterion -> criterio para dividir una rama en dos subramas o nodos hojas.
										Criterio de division: MSE (error cuadrado medio) diferencia entre la prediccion y el valor real al cuadrado. Se intenta minimizar. Se divide la rama en dos subramas intentando que el MSE de cada rama sea menor al MSE de la rama que se divide. Tambien, hay otros criterios de division como la entropia (Se minimiza la entropia). 
							max_features -> caracteristicas tenidas en cuenta a la hora de generar las divisiones.
							random_state -> reproductivilidad para obtener el mismo bosque.
					  

- Evaluar la eficacia de los modelos de clasificacion.

	- Falsos positivos y falsos negativos.
		
			Falsos positivos o errores de tipo 1. Ejemplo, Usuarios que no compraban, la observacion real del dato lo muestra, pero mi modelo indica que compran.
			
			Falsos negativos o errores de tipo 2. Ejemplo, Usuarios que compraban, la observacion real del dato lo muestra, pero mi modelo indica que no compran.
			
			De estos dos hay uno peor que otro. Falsos positivos o errores de tipo 1 son cuidado. Mientras, Falsos negativos o errores de tipo 2 son PELIGRO DE MUERTE (Son malos) ES EL PEOR DE AMBOS. En ciertas cuestiones medicas o biologicas puede provocar daños a las personas. Ej, No hay un tsunami (pero lo hubo).
			
			
	- Matriz de confusion: 
		
			Ejemplo Matriz de confunsion
									predichos
								
								0				1
						0		35				5	-> (falso positivo)
			observados
			(real)		1		10				50
								 |
								 V
							   (falso negativo)
	
	       calcular dos prporciones:
		   
				Ratio de presicion (RP) = Correctos / Total =  85/100 = 85 %
				Ratio de error (ER) = Incorrecto / Total = 15 / 100 = 15 %
				
				
	- Paradoja de presicion:
	
			Ejemplo Matriz de confunsion
									predichos
								
								0				1
						0		9700			150	-> (falso positivo)
			observados
			(real)		1		50				100
								 |
								 V
							   (falso negativo)		
							   
			Ratio de presicion (RP) = Correctos / Total =  9800/10000 = 98 %
			Ratio de error (ER) = Incorrecto / Total = 200 / 10000 = 2 %
			
			Segun la matriz de confusion la gran mayoria de datos son marcados como Cero (9700). Hay muchos datos marcado como Cero. Dejamos el modelo y marco el dato como Cero. Muevo los datos que eran predichos como 1 a 0. La razon es porque es mas probable un 0 que un 1:
			
											predichos
										
										0				1
								0		9850			0	-> (falso positivo)
					observados
					(real)		1		150				0
										 |
										 V
									   (falso negativo)		
									
					Ratio de presicion (RP) = Correctos / Total =  9850/10000 = 98,5 %
					Ratio de error (ER) = Incorrecto / Total = 200 / 10000 = 2 %
			
			Se aumenta la presicion sin haber usado el modelo. Esto es la paradoja de la presicion. Cuando tenes un suceso raro o muy raro que ocurra, lo mejor es no usar modelos. Por ejemplo, si un 0,1 % de la poblacion tiene una enfermedad, lo mas normal es que nadie la tenga. Esto puede mostrar que en sucesos raros los modelos a veces son inutiles.
		
		
	- Curva CAP (Perfil de Presicion Acumulado):
			
			Como se sabe que un Perfil acumulado es mejor que otro? El area que queda entre la curva de color rojo y la recta azul. Cuanto mayor es esa area, mejor resultado se obtiene. Lave la pena de representar cuan se lleven a cavo algoritmos de clasificacion para ver que tambien el algoritmo funciona. Generalmente, se traduce a porcentajes: Del 100% de personas (o datos de nuestro dataset) suceptibles a ser contactadas, a que porcentaje tengo que atacar para conseguir el 50% o 60 % de las ventas. Si empleamos varios algoritmos de clasificacion y generamos el Perfil de Presicion Acumulado podemos compararlos. 
			
			Perfil de Presicion Acumulado es adecuado para comparar algoritmos de Clasificacion. Al compara los algoritmos con "(Perfil de Presicion Acumulado)" podemos saber que tanto mas ganamos de un modelo a otro o con respecto al modelo aleatorio (tomar individuos al azar). 
			Existe una curva llamada "Bola de cristal(o Modelo Perfecto)" que es el mejor modelo. Con el se puede contactar el 10% de los clientes y que los mismos realicen el 100% de las compras. Se sabe a quien contactar y que realizara todas las compras. No ocurre casi nunca pero es adonde intentamos ir.
			
			
	- Analisis de las curvas CAP		
	
			La linea del Modelo Buen es obtenida desde la salida del algotimo de clasificacion.
			
			Para evaluar modelos de clasificacion. Tenemos varias lineas que representan las salidas de nuestro modelos. Sin embargo, existen 3 lineas principales: La linea del modelo perfecto, la linea del buen modelo y la linea del modelo aleatorio. Lo que realizamos es cociente de areas, el limite al que podemos llegar.Como de cerca estamos de la linea perfecto o buen modelo (Bola de cristal). Para ello tomamos el area que va desde la linea del Modelo Perfecto hasta la linea del Modelo Aleatorio. Esta es el area del Modelo Perfecto (Ap). Esta es el area maximo que puede conseguir un modelo de clasificacion. Del mismo modo calculamos el area que va desde la linea del Modelo Bueno hasta la linea del Modelo aleatorio. La llamamos (Ar). Lo que queremos saber que porcentaje del area del Modelo Perfecto (AP) cubre el area del Modelo Bueno.
					
					AR = Ar/Ap . 
					
					Esto nos da un valor entre 0 y 1. Cuanto mas cerca de 1 mejor ajusta al Modelo Perfecto. Cuanto mas lejos de 1 sera peor. Si el valor es 0 es lo mismo realizar elecciones con el modelo o realizarlas aleatoriamente.
					
					El calculo de AR no se suele realizar a mano. Suele haber librerias que ayuden. La curva es dificil de calcular. Normalmente se realiza una serie de muestreos, se calcula con el modelo con el 20% de los datos, con el 40% de los datos, con el 60% de los datos. Se obtiene un poligono que ajusta el area de nuestro modelo y se calcula el area basado en este poligono. 
					
					Otra forma de ver que tan bueno es nuestro modelo es establecer una cota del total de contactados. Tipicamente la cota suele ser 50%. Luego se mira que porcentaje de Ventas se consigue en el modelo con el 50% de compradores. Busco el 50% de compradores y analizo cuanto me pronostica el modelo (X %). Se usa la siguiente regla para clasificar el modelo:
					
							90% < X < 100% 	Demasiado Bueno.
							80% < X < 90% 	Muy Bueno.
							70% < X < 80% 	Bueno.
							60% < X < 70% 	Malo.
							60% < X 		Muy malo.
					
							Entre 70% y el 80% es un modelo bastante acertado. No presenta problemas de overfitting
							Entre 80% y el 90% es muy bueno. No presenta problemas de overfitting. Si se presente es Muy bueno.
							EWntre 90% < X < 100% 	Demasiado Bueno. Puede presentar overfitting. Estudiarlo. Puede ser que en un 100% la variable dependiente forma parte de las independientes. Tambien, puede haber una relacion (o correlacion) entre las variables independientes.
							
							NOTA: Ir con  cuidado cuando los modelos superen el 90% de presicion. Es imposible conseguir el 100% de prediccion. Seguramente existe overfitting.
							
 - Conclusion - Clasificación


		¿Cuales son los pros y los contras de cada modelo?

		¿Cómo sé qué modelo debo elegir para resolver mi problema?

		¿Cómo puedo mejorar cada uno de estos modelos ?


		Resolvamos cada pregunta una por una:

		1. ¿Cuales son los pros y los contras de cada modelo?

		En esta clase encontrarás un documento que te dará todos los pros y contras de cada modelo de clasificación.


		2. ¿Cómo sé qué modelo debo elegir para resolver mi problema?

		Al igual que con los modelos de regresión, primero tendrás que averiguar si tu problema es o no es lineal.

		Una vez lo sepas, Si tu problema es lineal, deberás intentar crear un modelo de Regresión Logística o bien SVM.

		Si tu problema no es lineal, entonces tenemos varias técnicas donde elegir, como K-NN, Naïve Bayes, Árboles de Decisión o Random Forest.

		Desde un punto de vista empresarial, entonces deberías usar:

		Regresión Logística o Naïve Bayes cuando quieras ordenar tus predicciones por probabilidad. Por ejemplo, deberías usar estas técnicas si quieres crear un ranking de clientes desde el más probable al menos probable que compre un producto. Esto permite crear objetivos específicos para las campañas de marketing, por ejemplo. Y por supuesto, si tu problema de empresa es lineal, mejor utiliza la regresión logística, y si no lo es, intenta con Naïve Bayes.

		SVM  cuando quieras predecir a qué segmento pertenece un cliente. Los segmentos pueden ser cualquier conjunto de características que definan a los clientes, como los que identificaremos en la Parte 4 - Clustering.

		Los árboles de decisión cuando necesites tener una interpretación clara de los resultados modelizados.

		Y por último, Random Forest cuando busques un mejor resultado de predicción y te preocupe menos la interpretación de los modelos.


		3. ¿Cómo puedo mejorar cada uno de estos modelos ?

		Igual que en la parte 2, en la Parte 10 - Selección de Modelos, la segunda sección está dedicada a los Ajustes de Parámetros que permite mejorar la eficacia de nuestros modelos ajustando los valores de los parámetros. Como habrás comprobado, existen dos tipos de parámetros en nuestros modelos:

		los parámetros que el modelo aprende, como los coeficientes de la Regresión Lineal,

		los hiper parámetros del algoritmo.

		En este último caso, los hiper parámetros  son parámetros que el algoritmo no aprende, si no que son fijos y forman parte de las ecuaciones de los modelos. Por ejemplo, el parámetro lambda de regularización o el factor de penalización C son hiper parámetros. Hasta el momento hemos tomado los valores por defecto y no nos hemos preocupados de afinar su valor óptimo para mejorar la eficacia del modelo. Entontrar el valor óptimo es parte del Ajuste de Parámetros, así que si estás interesado en descubrir cómo hacerlo, te recomiendo ir directamente a la Parte 10 del curso donde veremos juntos cómo hacerlo.
						
					
-------------------------
Clustering o Segmentacion
-------------------------

		Clustering es un proceso similar al de clasificación, pero con un fundamento diferente. En el Clustering no sabes qué categorías estás buscando, si no que intentas crear una segmentación de tus propios datos en grupos más o menos homogéneos. Cuando utilizamos algoritmos de clustering en el data set, surgen de entre los datos cosas inesperadas como estructuras, clusters o agrupaciones que un humano no se habría imaginado pero la máquina crea por nosotros.

		Algoritmos de Clustering para Machine Learning:

			Clustering con K-Means

			Clustering Jerárquico
			
- K-Means

		Se usaron en el ejemplo 2 variables pero se pueden tener mas variables (3, 4, etc.) para identificar los puntos.
		
		Tenemos un conjunto de puntos que queremos segmentarlos o clasificarlos. La diferencia es que no conocemos las categorias a asignar a cada conjunto. Los datos se agrupan por homogéneos. No hay categorias definidas conocidas previamente. Por ejemplo, compra o no compra.
		
		Como funciona:
		
			Paso 1. Elegir el numero k de cluster. (NOTA: Ver como calcular el numero K de cluster)
				
			Paso 2. Se seleccionan K puntos. Son los baricentros iniciales. (no necesariamente tienen que ser de nuestro dataset). Seran los hipoteticos centros geometricos de cada cluster. Los baricentros son lo puntos en el medio de cada cluster. Se seleccionan al azar. 

			Paso 3. Se asignan cada uno de los puntos del dataset al baricentro mas cercano. Esto forma K cluster.

			Paso 4. Se calcula el nuevo baricentro de cada cluster usando los puntos de cada cluster. Calculo la media geometrica de cada coordenadas (variables o caracteristicas de los datos. Ejemplo X1, x2, etc.) del cluster. Los baricentros originales quedan recalculado. Si en el calculo del nuevo baricentro uso distancia euclidea, los cluster quedaran redondos o elipse. En cambio si uso la distancia manhattan los cluster serian cuadrados. 
			NOTA: La distancia que elijamos es importante. La euclidea es la mas comun. Que distancia debo utilizar? Depende del problema a resolver. IMPORTANTE: Investigar sobre la forma de calcular las distancias y cuales son sus concecuencias en la forma del cluster.
			
			Paso 5. Se reasignan cada uno de los puntos del dataset al baricentro mas cercano. Si se realizan nuevas asignaciones ir al Paso 4 sino ir a Fin.
			
			FIN. Modelo listo!
			
			
		- La trampa de la incializacion aleatoria:
		
			Al inicio se eligen baricentros al azar para comenzar el algoritmo. Segun como se inicialicen esos baricentros el resultado puede ser bastante diferentes. Hay que tener cuidado con la eleccion de baricentros.
			
			¿Que pasa si elegimos mal los baricentros?
			
				Segun como inicialicemos nuestros baricentros al comienzo de algoritmo podemos sesgar el resultado final. Esto no es bueno porque una eleccion aleatoria puede llevar a una segmentacion incorrecta. Como corregimos esto? No es sencillo. Existe una modificacion del algoritmo de  K-Means que permite seleccionar bien los baricentros al incio del algoritmo. Se llama K-means ++ (plus plus). Es bastante dificil de entender pero se puede leer en otras fuentes. El algoritmo de K-means ++ esta implementado en python o R y no hace falta entender el fundamento del algoritmo K-means ++ para utilizarlo en esos lenguajes. NOTA. La seleccion de los baricentro iniciales puede sesgar la clasificacion. La respuesta:  K-means ++.
				
				
		- Como seleccionar el numero correcto de K cluster?
		
			K indica la cantidad de grupos que salen de la segmentacion.
			
			Dependiendo del numero K elegido cambia completamente el resultado del algoritmo.
		
			La eleccion depende de una metrica concreta. La metrica se llama la "Suma de los Cuadrados del Centro del Cluster" (WCSS). La formula es la suma de los cuadrados de las distancia de cada punto con respecto al cluster que pertenece (al centro geometrico del cluster). Luego, se suman todos los resultados de cada cluster.
			
			WCSS1 = Σ d(Pi, C1)^2
					Pi E Cluster 1
				
			WCSS2 = Σ d(Pi, C1)^2  +  Σ d(Pi, C2)^2  
					Pi E Cluster 1	 Pi E Cluster 2	 
		
			WCSS3 = Σ d(Pi, C1)^2  +  Σ d(Pi, C2)^2  + Σ d(Pi, C3)^2  
					Pi E Cluster 1	 Pi E Cluster 2	   Pi E Cluster 3
					
			Se busca minimizar el WCSS. A medida que se aumenta el numero de cluster las Sumas de los Cuadrados (WCSS) se reducen.
					
			¿Hasta donde se disminuyen? 
			 
			El decremento no sera constante. Al principio decrece rapidamente pero existira un momento en que al agregar mas cluster, los puntos estan tan cerca que no existe una reduccion o mejora considerable. Se traba de buscar el mejor equilibrio, ya que en un extremo puedo tener tantos cluster como puntos (esa es la menor distancia) pero no sirve para el modelo. Se busca al agregar un nuevo cluster una disminucion palpable de la metrica WCSS. Para ello, se usa la tecnica del codo, el cual es un metodo visual que se basa en decidir a partir de que punto, añadir un nuevo cluster a nuestro algoritmo de K-means no aporta una mejora. Es decir, disminuir la suma de los cuadrados de las distancias entro los puntos y el cluster al que pertenecen. 
			Se hace el grafico del codo y de forma visual detectar el cambio en la tendencia de disminucion del valor WCSS. 
			Nro. optimo de cluster = Tecnica del Codo. A veces es dificil detectar el numero K correcto en el grafico. No existe una formula matematica perfecta que permita sacar el valor correcto de K. 	La Tecnica del Codo es la mas pactada que funciona. Al final, es nuestra desicion como Analista elegir el K numero de cluster.
			
			# Método del codo para averiguar el número óptimo de clusters
			from sklearn.cluster import KMeans
			wcss = []
			for i in range(1, 11): # Hace de 1 a 10
				kmeans = KMeans(n_clusters = i, init = "k-means++", max_iter = 300, n_init = 10, random_state = 0)
				kmeans.fit(X)
				wcss.append(kmeans.inertia_) #kmeans.inertia_: Trae la Suma de los Cuadrados de las distancias(WCSS).
				
			# Parametros:
			# n_clusters: Nro. de cluster
			# init: inicializacion de los baricentro. Como no queremos caer en la trampa usamos el k-means++.
			# max_iter: Puede ser que el algoritmo no finalice, moviendo el centro de un lado al otro. Se recomienda indicar el numero maximo de iteraciones. Indica que luego de n iteraciones finalice el algoritmo. Default 300. 
			# n_init: Inicializacion aleatoria. Defecto 10.
			
			plt.plot(range(1,11), wcss)
			plt.title("Método del codo")
			plt.xlabel("Número de Clusters")
			plt.ylabel("WCSS(k)")
			plt.show()
			
			
				
			
