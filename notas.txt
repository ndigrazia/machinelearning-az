https://docs.google.com/spreadsheets/d/1I6hcZlYZqGIQEsnKMoXMVsccITrSsbFdnLyJyAk55-Q/edit?pli=1#gid=119576526

https://github.com/joanby/tensorflow2

Machine Learning de la A a la Z (LIBRO)
https://joanby.github.io/bookdown-mlaz/clasificaci%C3%B3n.html#regresi%C3%B3n-log%C3%ADstica

Machine Learning A-Z: Download Codes and Datasets
https://www.superdatascience.com/pages/machine-learning

Repositorio del Curso Machine Learning de A a la Z: R y Python para Data Science
https://github.com/joanby/machinelearning-az

https://discord.com/invite/Gq5NX6a	

Bienvenido a la Parte 2: Regresión
Los modelos de regresión (tanto lineal como no lineal) se utilizan muchísimo para predecir valores numéricos como por ejemplo el sueldo. Si nuestra variable independiente es tiempo entonces podemos hacer predicciones de valores futuros, sin embargo nuestro modelo puede predecir también valores desconocidos del presente.  Las técnicas de Regresión son muy variadas, desde la regresión lineal hasta la SVR o la Regresión con Bosques Aleatorios.

En esta parte, vamos a entender e implementar los siguientes modelos de Regresión dentro del Machine Learning:

Simple Linear Regression

Multiple Linear Regression

Polynomial Regression

Support Vector for Regression (SVR)

Decision Tree Classification

Random Forest Classification

-----------
Regression
-----------

Modelos de regresion lineal

Modelos de regresion lineal tienen una series de restricciones que se deben comprobar antes de aplicar el modelo de regresion:
	-linealidad
	-homocedasticidad
	-Normalidad multivariable
	-Independencia de errores
	-ausencia de multicolinealidad
	
Si no se da alguna de las restricciones anteriores el modelo de regresion lineal no tiene sentido.

-	Simple Linear Regression (SLR)

	En el grafico las variables independientes van en el eje de las x y la dependiente en el eje de las Y.

	y = b0 + b1 * x1
		b0 constante (ordenada all origen)
		b1 coficiente
		x1 variable independiente
		y variable dependiente (a predecir)
		
		Para identificar la recta se usa el "metodo de los minimos cuadrados"
						  ^   2
			min	SUM (yi - yi)
				   i
		(SLR) no requiere scalado
		
		La recta de regresion es la misma para los datos entrenados como los datos de test. Se deja el conjunto de entrenamiento.
		

- Multiple Linear Regression	

		(MLR) no requiere scalado
		
		y = b0 + b1 * x1 + b2 * x2 + ... + bn *xn
			b0 constante (ordenada all origen)
			bi coficientes
			xi variables independientes
			y variable dependiente (a predecir)
			
			notar:
				bi coficientes positivos(+) aportan valor a la y 
				bi coficientes negativos(-) restan valor a la y 
			
			Dummy: propiedad  categorica (no ordinal). Ejemplo: Ciudades. 
				
			Es contra producente agregar todas las columnas dummy. Se deben añadir todas las columnas dummy salvo una. Evita el efecto de multicolinealidad.
			
			y = b0 + b1 * x1 + b2 * x2 +  b4 * d1
				d: variable dummy
			
			Trampa de variables dummy:
			
				Al incluir todas las variables dummy obtenemos una multicolinealidad. Esto debe evitarse. El modelo es incapaz de determinar los efectos de todas las variables dummy
					y = b0 + b1 * x1 + b2 * x2 +  b4 * d1 + b5 * d2
							El modelo no puede determinar los efectos o impacto de d1 y de d2.
							d2 = 1 - d1
				
				SIEMPRE DEBEMOS OMITIR UNA VARIABLE Dummy. Esto evita el efecto de multicolinealidad.
				
				Si tuvieramos dos variable dummy, deberia quitar un valor para cada variable dummy. Por ejemplo, pais y sector. Quito un valor (de todos los valores posibles) para pais y uno (de todos los valores posibles) para sector.	
				
				https://www.wikihow.com/Calculate-P-Value
				https://www.mathbootcamps.com/what-is-a-p-value/
				
				Entre varios modelos con variables independientes, preferimos el que tenga la menos cantidad de variables independientes que ayuden a predecir la variable dependiente.
				 Porque?
					1- Puede ser que añadir mas variables no aporte a predecir la variable dependiente.
					2- Mayor variables independientes hace que complejo el modelo. (dificulta la explicacion del modelo)
					
					SOLO MANTENER LAS VARIABLES INDEPENDIENTES IMPORTANTES CON CAPACIDADES DE PREDECIR EL VALOR DE LA VARIABLE DEPENDIENTE. SE debe seleccionar las variables importantes capaces de predecir algo:
						5 Modelos de seleccion:
						https://www.youtube.com/watch?v=tCXc2zl3dew
							1- Exhaustivo (all-in).
							2- Eliminacion hacia atras.   -------------
							3- Seleccion hacia adelante.			   --------	Regresion paso a paso los incluye 
							4- Eliminacion bidireccional.  ------------
							5- Comparacion de scores.
												
							1- Exhaustivo (all-in)
				
								La usamos por:
								
								1. Metemos todas la variables y vemos que sucede, ya que conocemos (por el conocimiento del negocio o por necesidad) que todas son predictoras.
								
								o
								
								2. Preparacion previa a la Eliminacion hacia atras. 	
									
			
							2- Eliminacion hacia atras. (Backward)
								Elimina cualquier variable independiente que no sea significativa.
									Paso 1. 
									Elegimos un nivel de significacion para que una variable permanezca en el modelo. (Standard 0.05)
									
									Paso 2.
										Se calcula el modelo usando todas las variables como predictoras.
										
									Paso 3.
										Se considera la variable predictora con el p-valor mas grande. Si p-valor > Standard vamos al paso 4 sino a fin.
										
									Paso 4.
										Se elimina la variable con p-valor > Standard
									
									Paso 5.
										Se ajusta el modelo sin la variable con p-valor > Standard
									
									FIN - El modelo esta listo!
									
									Repetimos del paso 3 al paso 5 hasta no tener variables con p-valor > Standard. Nos quedamos con ese modelo.
							
									WARNING: Para aplicar esta seleccion en Python, se agrega una columna de todos unos a las columnas de variables independientes. Esta representa la ordenada al origen. PAra analizar si tiene significado la ordenada al origen en la funcion a calcular.
							
							3- Seleccion hacia adelante. (Forward)						
									Paso 1. 
									Elegimos un nivel de significacion para que una variable pueda entrar en el modelo. (Standard 0.05)
									
									Paso 2.
										Ajustamos todos los modelos de regresion lineal simple y ~ Xn (hacemos todos los modelos de regresion linial simple para cada variable independiente con la dependiente). Elegimos el que tiene menor p-valor.
									
									Paso 3.
										Se conserva esta variable, y ajustamos todos los posibles modelos con una variable extra añadida a la(s) que ya tenga(s) el modelo hasta el momento.
										
									Paso 4.
										Se considera la variable predictora con el menor p-valor. Si p-valor < Standard vamos al paso 3 sino a fin.
										
									FIN	- Conservar  el modelo anterior.
									
									Repetimos el paso 3 hasta  que la variable añadida supere el p-valor.  Nos quedamos con el modelo anterior.
									
							4- Eliminacion bidireccional (stepwise)
							https://www.youtube.com/watch?v=6wpTEwaFbY0
									Combina: Seleccion hacia adelante y Eliminacion hacia atras.
									
									Paso 1. 
										- Elegimos un nivel de significacion para que una variable pueda entrar en el modelo. (Standard 0.05) SLENTER
										- Elegimos un nivel de significacion para que una variable permanezca en el modelo. (Standard 0.05) SLSTAY
										
									Paso 2.
										Inicia como forward.
										Llevar a cabo el Paso de selccion hacia adelante (con las nuevas variables con p < SLENTER para entrar)
										
									Paso 3. 
										Se lleva adelante la eliminacion hacia atras (variables antiguas con p-valor < SLSTAY para quedarse)
										
										Repetir de paso 2 hasta 3 que no queden variables para añadir.
										
									El mejor de los tres modelos.
									
									FIN- Modelo esta listo.
									
							5- Fuerza bruta: (Comparacion de scores.)
							
									Genera todos los modelos de regresion posibles. Todos los de una variable, Todos los de dos variables, Todos los de test variables, etc.
									
									Paso 1.
									Seleccionar un criterio de bondad de ajuste. Ej: Vallesiano, Akaike,etc.
									
									Paso 2.
									Construir todos los modelos de regresion posibles. (2 elevado a la N) - 1. N numero de variables. Por ejemplo: Con 10 variables -> (2 elevado a 10) - 1 = 1023 modelos.
									
									Paso 3.
									Seleccionamos el modelo basado el criterio elegido. El mejor modelo.
									
									FIN- Modelo esta listo.
									
									
							Conclusion:
									No hay un modelo mejor que otro. Hay que conocer todos los metodos.
									
									Si cualquiera de los coeficientes es cercano a cero no es necesario agregarlo a la formula de regresion.
									 OLS-> ordinary list square (minimo de cuadrados ordinarios)
								
									Cuanto mas cercano es el R-Cuadrado Ajustado al 1 (uno) mejor se explica el modelo de la regresion lineal (Es el mejor modelo)
									R-Cuadrado Ajustado > 0.7 se suele aceptar qu el modelo es lineal.

									WARNING: No Dividir el conjunto de datos entre entrenamiento y testing cuando son pocos casos o datos.
									
Regresión Lineal Múltiple en Python - Eliminación hacia atrás automática

Si estás interesado en implementaciones automáticas de la Eliminación hacia atrás en Python, aquí te presentamos dos de ellas. Se ha adaptado el código para que utilice la transformación .tolist() sobre el ndarray y así se adapte a Python 3.7.

Eliminación hacia atrás utilizando solamente p-valores:

import statsmodels.formula.api as sm
def backwardElimination(x, sl):    
    numVars = len(x[0])    
    for i in range(0, numVars):        
        regressor_OLS = sm.OLS(y, x.tolist()).fit()        
        maxVar = max(regressor_OLS.pvalues).astype(float)        
        if maxVar > sl:            
            for j in range(0, numVars - i):                
                if (regressor_OLS.pvalues[j].astype(float) == maxVar):                    
                    x = np.delete(x, j, 1)    
    regressor_OLS.summary()    
    return x 
 
SL = 0.05
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_Modeled = backwardElimination(X_opt, SL)

Eliminación hacia atrás utilizando  p-valores y el valor de  R Cuadrado Ajustado:

import statsmodels.formula.api as sm
def backwardElimination(x, SL):    
    numVars = len(x[0])    
    temp = np.zeros((50,6)).astype(int)    
    for i in range(0, numVars):        
        regressor_OLS = sm.OLS(y, x.tolist()).fit()        
        maxVar = max(regressor_OLS.pvalues).astype(float)        
        adjR_before = regressor_OLS.rsquared_adj.astype(float)        
        if maxVar > SL:            
            for j in range(0, numVars - i):                
                if (regressor_OLS.pvalues[j].astype(float) == maxVar):                    
                    temp[:,j] = x[:, j]                    
                    x = np.delete(x, j, 1)                    
                    tmp_regressor = sm.OLS(y, x.tolist()).fit()                    
                    adjR_after = tmp_regressor.rsquared_adj.astype(float)                    
                    if (adjR_before >= adjR_after):                        
                        x_rollback = np.hstack((x, temp[:,[0,j]]))                        
                        x_rollback = np.delete(x_rollback, j, 1)     
                        print (regressor_OLS.summary())                        
                        return x_rollback                    
                    else:                        
                        continue    
    regressor_OLS.summary()    
    return x 
 
SL = 0.05
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_Modeled = backwardElimination(X_opt, SL)


Instalar la nueva versión de ElemStatsLearn en R
Los que tengáis problemas al instalar ElemStatLearn desde R probad con esta línea de código

install.packages("https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/ElemStatLearn_2015.6.26.2.tar.gz",repos=NULL, type="source")
									
						
Regresión Lineal Múltiple en R - Eliminación hacia atrás automática
Si quieres tener una implementación automática de la eliminación hacia atrás en R, aquí te la dejo:

backwardElimination <- function(x, sl) {
  numVars = length(x)
  for (i in c(1:numVars)){
    regressor = lm(formula = Profit ~ ., data = x)
    maxVar = max(coef(summary(regressor))[c(2:numVars), "Pr(>|t|)"])
    if (maxVar > sl){
      j = which(coef(summary(regressor))[c(2:numVars), "Pr(>|t|)"] == maxVar)
      x = x[, -j]
    }
    numVars = numVars - 1
  }
  return(summary(regressor))
}
 
SL = 0.05
dataset = dataset[, c(1,2,3,4,5)]
backwardElimination(training_set, SL)


- Regresion Polinomica (RP)

	Esta regresion tiene una unica variable independiente.-

	Regresion lineal -> 	y = b0 + b1X1
	Regresion multiple -> 	y = b0 + b1X1 + b2X2 + ... + bnXn
	Regresion polinomica -> y = b0 + b1X1 + b2X1^2 + ... + bnX1^n (tenes una misma variable (x1) con potencias sucesivas) 
	
	Cuando usar:
	
	Cuando los datos relevados siguen una parabola o una curva (no una recta).
	
	Regresion polinomica de grado 2 -> y = b0 + b1X1 + b2X1^2
	
	En la RP debemos elegir hasta que grado queremos. Grado 2 o Grado 3 o ... o Grado n. El grado tiene que ver con el exponente o potencia.
	
	¿Porque se llama Regresion lineal Polinomica?
		Es lineal por los coeficientes. Los coeficientes tienen una relacion lineal con las variables. 
		En la regresion lineal las incognitas son los coeficientes.
		La variable y se predise como combinacion lineal de esos coeficientes con las variables.
		

	WARNING: No se escalan los datos porque la relacion no lineal podria perderse.
	
	poly_reg = PolynomialFeatures(degree = 4) - degree representa grado del polinomio . Hasta que grado deseo tener las caracteristicas polimonialies de mi matriz original. Se elige el deseado.
	
	# Training the Linear Regression model on the whole dataset
	from sklearn.linear_model import LinearRegression
	lin_reg = LinearRegression()
	lin_reg.fit(X, y)

	# Training the Polynomial Regression model on the whole dataset
	from sklearn.preprocessing import PolynomialFeatures
	poly_reg = PolynomialFeatures(degree = 2)
	X_poly = poly_reg.fit_transform(X) 
	lin_reg_2 = LinearRegression()
	lin_reg_2.fit(X_poly, y)    
	
	El codigo anterior genera una matriz llamada X_poly con tres columnas. La primera el termino independiente (ordenada al origen - todos unos), la segunda columna el valor de la variable independiente y la tercer columna el cuadrado de la variable independiente.
	
	WARNING: En un modelo polinomial nunca se representa una recta.- Es una Parabola o curva. 
	
	
-	Regresion con Maquinas de Soporte Vectorial (SVR)

	Estos métodos están propiamente relacionados con problemas de clasificación y regresión. 

	Sirven para Regresiones lineales (Kernel lineal )como no lineales (rbf, etc...). Dependen de un nucleo (Kernel). El Kernel determina el tipo de regresion que lleva a cabo la maquina de soporte vectorial.
		
	Para regresion hay que ajustar el mayor corredor (o calle) posible entre dos clases. Intentando que en el interior de ese pasillo o clase queden la mayoria de los puntos. Hay que limitar la anchura (margen maximo). 
	Imaginemos una recta o calle entorno a la misma dos pasillos (uno por arriba y otro por debajo) donde queden la mayoria de los puntos. 
	
	En el caso de clasificacion se intenta que la anchura del pasillo separe las dos clases lo mas posible. En la Regresion encontrar la anchura que incluya la mayoria de los puntos. 

	Como controlamos la anchura del pasillo. Se establece un parametro llamado epsilon. A mayor valor de epsilon mayor anchura.	
	
	epsilon: Distancia entre las lineas de soporte vectorial y la linea de hiperplano. 
	
	La calle se llama hiperplano. Hay que obtener la ecuacion del hiperplano. Los puntos que hacen cero a la ecuacion estan en el hiperplano. Si el son distintos de cero partenecen a una clase o a otra. Para que funcione hay que encontrar la ecuacion que represente al mejor hiperplano (aquel que permite que no haya sesgo entre una u otra categoria). Es decir que permita realizar la mejor separacion. Las maquinas de soporte vectorial ayudan a encontrar el mejor hiperplano.
	
	El algoritmo detecta los puntos mas cercanos entre una clase y otra. Luego, encuentra la linea que los conecta. Finalmente, traza una perpendicular que divide esta linea en dos. Esta linea es el hiperplano optimo. Se busca maximizar el margen. 
	
	Vocabularion:
	
		Hiperplano = recta de regresion.
		
		Los vectores de soporte: los puntos mas cercanos entre una clase y otra. 
		Margen: distancia entre los vectores de soporte.
		Hiperplano optimo: frontera de separacion que consige la mayor separacion entre una clase y otra. 
		
		Problema:
			Si aparece un dato excepcional o valor a tipico (outlier) que lleva a reducir el margen y por lo tanto a producir un overfitting (clasificacion incorrecta de un dato nuevo por el algoritmo). Para solucionar este problema al alogritmo tradicional (Hard margin) se le agrega un parametro C (Soft margin) para flexibilizar el margen. Este valor de C es elegido por el diseñador en el entrenamiento. A menor C  mayor margen o a mayor C menor margen. El valor se escoge de manera emperica analizando el error obtenido en la clasificacion comparado con diferentes valores de C. Se escoge el menor error posible. Es decir, menor C.
			
	La idea de las Maquinas de Soporte Vectorial es obtener el mayor margen posible entre una clase y otra.
	
	Problema: 
		Maquinas de Soporte Vectorial (SVR) permiten obtener fronteras de clasificacion o hiperplanos lineales. A veces es necesario en aplicacion reales obtener hiperplanos o fronteras no lineales. Como solucion una alternativa es agregar mas dimensiones a cada dato para separar las dos categorias. Es decir, usamos Maquinas de Soporte Vectorial para obtener hiperplanos en mas dimensiones. Como agregamos mas dimensiones a los datos para poder clasificarlos: Kernel.
		
		Podemos pensar que cada punto de datos de entrenamiento representara su propia dimension. 
		
		Kernel: Toma el dataset de datos original y lo mapea a un espacio de mayor dimensiones usando una funcion no lineal. Con esta tranformacion el dataset es linealmente separable. Luego, se aplica Maquinas de Soporte Vectorial (SVR) para obtener el hiperplano optimo. Luego, obtenido el hiperplano optimo se vuelve al espacio original y finalmente, se realiza la clasificacion. Se puede representar la funcion en el espacio original.
		
		Kernel: Mapea y realiza el calculo del hiperplano. Para el mapeo usa funciones polinomiales o de gaussianas.
		
		Las Maquinas de Soporte Vectorial sirven para aumentar la dimensionalidad del problema y calcular la regresion en ese espacio de dimension superior. Luego ese funcion se puede volver al espacio de dimension original y proyectar la linea. 
		
		Objetivo de las Maquinas de Soporte Vectorial es encontrar una funcion de regresion a partir de los puntos de entrenamientos y que los errores no superen el umbral establecido (el epsilon)
		
	Algoritmo SVR:
	
		Analizar si se escalan los datos de X (independientes) y Y (dependiente)
	
		1. Elegir el conjunto de entrenamiento. (variables independiente y dependiente)
		2. Elegir la funcion de nucleo y sus parametros. Tambien, analizar si hay que realizar una regulacion adicional que elimine el ruido en el conjunto de entrenamiento. 
		3. Crear la matriz de correlaciones (K).  
		4. Entrenamos el modelo en forma exacta o aproximada para obtener los coeficientes de contraccion para cada uno de los datos.
		5. Con los coeficientes de contraccion creamos un estimador.	
		
		Se debe elegir el nucleo:
				- Lineal
				- No lineal - (Gaussiano - Es el defecto) Es el la constante rbf en python.
				
		Regulacion para eliminar el ruido de los datos.
		
		IMPORTANTE: Considerar escalar ya que el algoritmo se basa en distancias euclideas.
			
- Regresion con Arboles de desicion

		Los saltos en los arboles son discretos. Hay saltos en el diagrama del arbol.
		
		Puede tener varias variables independientes.
		
		En los arboles de desicon no suele ser necesario ningun tipo de escalado.
		
		Escalar si el algoritmo usa distancias euclideas. EL ARbol de desicion no utiliza distancias euclideas.
		
		
		CART
			Classification & Regression Trees
				Hay dos Tipos:
					---> Arboles de Clasificacion
					---> Arboles de regresion para predecir valor.
					
		Arboles de Regresion:
				 Dos variables independientes x1 y x2. Objetivo predecir una tercer variable dependiente Y.
				 
				 Predice no necesariamente de forma lineal. 
				 
				 El algoritmo divide los puntos (nuestros datos) en un conjunto de secciones. 	
				 El algoritmo mira la entropia. Como de juntos o dispersos estan los puntos (similitudes entre los puntos)-
				 Agrupa los puntos en comunes basado la entropia. Hay que establecer los puntos que se quedan en un nodo hoja (Ejemplo 5% de datos).
				 Se puede usar otra regla para definir los puntos que quedan en la hoja.
				 El algoritmo encuentra las divisiones optimas del conjunto de datos.
					
				 Alg ejemplo:
				 
					1. desicion X1 < 20
							Si -> X2 < 200
									Si -> 
									No -> 
							No -> X2 < 170
									Si -> X1 < 40 
											Si -> La proyeccion o prediccion es el promedio de los puntos que conforman la hoja o seccion de division. Para un mismo nodo hoja retorna la misma prediccion.
											No -> 
									No -> 
		Python:
				Objeto en python para representar un arbol de desicion es DecisionTreeRegressor.
				Cuando se crea se establece el criterios de division de los datos. Se suele utilizar la medida del "error cuadrado medio" como forma de minimizar la diferencia entre la prediccion y el resultado. Este criterio busca cual de las formas de cortar minimiza los cuadrados de los errores. Este es el criterio por defecto.
				
				regressor = DecisionTreeRegressor(random_state = 0)
				
				Enfoques mas avanzados suelen definir enfoques que corten por mas de un rasgo a la vez. En el constructor del objeto DecisionTreeRegressor se definen las caracteristicas o parametros del arbol de desicion. Tambien se puede definir el numero maximo de nodos hojas, numeros maximos de elementos que forman el nodo hoja, etc. Elige que parametros pasar al objeto.
				
				
	WARNING: hay que escalar o no? Por defecto no escalar y ver que buena es la prediccion y luego, ver como es la prediccion con los datos escalados. Escalar si el algoritmo usa distancias euclideas. No se usa en Arboles de desicion. 
	
	Cuando tenes una linea horizontal (en grafico) que abarca todo el conjunto de datos podria pasar que los datos estan mal interpretados. Puede ser que el arbol tuviera unas restricciones a la hora de dividir una rama en nodos hojas. Es decir, que una rama no se divide en nodos hojas si no hay suficientes datos para formar parte del nodo hoja. Tambien, puede ocurrir que no se dividen las ramas a menos que se establezcan ciertas condiciones o las condiciones de division estan mal establecidas. Por lo tanto, se agrupan muchos datos en una sola hoja y se otorga el promedio de todos los datos de la hoja a las proyecciones que caen en ese nodo hoja. Con parametros se puede cambier los criterios de division de ramas. En R npar.control(minsplit=1) Establece que una rama puede tener un dato para en su hoja. 

	 		
- Regresion con Bosques aleatorios

	Se usan para regresion y para clasificacion.
	
	los saltos en los arboles son discretos.
	
	Los arboles cuando hacen la prediccion lo hacen sobre un conjunto discreto de datos.

	Este algoritmo entra en Aprendizaje en Conjunto. Basicamente, toma un algoritmo y lo replica n veces o toma varios algoritmos diferentes y los junta para obtener un mejor algotimo.  Aprendizaje en Conjunto permite tener algoritmos mas estables.
	
	Muchos arboles de desicion conforman un bosque aleatorio.
	
	En este caso toma un algorimto de arboles de desicion y los replica n veces.
	
	Pasos:
	
	Paso 1. Se elige un numero K (numero aleatorio). Representa un subconjunto de los datos del conjunto de datos de entrenamiento. En lugar de dividir los datos en conjunto de entrenamiento y conjunto de test. Elegimos un subconjunto de datos de los datos de entrenamiento.
	
	Paso 2. Construir un arbol de desicion para los K puntos de datos. Lo que hace que el arbol tenga una vision parcial de los datos.
	
	Paso 3. Se define el numero de arboles de desicion y se repite el paso 1 y 2. Cada nuevo arbol tiene un subconjunto de datos del total de datos de entrenamiento.
	
	Paso 4. Cuando llegamos a construir el numero de arboles deseados. Se puede usar todo el bosque para hacer la prediccion. A cada arbol se le indica que realice la prediccion de Y. Finalmente, la prediccion final es el promedio de las predicciones Y dadas por los arboles construidos.
	
	Este algoritmo mejor la prediccion de un solo arbol de desicion. 
	
	Cuando hay valores outlier (valores extremos) podemos tomar la mediana en lugar de la media en cada arbol. Tambien se puede usar la media recortada.
		
	Python:
		RandomForestRegressor(n_estimators = 10, random_state = 0)	
			Parametros de ForestRegressor
					n_estimators -> cantidad de arboles del bosque.
					criterion -> criterio para dividir una rama en dos subramas o nodos hojas.
								Criterio de division: MSE (error cuadrado medio) diferencia entre la prediccion y el valor real al cuadrado. Se intenta minimizar. Se divide la rama en dos subramas intentando que el MSE de cada rama sea menor al MSE de la rama que se divide. Tambien, hay otros criterios de division.
					max_features -> caracteristicas tenidas en cuenta a la hora de generar las divisiones.
					random_state -> reproductivilidad para obtener el mismo bosque.
					
			
	
	 IMPORTANTE: Cuando se conbinan diferentes modelos o algoritmo (Aprendizaje en Conjunto) en varias maquinas sale a relucir mejores predicciones.
	 

Generalidades:

	 R Cuadrado:
	 
		Buscamos el modelo que predise el menor error posible con respecto a los datos dados.
		
		R 2 = 1- SSE/SST 
		
		SSE es la suma de cuadrados de error o residuos - Se calcula sumando las diferencias al cuadrado entre los valores predichos y los valores reales de la variable dependiente.
						
		SST es la suma de cuadrados total - Es la suma de los cuadrados de las diferencias entre cada punto de datos (valores reales de la variable dependiente-valores observados) y la media de todo el conjunto de datos
		
		Cuanto mejor sea mi modelo mas cerca estara R Cuadrado del numero uno. Mide que tan bueno es el modelo que elijo con respecto a una prediccion de media para todos los valores del conjunto de datos.
		
		Si el R Cuadrado es negativo el resultado es un pesimo modelo de prediccion.
		
		Los algoritmos o tecnicas de regresion se enfocan en mejorar el R Cuadrado para indicar lo bueno que es mi modelo para predecir los datos.
		
		En R se llama "multiple R-squared"
		
		El valor mas grande que puede tomar es uno. 
		
		R cuadrado no es un estimador muy exacto de la bondad de ajuste para algoritmos de variables multiples. Siempre sera mejor o mayor cuanta mas variables agreguemos. Necesitamos usar un valor u otro indicador que no este sesgado. Usamos R Cuadrado Ajustado. (regresion lineal multiple)
	
	 R Cuadrado Ajustado:
	
		Regresion Lineal multiple: El R Cuadrado se sesga y no sirve para medir o evaluar modelos con numero de variables diferentes. Es decir, si agregar una variable mas al modelo lo mejora y en consecuencia, mejora la prediccion. (R2 nunca decrece, se incrementa). En ese caso hay que usar el R Cuadrado Ajustado para evaluar si agregar una variable mejora nuestro modelo.
		
		Adj R 2  = 1 - (1 - R2) * (n - 1) / (n - p - 1)
		
		R2 es el R cuadrado
		n tamaño de la muestra.
		p numero de variables independientes (o variables regresoras) en nuestro modelo.
		
		Al agregar mas variables independientes hace que el R 2 decrezca, ya que penaliza el cociente.	
		
		Adj R 2 Evalua si gana mas el valor del (1 - R2) o el cociente ((n - 1) / (n - p - 1)) por agregar esa nueva variable.
		
		
		A veces el umbral de rechazo de una variable para pertenecer al modelo (regresion multiple), el p valor, es muy cercano al nivel que tomemos como umbral. Si no señimos al algoritmo deberiamos rechazar la varible pero como su valor es muy cercano podriamos tener presente otros valores. Dar una segunda opinion si eliminamos o no ese variable.  
		
		Nos quedamos con el modelo de mayor R Cuadrado Ajustado cuando comparamos varios modelos en regresion multiple para determinar si agregar o eliminar una variable aporta al modelos. Cuando comenzas a sacar variables del modelo y cae el R Cuadrado Ajustado debemos para de eliminar variables (metodo de reduccion hacia atras).
		
     
	 Interpretar Coeficientes de Regresion Lineal
	 
		1. Signo del coeficiente. (Negativo) aporta negativamente a la prediccion.  (Positivo) aporta positivamente a la prediccion.  
		2. La magnitud del coeficiente. (Ojo con las unidades de los coeficientes) Analizar si son diferentes sus escalas. SIEWMPRE Analizar las unidades de los coeficientes. Hacer transformaciones para que todo cobre sentido.
		
		Interpretar el coeficiente como el incremento o decremento "por unidad de" ... (Dolar, Empleado, distancia, etc.). La unidad de la variable independiente que aporta positivamente o negativamente al modelo.

Fin de la Parte 2 - Regresión

¿Cómo sé qué modelo debo elegir para resolver mi problema?

Paso 1.

Averiguar si el problema es una prediccion (regresion), clasificacion o agrupamiento (clustering). Para ello analizamos la variable dependiente.

Si no existe la variable dependiente es un agrupamiento.
Si tiene la variable dependiente y la variable es continua o discreta es una regresion. Finalmente, si la variable dependiente es categorica es una clasificación.

Paso 2. Preguntarse si tu problema es o no es lineal. Se puede responder con la tecnica Grid Search.

Una vez lo sepas, Si tu problema es lineal, deberás intentar crear un modelo de Regresión Lineal Simple si solo tienes una variable independiente o un modelo de Regresión Lineal Múltiple en el caso de tener varias.

Si tu problema no es lineal, entonces tenemos varias técnicas donde elegir, como la Regresión Polinómica, SVR, Árboles de Decisión y Bosques Aleatorios. ¿Cuál te funcionará mejor? El método en este caso consiste en utilizar una técnica muy útil para evaluar modelos llamada k-Fold Cross Validation, y elegir el modelo que demuestre mejores resultados. 
 
¿Cómo puedo mejorar cada uno de estos modelos?

Existen dos tipos de parámetros en nuestros modelos:

- los parámetros que el modelo aprende, como los coeficientes de la Regresión Lineal,

- los hiper parámetros del algoritmo. En este último caso, los hiper parámetros son parámetros que el algoritmo no aprende, si no que son fijos y forman parte de las ecuaciones de los modelos. Por ejemplo, el parámetro lambda de regularización o el factor de penalización C son hiper parámetros.

---------------------
Seleccion de modelos.
---------------------

- - - 
K-Fold Cross Validation

Se utiliza para proporcionar una evaluacion relevante de la eficacia de nuestro modelo. 
Permite comparar diferentes algoritmos (Logistica, vectorial, KNN)y obtener una idea de cuan bien trabajaran.

Incrementa el rendimiento del modelo.

En la practica es comun dividir los datos en 10 bloques (Fold) - Python (cross_val_score (cv = nro. bloques)

Se muestrea varias veces nuestro dataset de partida. Incrementa el rendimiento del modelo porque todos los datos se usan para entrenar y evaluar.

Juzgar el rendimiento de nuestro modelo con un solo conjunto de datos de prueba no es el mejor enfoque (problema de varianza de los datos). No es la mejor forma de evaluar el rendimiento de un modelo.  K-Fold Cross Validation soluciona el problema de las varianzas entre un conjunto de datos de prueba.

Compensar el sesgo de la varianza

	Sesgo Bajo: diferencia baja entre la prediccion y el valor real observado. 
	Sesgo Alta: diferencia alat entre la prediccion y el valor real observado. 
		
	Varianza baja: Ejecutamos varias veces el modelo y la prediccion no varia demasiado.
	Varianza alta: Ejecutamos varias veces el modelo y la prediccion varia demasiado.
	
	En Python el resultado de aplicar este modelo es un array con la presicion de cada una de las corridas. Resultado final es la media de los resultados obtenidos y el desvio estandard.
	
- - - 
Grid search

Optimiza los hiperparametros. 

Nos permite identificar si es mejor elegir un modelo lineal o no lineal.

Trata de encontar los valores optimos de los hiperparametros. Hiperparametros son los que elige el usuario.

Realizamos primero la evaluacion deñ renmdimiento del algoritmo (K-Fold Cross Validation) y luego, ajustamos los hiperparametros para optimizarlos.

La optimizacion depende de los parametros del algoritmo utilizado. DEbe elegir los parametros que quiero optimizar y para ello debo conocerlos desde el algoritmo que voy a emplear. WARNING : No exagerar con la cantidad de parametros.


-------------
Clasificacion
-------------

Importante: Los algoritmos con distancia euclideas hay que escalarlos.

A diferencia de la regresión donde se predice un valor continuo, se utiliza la clasificación para predecir una categoría. Existen una gran amalgama de aplicaciones del proceso de clasificación desde medicina hasta marketing. Los modelos de clasificación incluyen desde modelos lineales como la Regresión Logística, SVM, así como otros no lineales como K-NN, Kernel SVM y Bosques Aleatorios.

Clasificar y catalogar.

- Regresion Logistica

	Es buena cuando los datos pueden ser separados por medio de una recta.
	
	Busca el mejor separador lineal recto (la mejor recta) posible en el proceso de clasificacion.
	
	En el grafico las variables independientes van en el eje de las x y la dependiente en el eje de las Y.
	
	Se usa para clasificacion. Es un clasificador lineal. Devuelve probabilidades.
	
	Se  predice una accion. Por ejemplo, compra o no compra. Abre o no abre.
	
	Accion: Comprar o no comprar un producto.
			Abrir o no abrir un correo electronico. 
			
			Es decir llevar a cabo una accion.
			
			La logistica utiliza la regresion lineal, en 2D (Sueldo y Edad por ejemplo), el separador es una recta. En 3D el separador es un plano. En multiple dimensiones el separador lineal es un hiper plano.
			
			Ejemplo, 
				Compra en funcion de la edad
				Abre el correo en funcion de la edad
				
			En lugar dee predecir lo que vaa suceder podriamos predecir que tan probable es un cliente acepte la oferta.
			Probabilidad entre cero y uno. Todo lo que esta por debajo de cero o cero sera no compra, todo lo que sea uno o supere a uno sera compra. Lo intermedio sera un probabilidad de que compre o no. La idea es encontrar esas franjas de edades y cuantificar la probabilidad que compre o no. 
			
			No se puede usar la regresion lineal porque los datos no siguen esa funcion. Si en lugar se le aplica una funcion sigmoide (por ejemplo para predecir la venta dada la edad) para despejar el valor de la variable Y, obtendriamos:

				funcion lineal 
			     		y = b0 + b1 * x
				
				funcion sigmoide 
							p = 1 /(1 + e^-y) 
				
					al despejar la y obtendriamos:
						 p = 1 /(1 + e^-y) => y = ln(p/1-p) 
						
				Finalmente, 
				
					ln(p/1-p) = b0 + b1 * x
								
			En resumen, a una regresion lineal se le aplica una funcion sigmoide para trasnformar el valor final de la prediccion en una probabilidad.
			
				Regresion logistica o sigmoide
						ln(p/1-p) = b0 + b1 * x
							
							Es un tuneo  de una regresion lineal. 
							
							El eje y se trasnforma en una probabilidad. (en la regresion lineal es un prediccion)
							Probabilidad entre 0 y 1. 
							
							Se busca la funcion sigmoide (o logistica) que mejor se ajusta con respecto al conjunto de funciones sigmoides. Objetivo final, si hay probabilidad de comprar o no (Si se realiza el suceso o no)
							
							En el grafico la recta X es la variable independiente y la recta Y la probabilidad de compar o no. (Si lleva a cabo la accion o no)
							
							Se toma valores aleatorios para la variable independiente X y se analiza la probabilidad del suceso dado ese valor de X en funcion de la regresion logistica. Luego, en funcion de un "valor central" se decide si ocurre o no el suceso (si compra o no). Ese valor central generalmente es 0,5. Define si ocurre o no el suceso dada la probabilidad obtenida. El valor central se puede elegir arbitrariamente.
							
							Si la probabilidad obtenida del modelo es menor o igual que 0,5 no ocurre el suceso, de lo contrario si ocurre.
							
							En resumenn, el calculo de probabilidades es necesario como herramienta intermedia para realizar la clasificacion (compra o no).
				
				Matriz de confusion
				
						Una matriz de confusión es una herramienta que permite la visualización del desempeño de un algoritmo que se emplea en aprendizaje supervisado. Cada columna de la matriz representa el número de predicciones de cada clase, mientras que cada fila representa a las instancias en la clase real. Uno de los beneficios de las matrices de confusión es que facilitan ver si el sistema está confundiendo dos clases.
		
			En R:
			
				# Ajustar el modelo de regresión logística con el conjunto de entrenamiento.
				classifier = glm(formula = Purchased ~ .,
							data = training_set, 
								family = binomial) Es (family = binomial) binomial porque compra o no compra es lo que queremos predecir.
				
					CUIDADO - Cambios en la sintaxis de las librerías

					En la clase instalo la librería ElemStatLearn. Si te sale un error debido a cambios en la librería, debes hacer la instalación indicada en los siguiente pasos:


					https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/


					1. Descarga la librería desde este link o utilizando el enlace anterior.

					2. A continuación abre R-Studio. Selecciona el menú de herramientas y haz click en instalar paquetes.

					3. Desde 'Install from:' elige el la opción package install file(.zip; .tar,etc)

					4. Selecciona el paquete descargado anteriormente.

					También puedes instalarlo haciendo uso de la linea de comandos con la instrucción : R CMD install <pkg>.

- K-Nearest neighbors (K-NN)

	Es usado para categorizacion.
	
	No es un clasificador lineal. Los datos pueden ser separados sin la existencia de una recta o linea.
	
	Como lo hace?
	
		Cuatro pasos:
		
				Paso 1. 
						Elegir numero k de vecinos mas cercanos que tenemos en cuenta para el proceso de clasificacion.
						Elegir K en impar para no tener empate. K = 5 es el mas utilizado generalmente.
						
				Paso 2.
						Tomar los k vecinos mas cercanos del nuevo dato utilizando la distancia Euclidea. Es decir, se seleccionan los k vecinos mas cercanos utilizando la distancia euclidea.
						No es necesario utilizar la distancia euclidea, se puede usar otra (ej, manhattan, metrica del infinito, etc..) pero euclidea es la mas utilizada.
						
						La distancia euclidea entre dos puntos es basicamente el teorema de pitagora:
								Distancia entre P1 y P2  = SQRT((x2-x1)^2 + (y2-y1)^2)
						
				Paso 3.
						Entre los k vecinos contamos la cantidad de puntos que pertenecen a cada una de las categorias.
				
				Paso 4.				
						Asignar el nuevo punto de dato a la categoria que tenga mas vecinos en ella. 
						
				Paso 5.
						El modelo ya esta elavorado.
											
	En Python:
	
		from sklearn.neighbors import KNeighborsClassifier
		classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
		classifier.fit(X_train, y_train)
		
		n_neighbors: nro de vecinos
		
		metric: metrica para calcular la distancia de los puntos. (minkowski)
						(minkowski) Familia de distancias.
									
		p: (1) manhattan (2) euclideana
		

- SVN (support vector machine)

		Detecta los puntos mas cercanos entre las categorias. Esto se llama el margen maximo. Distancia maxima para equivocarse. Luego,  traza una linea x distante de esos puntos. La suma de las distancias entre  los puntos mas cercanos y la linea se busca maximizarla.  Se busca crear ese margen lo mas grande posible. Margen la distancia entre los puntos (o vectores) mas cercanos. Los puntos mas cercanos entre las categorias se llaman vectores. La linea separadora se llama Hiperplano. El Hiperplano busca maximizar el margen. Las rectas asociadas a los vectores se llaman hiperplano positivo e hipleplano negativo.
		
		Python.
		
		from sklearn.svm import SVC
		classifier = SVC(kernel = "linear", random_state = 0)
		classifier.fit(X_train, y_train)
		
		kernel: define la forma de separacion. Hay varios: (rbf: Gausiano, linear: lineal, etc.)
		
		Kernel lineal: linear. Basicamente es como una regresion logistica.	 Es lineal. Se genera una linea de separacion.
		Kernel no lineales: rbf
		
		Parametro degree: se usa en un kernel polinomico.
		Parametro gamma: se usa en nucleo gauseano, polinomico o sigmoide.
		
		Kernel SVN:
		
			Idea de Kernel:
			
				Que paso con nucleos no lineales?
					
				No siempre el limite de separacion entre las categorias es una recta.
				Hay datos que no son linealmente separables.
				
				La SVN tiene una hipotesis la cual es, que el limite de separacion (o la forma de separacion) debe ser detectada o definida por el humano. La separacion puede ser por esferas, circulos, elipses, etc. 
				
				Cuando no son lineales puedo representarlos en otras dimensiones para ver que sucede. Por ejemplo, en 3D puede ser que los puntos esten al nivel del suelo o los otros puntos al nivel del techo. Alli puedo encontar un plano o una linea que los separe. 
				
				Datos no separables en forma lineal.
				
				Hay que tranformarlos en un espacio de nivel superior y obtener un conjunto de datos separables en forma lineal. Luego, se retorna al espacio original para obtener la separacion correcta.
				
				El algoritmo toma datos en una dimension que no son separables linealmente, los transforma en otra dimension, mediante una funcion, tratando de separarlos linealmente, intentando hallar un hiperplano, en la nueva dimension. Encontrada la linea de separacion lineal (o hiperplano) se retorna a la dimension original mediante una proyeccion y se representa, en la dimension original, el separador que es no lineal.
				Usar esta mecanismo de transformación tiene un precio. Al subir a una dimension el calculo computacional puede ser mas costoso.(puede requerirse un equipo con muchos recursos o puede tardarse en clasificar y dar una respuesta)
				
				Truco del Kernel:
				
					Kernel RBF (Gausiano) : No lineal. Se púede usar sin necesidad de entrar en un espacio de nivel superior.
				
				Tipos de Kernel:
				
						- Gausiano (RBF) Muy util cuando los datos deben ser separados como circulos 
						- Sigmoide 
						- Polinomico
						Hay muchos mas...
				
				NOTA: Cuando no quiero que una variable domine las demas variables aplico la estandarizacion. Tambien, en algoritmos que trabajan con distancia euclidea.
				
- Naive Bayes

		- Teorema de Bayes

				P(A|B) = P(B|A) * P(A) / P(B)
				
				A: condicionado
				B: Suceso condicionante
				P: 
				
				Maquina 1: 30 herramientas/hora - Las herramientas son marcadas indicando la maquina que la genero. Maquina 1
				Maquina 2: 20 herramientas/hora - Las herramientas son marcadas indicando la maquina que la genero. Maquina 2
				
				Total de herramientas/hora: 50 herramientas (30+20).
						
				De todas las herramientas producidas se observa que el 1% es defectuoso.
				
				De todas las herramientas defectuosas se observa que:
					El 50% de herramientas defectosas proviene de la maquina 1.
					El 50% de herramientas defectosas proviene de la maquina 2.
					
				Pregunta:

				¿Cual es la probabilidad que una herramienta producida por la maquina 2 sea defectuosa?
				La respuesta es el teorema de bayer.
				
				Probabilidad = Casos posibles / Total de casos.
				
				Si elijo una herramienta cualquiera la probabilidad que sea de una u otra maquina es:
				
				P(Match1) = 30/50 = 0,60 - 60% - Probabilidad de que la herramienta sea producida por la maquina 1 del total de herramientas
				P(Match2) = 20/50 = 0,40 - 40% - Probabilidad de que la herramienta sea producida por la maquina 2 del total de herramientas
				
				P(Defectuosa) = 1%
				
				P(Match1| Defectuosa) = 50% - Sabiendo que es defectuosa cual es la probabilidad de que la herramienta sea de la maquina 1
				P(Match2| Defectuosa) = 50% - Sabiendo que es defectuosa cual es la probabilidad que que la herramienta sea de la maquina 2
				
				Las anteriores son probabilidades condicionadas.
				
				¿Cual es la probabilidad que una herramienta producida por la maquina 2 sea defectuosa?
				
				P(Defectuosa|Match2) = ?
				
				La respuesta es el teorema de bayer.
				
				Teorema de bayer.
				
						P(A|B) = P(B|A) * P(A) / P(B)
				 
						P(Defectuosa|Match2) = P(Match2|Defectuosa) * P(Defectuosa) / P(Match2)
						
						P(Defectuosa|Match2) =  0,50 * 0,01 / 0,40 = 0,0125 (1,25%) De cada 1000 herramientas producidas de la maquina = 12,5 son defectuosas.
				
				En general, la probabilidad condicionada por un suceso se gira el condicionante y el condicionado, multiplicamos por la probabilidad del condicionado y dividimos por la probabilidad del condicionante.
				
				Veamos otro ejemplo:
				
					Calculo frecuentista:
						Total de herramientas: 1000 herramientas 
						Herramientas maquina 2: 400 herramientas 
						1% tienen defecto: 10 herramientas defectuosas.
						Herramientas defectuosas de producidas por la maquina 2: 5 herramientas 
						porcentaje de herramientas defectuosas provenientes de la maquina 2: 5 * 1 / 400 =  1,25%
						
					Porque no realizamos el calculo frecuentista(contamos las herramientas)?
							- Podria ser que tome mucho tiempo contar las herramientas por cada maquina e identificar las defectuosas.
							- Puede ser que no se tenga acceso a los datos o la informacion.
							
							
					Ejercicio rapido:
					
						P(Defectuosa|Match1) = P(Match1|Defectuosa) * P(Defectuosa) / P(Match1)
											 = 0,50 * 0,01 / 0,60 =
	
		- Naive Bayes.
		
				- Porque Naive?
					Naive( se traduce ingenuo)
					Supone una independencia entre los casos que aparencen dentro de la probabilidades. Que las variables independientes sean independientes es decir, no exista correlacion entre ellas. A veces, puede que no exista esa independencia. Es decir, el teorema de Bayes requiere que la Edad y el sueldo sean independientes entre si.
					Se puede aplicar igual, y se obtiene resultados bastantes buenos. Por eso se llama ingenuo.
					
				El ejemplo se trabajo con dos variables pero puede haber + variables.
				
				- Variables independientas: Sueldo y Edad. Vector: (Sueldo, Edad).
				- Categorias: "Camina al trabajo" o "Conduce al trabajo"
				
				Plan de Ataque:
					
					Se aplica dos veces. 
					
					(1) La primera es para conocer cual es la probabilidad de que la persona camine:
					 
							X: Es el vector de caracteristicas (Sueldo, Edad). Ej: (U$S30000, 25). (El dato lo observo o lo consulto).
							
							P(camine/X) = P(X/camine) * P(camine) / P(X)
							
								P(camine): Se llama Probabilidad Previa o a Priori. Conocer cuanta gente va caminando al trabajo. Probabilidad de que una persona camine. 
								Se cuenta todas las personas y se cuenta cuantas caminan.
								P(camine) = Cant. individuos que caminan/Total de individuos.
														
								P(X): Se llama Probabilidad Marginal con respecto a esas caracteristicas. Indivudos que presentan esas caracteristicas del total de individuos. Que probabilidad tiene una persona en presentar esas caracteristicas.
								Como se cualcula? La probabilidad de que un individuo seleccionado al azar presente esas caracteristicas.
								Para calcularla se selecciona un radio de variabilidad, se dibuja un circulo con ese radio cerca del individuo (punto) a clasificar. El individuo o punto se usa como centro. Luego, se cuenta la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. Es decir, cuantos individuos tienen caracteristicas similares al individuo a clasificar.
								El radio es un parametro de entrada del algoritmo. El radio define la cantidad de individuos o puntos forman parte de la muestra a ser usada para contar la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. 
								Todos los individuos o puntos que estan dentro del circulo se consideran similar al individuo a clasificar. Por lo tanto, todos los puntos dentro del circulo son similares. 
								Entra el concepto de distancia: No es lo mismo la distancia euclidea o manhattan. 
														
								P(X/camine): Se llama Probabilidad condicionada por un suceso.
								Del total de personas que camina cuantas presenta esas caracteristicas.
								Para realizar el calculo se usa el mismo circulo definido en el calculo de P(X). La diferencia es que dentro del circulo se cuenta el total de individuos que caminan.	
														
								P(camine/X): Se llama probabilidad a posteriori.

					(2) La segunda es para conocer cual es la probabilidad de que la persona conduce:
					 
							X: Es el vector de caracteristicas (Sueldo, Edad). Ej: (U$S30000, 25). (El dato lo observo o lo consulto).
							
							P(conducen/X) = P(X/conducen) * P(conducen) / P(X)
							
								P(conducen): Se llama Probabilidad Previa o a Priori. Conocer cuanta gente conduce al trabajo. Probabilidad de que una persona conduzca. 
								Se cuenta todas las personas y se cuenta cuantas personas conducen.
								P(conducen) = Cant. individuos que conducen/Total de individuos.
														
								P(X): Se llama Probabilidad Marginal con respecto a esas caracteristicas. Indivudos que presentan esas caracteristicas del total de individuos. Que probabilidad tiene una persona en presentar esas caracteristicas.
								Como se cualcula? La probabilidad de que un individuo seleccionado al azar presente esas caracteristicas.
								Para calcularla se selecciona un radio de variabilidad, se dibuja un circulo con ese radio cerca del individuo (punto) a clasificar. El individuo o punto se usa como centro. Luego, se cuenta la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. Es decir, cuantos individuos tienen caracteristicas similares al individuo a clasificar.
								El radio es un parametro de entrada del algoritmo. El radio define la cantidad de individuos o puntos forman parte de la muestra a ser usada para contar la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. 
								Todos los individuos o puntos que estan dentro del circulo se consideran similar al individuo a clasificar. Por lo tanto, todos los puntos dentro del circulo son similares. 
								Entra el concepto de distancia: No es lo mismo la distancia euclidea o manhattan. 
																				
								P(X/conducen): Se llama Probabilidad condicionada por un suceso.
								Del total de personas que conducen cuantas presenta esas caracteristicas.
								Para realizar el calculo se usa el mismo circulo definido en el calculo de P(X). La diferencia es que dentro del circulo se cuenta el total de individuos que conducen.	
								
								P(conducen/X): Se llama probabilidad a posteriori.
					
				   (3) - Paso 3. Compara las probabilidades. La mayor gana.
				   
							Naives Bayes compara las dos probabilidades y obtiene la mas probable.

							Compara P(camine/X) vs P(conducen/X) y obtiene la de mayor probabilidad.
							
					
					Ejemplo:
							
							Total de personas: 30
							Personas que conducen: 20 
							Personas que caminan: 10
							
							P(Camine): 10/30
							P(conducen): 20/30
							
							P(X): Como se cualcula? 
							Se selecciona un radio y luego se define un circulo tomando como centro el individuo a clasificar (sus caracteristicas.Ej: Edad, sueldo). Finalmente, se cuenta la cantidad de puntos o individuos dentro del circulo. En nuesto ejemplo son 4.
							
							Personas dentro del circulo: 4 (Individuos similares al individuo a clasificar)
							Personas dentro del circulo que caminan: 3
							Personas dentro del circulo que conducen: 1
							
							P(X): Cant. individuos dentro del circulo/Total de individuos.
							P(X): 4/30
							
							P(X/camine): Cant. individuos dentro del circulo que caminan/Total de individuos que caminan.
							P(X/camine): 3/10
							
							P(X/conducen): Cant. individuos dentro del circulo que conducen/Total de individuos que conducen.
							P(X/conducen): 1/20
												
							P(camine/X) = P(X/camine) * P(Camine) / P(X)
							P(camine/X) = (3/10 * 10/30) / (4/30) = 0,75
							
							P(conducen/X) = P(X/conducen) * P(conducen) / P(X)
							P(conducen/X) =	(1/20 * 20/30) / (4/30) = 0,25
							
							P(camine/X) vs  P(conducen/X) = 0,75 vs 0,25 => 0,75 > 0,25 => El individuo a catalogar tiene una mayor probabilidad de caminar. Finalmente, se categoriza como Camina. Es una probabilidad.
								Tiene el punto un 75% de que Camina
								Tiene el punto un 25% de que Conduce
							
					NOTA: Como P(X) es usado en ambas formulas, podemos eliminarlo en ambas formulas y el resultado es el mismo.
				
						P(camine/X) = P(X/camine) * P(Camine) 
						P(camine/X) = (3/10 * 10/30) = 0,1
						
						P(conducen/X) = P(X/conducen) * P(conducen) 
						P(conducen/X) =	(1/20 * 20/30) = 0,033
						
						P(camine/X) vs  P(conducen/X) = 0,1 > 0,033
						El punto a clasificar: Camina. No es una probabilidad. Solo me permite comparar cual es el valor mayor. Es una modificacion ligera. NOTA: No sirve para saber la probabilidad. La mas segura es realizar el calculo completo (usando el denominador P(X)).
					
					NOTA: Que pasa con mas de dos caracteriisticas independientes?
						
						Para el calculo de probabilidades se aplica Bayes a cada caracteristica independiente. 
						
						Recuerda que las probaabilidades no puede superar el 1. Con lo cual si tenemos una caracteristica con una probabilidad mayor al 0,5. El resto de las probabilidades sera menor y esa gana.
									
						Si son mas de tres caracteristicas independientes puedo calcular las probabilidades de dos caracteristicas y ver sus probabilidades viendo que si supera sus sumas el 0,5. La mayor de ellas es la que gana.

			Resumen: Se calculan las probabilidades condicionadas para cada una de las caracteristicas.
			
			NOTA: 
				En R hay que factorizar la variable dependiente si no es un factor. Hacerlo antes de aplicar el algoritmo de Naive Bayes.
				
				Un factor es una variable categórica con un número finito de valores o niveles (etiqueta). En R los factores se utilizan habitualmente para realizar clasificaciones de los datos, estableciendo su pertenencia a los grupos o categorías determinados por los niveles del factor.
	
	
- Arboles de desicion
		
		NOTA: Los arboles no utilizan el concepto de la distancia euclidea. No necesitan escalar.
		NOTA: Si el algotimo usa distancia euclidea, es necesario escalar. Si no, no es necesario.
				
		CART. (Classification and Regresion Trees)
			---> Arboles de Clasificacion. Basado en variables categoricas. Categorias ej, hombre o mujer, compra o no compra
			---> Arboles de Desicion. Basado en variables numericas. Predice un valor.
	
		Como sabe el algoritmo donde ir cortando los datos?
			Ejemplo:
				Rojo = compra
				Verde = no compra 
			Se hace de tal modo que se maximiza el numero de categorias (o puntos) dentro de cada una de las divisiones. Por ejmeplo, quremos que las categorias (o puntos) queden en un lado cuanto mas puntos rojos mejor y al otro lado cuanto mas puntos verdes.
			Concretamente, en la separacion se usa el concepto de entropia. La entropia mide el desorden de una serie de datos. Es un concepto matematico. 
			
		Despues de la separacion tendriamos nodos terminales u hojas con los puntos de separacion para la regresion o le asigna rojo o verde dependiendo el caso. 
		
		x1 y x2: variables independientes.
		
		Luego de la separacion:
		
				Si x2 > 60
						Si x1 < 50
								"Es Verde"
						else
								"Es Rojo"
				else 
					if x1 > 70
						si x2 < 20
							"Es Rojo"
						else
							"Es Verde" 
					else
						"Es Rojo"
						
				
		Un Arbol puede tener mas variables independientes.
		
		NOTA: La tecnica es muy vieja y no son muy utilizados. Son Simple. Se mejoran con otros algoritmos o tecnicas. Las nuevas tecnicas que los mejoran son varias por ejemplo, la tecnica de bosques aleatorios, Gradient Boosting, etc.
			  
		En python el objeto es DecisionTreeClassifier, sus parametros son:
		
				criterion: Criterio de division. Por defecto: "Gini". Tambien, esta la "entropy". Ambos son usados para la division. Generalmente se usa la entropia ("entropy"). Busca la homogenea en la separacion de los datos.
				Entropia 0 la rama es homogenea.


- Bosques aleatorios

		Aprendizaje en conjunto: Combinan la potencia de diferentes algoritmos de prediccion. Son varios algoritmos trabajandp en paralelo y luego juntan la informacion. Objetivo: Elevar el resultado.
	
		NOTA: En el ejemplo se escalo para que quede bonito el grafico. Los arboles no tienen necesidad de escalar. No usan distancia euclidea.

		NOTA: Puede ser que cuando agrego mas arboles al bosque la prediccion puede que no mejore por el overfitting.
		
		Pasos:
	
			Paso 1. Se elige un numero K (numero aleatorio). Representa un subconjunto de los datos del conjunto de datos de entrenamiento. En lugar de dividir los datos en conjunto de entrenamiento y conjunto de test. Elegimos un subconjunto de datos de los datos de entrenamiento.
			
			Paso 2. Construir un arbol de desicion para los K puntos de datos. Lo que hace que el arbol tenga una vision parcial de los datos.
			
			Paso 3. Se define el numero de arboles de desicion y se repite el paso 1 y 2. Cada nuevo arbol tiene un subconjunto de datos del total de datos de entrenamiento.
			
			Paso 4. Cuando llegamos a construir el numero de arboles deseados. Se puede usar todo el bosque para hacer la prediccion. A cada arbol se le indica que realice la prediccion de la categoria a la que pertenece el nuevo punto. Finalmente, la prediccion final es asignar al nuevo punto la categoria con mas votos. 
			
			Este algoritmo mejor la prediccion de un solo arbol de desicion. 
				
			Python:
				RandomForestRegressor(n_estimators = 10, random_state = 0)	
					Parametros de ForestRegressor
							n_estimators -> cantidad de arboles del bosque. Defecto 10 arboles.
							criterion -> criterio para dividir una rama en dos subramas o nodos hojas.
										Criterio de division: MSE (error cuadrado medio) diferencia entre la prediccion y el valor real al cuadrado. Se intenta minimizar. Se divide la rama en dos subramas intentando que el MSE de cada rama sea menor al MSE de la rama que se divide. Tambien, hay otros criterios de division como la entropia (Se minimiza la entropia). 
							max_features -> caracteristicas tenidas en cuenta a la hora de generar las divisiones.
							random_state -> reproductivilidad para obtener el mismo bosque.
					  

- Evaluar la eficacia de los modelos de clasificacion.

	- Falsos positivos y falsos negativos.
		
			Falsos positivos o errores de tipo 1. Ejemplo, Usuarios que no compraban, la observacion real del dato lo muestra, pero mi modelo indica que compran.
			
			Falsos negativos o errores de tipo 2. Ejemplo, Usuarios que compraban, la observacion real del dato lo muestra, pero mi modelo indica que no compran.
			
			De estos dos hay uno peor que otro. Falsos positivos o errores de tipo 1 son cuidado. Mientras, Falsos negativos o errores de tipo 2 son PELIGRO DE MUERTE (Son malos) ES EL PEOR DE AMBOS. En ciertas cuestiones medicas o biologicas puede provocar daños a las personas. Ej, No hay un tsunami (pero lo hubo).
			
			
	- Matriz de confusion: 
		
			Ejemplo Matriz de confunsion
									predichos
								
								0				1
						0		35				5	-> (falso positivo)
			observados
			(real)		1		10				50
								 |
								 V
							   (falso negativo)
	
	       calcular dos prporciones:
		   
				Ratio de presicion (RP) = Correctos / Total =  85/100 = 85 %
				Ratio de error (ER) = Incorrecto / Total = 15 / 100 = 15 %
				
				
	- Paradoja de presicion:
	
			Ejemplo Matriz de confunsion
									predichos
								
								0				1
						0		9700			150	-> (falso positivo)
			observados
			(real)		1		50				100
								 |
								 V
							   (falso negativo)		
							   
			Ratio de presicion (RP) = Correctos / Total =  9800/10000 = 98 %
			Ratio de error (ER) = Incorrecto / Total = 200 / 10000 = 2 %
			
			Segun la matriz de confusion la gran mayoria de datos son marcados como Cero (9700). Hay muchos datos marcado como Cero. Dejamos el modelo y marco el dato como Cero. Muevo los datos que eran predichos como 1 a 0. La razon es porque es mas probable un 0 que un 1:
			
											predichos
										
										0				1
								0		9850			0	-> (falso positivo)
					observados
					(real)		1		150				0
										 |
										 V
									   (falso negativo)		
									
					Ratio de presicion (RP) = Correctos / Total =  9850/10000 = 98,5 %
					Ratio de error (ER) = Incorrecto / Total = 200 / 10000 = 2 %
			
			Se aumenta la presicion sin haber usado el modelo. Esto es la paradoja de la presicion. Cuando tenes un suceso raro o muy raro que ocurra, lo mejor es no usar modelos. Por ejemplo, si un 0,1 % de la poblacion tiene una enfermedad, lo mas normal es que nadie la tenga. Esto puede mostrar que en sucesos raros los modelos a veces son inutiles.
		
		
	- Curva CAP (Perfil de Presicion Acumulado):
			
			Como se sabe que un Perfil acumulado es mejor que otro? El area que queda entre la curva de color rojo y la recta azul. Cuanto mayor es esa area, mejor resultado se obtiene. Lave la pena de representar cuan se lleven a cavo algoritmos de clasificacion para ver que tambien el algoritmo funciona. Generalmente, se traduce a porcentajes: Del 100% de personas (o datos de nuestro dataset) suceptibles a ser contactadas, a que porcentaje tengo que atacar para conseguir el 50% o 60 % de las ventas. Si empleamos varios algoritmos de clasificacion y generamos el Perfil de Presicion Acumulado podemos compararlos. 
			
			Perfil de Presicion Acumulado es adecuado para comparar algoritmos de Clasificacion. Al compara los algoritmos con "(Perfil de Presicion Acumulado)" podemos saber que tanto mas ganamos de un modelo a otro o con respecto al modelo aleatorio (tomar individuos al azar). 
			Existe una curva llamada "Bola de cristal(o Modelo Perfecto)" que es el mejor modelo. Con el se puede contactar el 10% de los clientes y que los mismos realicen el 100% de las compras. Se sabe a quien contactar y que realizara todas las compras. No ocurre casi nunca pero es adonde intentamos ir.
			
			
	- Analisis de las curvas CAP		
	
			La linea del Modelo Buen es obtenida desde la salida del algotimo de clasificacion.
			
			Para evaluar modelos de clasificacion. Tenemos varias lineas que representan las salidas de nuestro modelos. Sin embargo, existen 3 lineas principales: La linea del modelo perfecto, la linea del buen modelo y la linea del modelo aleatorio. Lo que realizamos es cociente de areas, el limite al que podemos llegar.Como de cerca estamos de la linea perfecto o buen modelo (Bola de cristal). Para ello tomamos el area que va desde la linea del Modelo Perfecto hasta la linea del Modelo Aleatorio. Esta es el area del Modelo Perfecto (Ap). Esta es el area maximo que puede conseguir un modelo de clasificacion. Del mismo modo calculamos el area que va desde la linea del Modelo Bueno hasta la linea del Modelo aleatorio. La llamamos (Ar). Lo que queremos saber que porcentaje del area del Modelo Perfecto (AP) cubre el area del Modelo Bueno.
					
					AR = Ar/Ap . 
					
					Esto nos da un valor entre 0 y 1. Cuanto mas cerca de 1 mejor ajusta al Modelo Perfecto. Cuanto mas lejos de 1 sera peor. Si el valor es 0 es lo mismo realizar elecciones con el modelo o realizarlas aleatoriamente.
					
					El calculo de AR no se suele realizar a mano. Suele haber librerias que ayuden. La curva es dificil de calcular. Normalmente se realiza una serie de muestreos, se calcula con el modelo con el 20% de los datos, con el 40% de los datos, con el 60% de los datos. Se obtiene un poligono que ajusta el area de nuestro modelo y se calcula el area basado en este poligono. 
					
					Otra forma de ver que tan bueno es nuestro modelo es establecer una cota del total de contactados. Tipicamente la cota suele ser 50%. Luego se mira que porcentaje de Ventas se consigue en el modelo con el 50% de compradores. Busco el 50% de compradores y analizo cuanto me pronostica el modelo (X %). Se usa la siguiente regla para clasificar el modelo:
					
							90% < X < 100% 	Demasiado Bueno.
							80% < X < 90% 	Muy Bueno.
							70% < X < 80% 	Bueno.
							60% < X < 70% 	Malo.
							60% < X 		Muy malo.
					
							Entre 70% y el 80% es un modelo bastante acertado. No presenta problemas de overfitting
							Entre 80% y el 90% es muy bueno. No presenta problemas de overfitting. Si se presente es Muy bueno.
							EWntre 90% < X < 100% 	Demasiado Bueno. Puede presentar overfitting. Estudiarlo. Puede ser que en un 100% la variable dependiente forma parte de las independientes. Tambien, puede haber una relacion (o correlacion) entre las variables independientes.
							
							NOTA: Ir con  cuidado cuando los modelos superen el 90% de presicion. Es imposible conseguir el 100% de prediccion. Seguramente existe overfitting.
							
 - Conclusion - Clasificación


		¿Cuales son los pros y los contras de cada modelo?

		¿Cómo sé qué modelo debo elegir para resolver mi problema?

		¿Cómo puedo mejorar cada uno de estos modelos ?


		Resolvamos cada pregunta una por una:

		1. ¿Cuales son los pros y los contras de cada modelo?

		En esta clase encontrarás un documento que te dará todos los pros y contras de cada modelo de clasificación.


		2. ¿Cómo sé qué modelo debo elegir para resolver mi problema?

		Al igual que con los modelos de regresión, primero tendrás que averiguar si tu problema es o no es lineal.

		Una vez lo sepas, Si tu problema es lineal, deberás intentar crear un modelo de Regresión Logística o bien SVM.

		Si tu problema no es lineal, entonces tenemos varias técnicas donde elegir, como K-NN, Naïve Bayes, Árboles de Decisión o Random Forest.

		Desde un punto de vista empresarial, entonces deberías usar:

		Regresión Logística o Naïve Bayes cuando quieras ordenar tus predicciones por probabilidad. Por ejemplo, deberías usar estas técnicas si quieres crear un ranking de clientes desde el más probable al menos probable que compre un producto. Esto permite crear objetivos específicos para las campañas de marketing, por ejemplo. Y por supuesto, si tu problema de empresa es lineal, mejor utiliza la regresión logística, y si no lo es, intenta con Naïve Bayes.

		SVM  cuando quieras predecir a qué segmento pertenece un cliente. Los segmentos pueden ser cualquier conjunto de características que definan a los clientes, como los que identificaremos en la Parte 4 - Clustering.

		Los árboles de decisión cuando necesites tener una interpretación clara de los resultados modelizados.

		Y por último, Random Forest cuando busques un mejor resultado de predicción y te preocupe menos la interpretación de los modelos.


		3. ¿Cómo puedo mejorar cada uno de estos modelos ?

		Igual que en la parte 2, en la Parte 10 - Selección de Modelos, la segunda sección está dedicada a los Ajustes de Parámetros que permite mejorar la eficacia de nuestros modelos ajustando los valores de los parámetros. Como habrás comprobado, existen dos tipos de parámetros en nuestros modelos:

		los parámetros que el modelo aprende, como los coeficientes de la Regresión Lineal,

		los hiper parámetros del algoritmo.

		En este último caso, los hiper parámetros  son parámetros que el algoritmo no aprende, si no que son fijos y forman parte de las ecuaciones de los modelos. Por ejemplo, el parámetro lambda de regularización o el factor de penalización C son hiper parámetros. Hasta el momento hemos tomado los valores por defecto y no nos hemos preocupados de afinar su valor óptimo para mejorar la eficacia del modelo. Entontrar el valor óptimo es parte del Ajuste de Parámetros, así que si estás interesado en descubrir cómo hacerlo, te recomiendo ir directamente a la Parte 10 del curso donde veremos juntos cómo hacerlo.
						
					
-------------------------
Clustering o Segmentacion
-------------------------

		Clustering es un proceso similar al de clasificación, pero con un fundamento diferente. En el Clustering no sabes qué categorías estás buscando, si no que intentas crear una segmentación de tus propios datos en grupos más o menos homogéneos. Cuando utilizamos algoritmos de clustering en el data set, surgen de entre los datos cosas inesperadas como estructuras, clusters o agrupaciones que un humano no se habría imaginado pero la máquina crea por nosotros.

		Algoritmos de Clustering para Machine Learning:

			Clustering con K-Means

			Clustering Jerárquico
			
- K-Means

		Se usaron en el ejemplo 2 variables pero se pueden tener mas variables (3, 4, etc.) para identificar los puntos.
		
		Tenemos un conjunto de puntos que queremos segmentarlos o clasificarlos. La diferencia con Clasificacion es que no conocemos las categorias a asignar a cada conjunto. Los datos se deben segmentar. No sabemos los grupos a segmentar. Los datos se agrupan por homogéneos. No hay categorias definidas conocidas previamente. Por ejemplo, compra o no compra.
		
		Como funciona:
		
			Paso 1. Elegir el numero k de cluster. (NOTA: Ver como calcular el numero K de cluster)
				
			Paso 2. Se seleccionan K puntos. Son los baricentros iniciales. (no necesariamente tienen que ser de nuestro dataset). Seran los hipoteticos centros geometricos de cada cluster. Los baricentros son lo puntos en el medio de cada cluster. Se seleccionan al azar. 

			Paso 3. Se asignan cada uno de los puntos del dataset al baricentro mas cercano. Esto forma K cluster.

			Paso 4. Se calcula el nuevo baricentro de cada cluster usando los puntos de cada cluster. Calculo la media geometrica de cada coordenadas (variables o caracteristicas de los datos. Ejemplo X1, x2, etc.) del cluster. Los baricentros originales quedan recalculado. Si en el calculo del nuevo baricentro uso distancia euclidea, los cluster quedaran redondos o elipse. En cambio si uso la distancia manhattan los cluster serian cuadrados. 
			NOTA: La distancia que elijamos es importante. La euclidea es la mas comun. Que distancia debo utilizar? Depende del problema a resolver. IMPORTANTE: Investigar sobre la forma de calcular las distancias y cuales son sus concecuencias en la forma del cluster.
			
			Paso 5. Se reasignan cada uno de los puntos del dataset al baricentro mas cercano. Si se realizan nuevas asignaciones ir al Paso 4 sino ir a Fin.
			
			FIN. Modelo listo!
			
			
		- La trampa de la incializacion aleatoria:
		
			Al inicio se eligen baricentros al azar para comenzar el algoritmo. Segun como se inicialicen esos baricentros el resultado puede ser bastante diferentes. Hay que tener cuidado con la eleccion de baricentros.
			
			¿Que pasa si elegimos mal los baricentros?
			
				Segun como inicialicemos nuestros baricentros al comienzo de algoritmo podemos sesgar el resultado final. Esto no es bueno porque una eleccion aleatoria puede llevar a una segmentacion incorrecta. Como corregimos esto? No es sencillo. Existe una modificacion del algoritmo de  K-Means que permite seleccionar bien los baricentros al incio del algoritmo. Se llama K-means ++ (plus plus). Es bastante dificil de entender pero se puede leer en otras fuentes. El algoritmo de K-means ++ esta implementado en python o R y no hace falta entender el fundamento del algoritmo K-means ++ para utilizarlo en esos lenguajes. NOTA. La seleccion de los baricentro iniciales puede sesgar la clasificacion. La respuesta:  K-means ++.
				
				
		- Como seleccionar el numero correcto de K cluster?
		
			K indica la cantidad de grupos que salen de la segmentacion.
			
			Dependiendo del numero K elegido cambia completamente el resultado del algoritmo.
		
			La eleccion depende de una metrica concreta. La metrica se llama la "Suma de los Cuadrados del Centro del Cluster" (WCSS). La formula es la suma de los cuadrados de las distancia de cada punto con respecto al cluster que pertenece (al centro geometrico del cluster). Luego, se suman todos los resultados de cada cluster.
			
			WCSS1 = Σ d(Pi, C1)^2
					Pi E Cluster 1
				
			WCSS2 = Σ d(Pi, C1)^2  +  Σ d(Pi, C2)^2  
					Pi E Cluster 1	 Pi E Cluster 2	 
		
			WCSS3 = Σ d(Pi, C1)^2  +  Σ d(Pi, C2)^2  + Σ d(Pi, C3)^2  
					Pi E Cluster 1	 Pi E Cluster 2	   Pi E Cluster 3
					
			Se busca minimizar el WCSS. A medida que se aumenta el numero de cluster las Sumas de los Cuadrados (WCSS) se reducen.
					
			¿Hasta donde se disminuyen? 
			 
			El decremento no sera constante. Al principio decrece rapidamente pero existira un momento en que al agregar mas cluster, los puntos estan tan cerca que no existe una reduccion o mejora considerable. Se traba de buscar el mejor equilibrio, ya que en un extremo puedo tener tantos cluster como puntos (esa es la menor distancia) pero no sirve para el modelo. Se busca al agregar un nuevo cluster una disminucion palpable de la metrica WCSS. Para ello, se usa la tecnica del codo, el cual es un metodo visual que se basa en decidir a partir de que punto, añadir un nuevo cluster a nuestro algoritmo de K-means no aporta una mejora. Es decir, disminuir la suma de los cuadrados de las distancias entro los puntos y el cluster al que pertenecen. 
			Se hace el grafico del codo y de forma visual detectar el cambio en la tendencia de disminucion del valor WCSS. 
			Nro. optimo de cluster = Tecnica del Codo. A veces es dificil detectar el numero K correcto en el grafico. No existe una formula matematica perfecta que permita sacar el valor correcto de K. 	La Tecnica del Codo es la mas pactada que funciona. Al final, es nuestra desicion como Analista elegir el K numero de cluster.
			
			Python:
			
			# Método del codo para averiguar el número óptimo de clusters
			from sklearn.cluster import KMeans
			wcss = []
			for i in range(1, 11): # Hace de 1 a 10
				kmeans = KMeans(n_clusters = i, init = "k-means++", max_iter = 300, n_init = 10, random_state = 0)
				kmeans.fit(X)
				wcss.append(kmeans.inertia_) #kmeans.inertia_: Trae la Suma de los Cuadrados de las distancias(WCSS).
				
			# Parametros:
			# n_clusters: Nro. de cluster
			# init: inicializacion de los baricentro. Como no queremos caer en la trampa usamos el k-means++.
			# max_iter: Puede ser que el algoritmo no finalice, moviendo el centro de un lado al otro. Se recomienda indicar el numero maximo de iteraciones. Indica que luego de n iteraciones finalice el algoritmo. Default 300. 
			# n_init: Inicializacion aleatoria. Defecto 10.
			
			plt.plot(range(1,11), wcss)
			plt.title("Método del codo")
			plt.xlabel("Número de Clusters")
			plt.ylabel("WCSS(k)")
			plt.show()
			
			En R:
			
				NOTA: El algoritmo estandariza los valoes de las variables. Cambia la escala.
				EL objeto "clusplot" de R es para usarlo en dos dimensiones (no mas de dos variables).
				
				
- Clustering Jerarquico
        
		No son apropiado para grandes datasets
		
		Como funciona?
		
			El enfoque y el proceso es diferente al de k-means. El resultado puede ser exactamente igual al de k-means.
			
			Existen dos tipo de agrupaciones jerarquicas: Aglomerativo y divisitivo. No es lo mismo un cluster jerarquico Aglomerativo que un cluster jerarquico Divisitivo. El aglomerativo es un enfoque de abajo hacia arriba. En el curso vemos el aglomerativo. Comenzamos desde los elementos (desde abajo) y construimos nuestro cluster jearquico. Comenzamos desde los elementos y vamos juntando objetos similares para crear conglomerados. 
			
			Cluster jerarquico Divisitivo: Partimos del total de elementos y comenzamos a partir por donde nos interesa para crear el cluster.
			
			Trabajamos con Cluster jerarquico Aglomerativo:
			
				- Como funciona el algoritmo?
				
						Paso 1: 
							Hacer que cada punto sea un cluster (Asi tenemos tantos cluster como puntos). (N Clusteres)
							
						Paso 2:
							Elegimos los dos puntos mas cercanos y los juntamos en un unico cluster. (N-1 Clusteres)
							Por que los puntos mas cercanos? Porque el algoritmo aglomerativo se basa en distancias. 
							
							OJO que dependiendo como calculo la distancia (euclidea, manhattan, etc) afecta al resultado del algoritmo. Ecuclidea es la mas usada.
							
							Se repite, es decir se van juntando los dos puntos mas cercanos en cluster hasta no poder armar mas clusteres.
							
							Los puntos mas alejados son mas diferentes.
							
							Pueden quedar cluster con un unico punto dado que ese/esos punto/puntos esta/estan mas alejado/alejados del resto de los puntos.
							
						Paso 3:
							Elegimos los dos clusteres mas cercanos y los juntamos en un unico cluster. (N-2 Clusteres)
							La clave es elegir simpre los clusteres mas cercanos. Como calculo la distancia entre dos clusteres? 
							
							Puede haber varias opciones para seleccionar los puntos de referencia que representan a cada cluster:
							
							(1) Distancia entre los puntos mas cercanos de cada cluster.
							(2) Distancia entre los puntos mas alejados de cada cluster.
							(3) Distancia media. Distancia entre todas las combinaciones de puntos entre un cluster y otro. Finalmente, se hace la media de todas esas distancias (de todos esos resultados).
							(4) Distancia entre sus baricentros. Se busca el centro geometico y se calcula la distancia entre esos centros.
					
							OJO que dependiendo como selecciono los puntos (usando cualquiera de las tecnicas anteriores) se afecta al resultado del algoritmo. Se elige dependiendo del problema a tratar.
							
							Luego de elegir la opcion para seleccionar los puntos de referencia, se calcula la distancia entre esos puntos de referencia usando euclidea, manhattan, etc. Defecto euclidea.
									
						Paso 4:
							Repetir el Paso 3. Es decir, ir juntando los clusteres mas cercanos hasta tener un unico cluster.
							
						FIN.
				
				
				- Como funcionan el Dendrograma?
				
						Dendrograma es una arbol.
										
						Para saber el numero de clusteres que van a salir necesitamos pasar por una herramienta visual llamada dendograma. 
						
						Paso 1 .
						
							Se representan cada punto en el eje X indicando (P1, P2, .., Px) P1. Punto 1, P2 Punto 2,... Pn Punto n. En el eje de las Y se indican valores que representan las futuras distancias.
							
							Este es el grafico de Dendrograma.
							
						Paso 2. Se incia el  Cluster jerarquico Aglomerativo:
						
								Paso 2.1: 
									Hacer que cada punto sea un cluster (Asi tenemos tantos cluster como puntos). (N Clusteres)
						
								Paso 2.2:
									Elegimos los dos puntos mas cercanos y los juntamos en un unico cluster. (N-1 Clusteres)
									Se calcula la distancia entre esos puntos (Euclidea, manhattan, etc. ). Euclidea es la que mas se usa. Se indica una raya horizontal que une esos dos puntos en el grafico Dendrograma, tomando como Y el valor de sus diferencias. Luego, se une la raya horizontal con el eje de las X (donde se representan los puntos). Esto se hace para cada punto cercano hasta no poder formar mas clusteres.
									
									La barra resultante es mayor a mayor distnacia entre los puntos.
									
								Paso 2.3:
									Elegimos los dos clusteres mas cercanos y los juntamos en un unico cluster.	Se calcula la distancia entre esos clusteres (usando alguno de las opciones de calculo). Se indica una raya horizontal que une esos dos clusteres en el grafico Dendrograma, tomando como Y el valor de sus diferencias.
									
								Paso 2.4:
									Repetir el Paso 2.3. Es decir, ir juntando los clusteres mas cercanos hasta tener un unico cluster. Se indica una raya horizontal que une todos los clusteres en el grafico Dendrograma, tomando como Y el valor de sus diferencias.
							
								FIN.	
							
						El Dendograma representa las uniones y jerarquias entre los clusteres a media que se van construyendo.
	
	
				- Como utilizar los Dendograma?
						
						Dendograma: Permite identificar el numero adecuado de clusteres.
						
						Se miran los niveles horizontales. Se define una distancia para cortar el dendograma horizontalmente. Ese umbral (Ej. una distancia de 1,7) me permite separar los clusteres. Luego, marcar la raya horizontal que representa el umbral en el dendograma. Como resultado, voy a tener clusteres separados. En el ejemplo, todo lo que esta por debajo del umbral se conforman en cluster que tiene una diferencia no superior a 1,7.
						
						NOTA: La cantidad de lineas verticales que corta la linea horizontal que representa el umbral es la cantidad de clusteres.
						
						Segun como seleccionemos el umbral tendremos diferentes numeros de clusteres. Cual es la forma optima de identificar el numero de clusteres? Hay una regla que se puede usar y es que la recta vertical mas larga hasta que cruza a una linea horizontal (las que representan las uniones entre las agrupaciones), es la que debemos cortar con la raya horizontal del umbral. Se anlizan las lineas horizontales que salen de una horizontal hasta otra horizontal (imaginariamente). La mas larga es la que se corta. Una linea vertical mas estrecha marca menor distancia entre los elementos. Sin embargo, a mayor linea vertical mayor distancia entre los elementos (agrupaciones o puntos). Se corta por cualquier punto de la recta vertical mas larga. NOTA: Cuando se corta con la linea horizontal (umbral) no se debe tocar las rectas horizontales que representan las uniones de los elementos (puntos o grupos). La cantidad de rectas verticales que se cortan con el recta horizontal (umbral) es la cantidad de cluster a usar.
						
						
						Python:
												
							# Utilizar el dendrograma para encontrar el número óptimo de clusters
							import scipy.cluster.hierarchy as sch
							dendrogram = sch.dendrogram(sch.linkage(X, method = "ward"))
							plt.title("Dendrograma")
							plt.xlabel("Clientes")
							plt.ylabel("Distancia Euclídea")
							plt.show()
									
							# sch.linkage: Representa el algoritmo Aglomerativo
							# Parametros
								# X: La lista de variables (matriz) a segregar.
								# method: metodo usado para encontrar los cluster o armar los clusteres. Ward: Intenta minimizar la varianza entre los puntos que exiten en los cluster. Se minimiza la varianza intracluster. En R hay varios tipos de "ward". Se usa "ward.D", ward.D2 es una mejora
							
							# Ajustar el clustetring jerárquico a nuestro conjunto de datos
							from sklearn.cluster import AgglomerativeClustering
							hc = AgglomerativeClustering(n_clusters = 5, affinity = "euclidean", linkage = "ward")
							y_hc = hc.fit_predict(X)
							
							# Parametros
							# n_clusters: nros. de clusteres.
							# affinity: distancia a utilizar.
							# linkage: metodo usado para encontrar los cluster o armar los clusteres. Ward: Intenta minimizar la varianza entre los puntos que exiten en los cluster. Se minimiza la varianza intracluster.
							#El metodo usado en linkage debe ser el mismo al metodo usado en el dendograma (parametro "method").
					
					NOTA: El grafico es para dos dimensiones. Si se trabaja con mas dimensiones. Comentar el codigo del dibujo, ya que es imposible realizarlo. Se puede usar ACP para la reduccion de dimensiones y reducirlas tal vez hasta conservar solo dos y poder representar el grafico.-


Conclusión de la Parte 4 - Clustering

Hemos aprendido dos modelos de clustering: K-Means y el Clustering Jerárquico.

Adjunta en esta clase encontrarás una hoja de ayuda con los pros y contras de los dos modelos de clustering.



------------------------------------
Aprendizaje con Reglas de Asociación
------------------------------------

NOTA: Trata el problema de recomendaciones.
Busca asoiaciones entre transacciones u operaciones.

Premisa: 

	La gente que compre este producto, tambien compro este otro producto.
	La gente que hizo esto, tambien hizo esto otro.
				 
	Analiza el comportamiento emparejado de los usuarios.
	
	Lo compra o no lo compra. No hablamos de cantidades. (Presencia o Ausencia	)
	
	Sirve para realizar recomendaciones. Por ejemplo, si compro miel y manteca, se puede recomendar que compore gallettitas. Esta recomendacion se extrae de los datos del set de datos trabajado.
	
Dos modelos de reglas de asociación:

-(1) Apriori
-(2) Eclat


- Apriori

		Ejemplo: Sistema de recomendacion de peliculas(Netflix), Optimizacion de Cesta de Compra (Amazon)
		
		Como funciona?
				
				Se dividi en tres partes.
				
				(1) Soporte de la regla de asociacion:
				
						Recomendacion de Peliculas
						
								Sop(M) = |Usuarios que vieron M|/|usuarios|
								Usuarios que van a favor de lo que estamos evaluando/total de usuarios	
				
						Optimizacion de Cesta de Compra
								Sop(I) = |transacciones que contiene I|/|transacciones|
								Transacciones que van a favor de lo que estamos evaluando/total de transacciones
					
				(2)	Confianza
				
						Recomendacion de Peliculas
						
								Conf(M1=>M2) = |Usuarios que vieron M1 y M2|/|Usuarios que vieron M1|
								
				
						Optimizacion de Cesta de Compra
					
								Conf(I1=>I2) = |transacciones que contiene I1 y I2|/|transacciones que contiene I1|
						
						Que tan seguros estamos de que ocurren dos cosas a la vez. (Una especie de pprobabilidad condicionada) Ej. Que usuario consumio un item y otro al mismo tiempo?.  Que confianza hay cuando vemos una pelicula y luego ver otra?. Numero de  usuarios que ven una pelicula y luego otra, dividido el numero total de usuarios que ven la primera de ellas. Ej. Numero de personas que ven Interestelar y Luego ven ex-maquina, dividido el numero de personas que ven interestelar. Cantidad de usuarios que compran el item 1, luego compran el item 2, dividido la cantidad de uarios que compran el item 1.
										
				(3)	Lift
				
						Recomendacion de Peliculas
						   Lift(M1=>M2) = Conf(M1=>M2)/Sop(M2)
						
						Optimizacion de Cesta de Compra
							Lift(I1=>I2) = Conf(I1=>I2)/Sop(I2)
							
				Prob = Casos posibles / casos totales.
							
				Ejemplo General:
				
						- Total de poblacion de personas: 100.
						
						- Total de personas que vieron ex-maquina del total de la poblacion = 10.
						
						- Soporte de Ex-maquina: Nivel superior al 10 % (Elegido por caso de uso)
							Sop(ex-maquina) = 10 / 100 = 0,1 = 10%
							10 personas vieron ex-maquina del total de la poblacion.
							
					    - Total de personas que vieron Interestelar del total de la poblacion: 40
						
						- Total de personas que vieron Interestelar y Luego	ex-maquina: 7

						- Confianza(Interestelar=>ex-maquina) - Nivel superior al 10 % (Elegido por caso de uso)
							Conf(Interestelar=>ex-maquina) = 7 / 40 = 0,175 = 17,5%
							
						-  Lift(Interestelar=>ex-maquina) = 0,175 / 0,1  = 1,75 
						
							Si vieste Interestelar, recomendar ver ex-maquina es mejor que recomendar sin saber nada. 
				
		Algoritmo Apriori:

					Paso 1. Decidir el soporte y el nivel de confianza minimo.(Se decide de acuerdo al problema a tratar - Subjetivo)
							Para evitar que el algoritmo crezca y se tranforme en algo dificil de tratar, que no converja, se establece un soporte minimo. Por ejemplo, no queremos es tratar productos que tengan un soporte menor al 20% (por decir algo). Porque sera una perdida de tiempo construir algo para ese valor porque seguramente sera dificil que la gente vea esa recomendacion o es insignificante. Se debe jugar con esos umbrales.
							
							- Si busco valores muy altos de soporte y nivel de confianza, el sistema no recomendara nada.  
							
							- Si busco valores muy bajos de soporte y nivel de confianza, el sistema realizara muchas recomendaciones y lo probable es que el usuario no lo utilice. 

							Sera un tira y afloje.
					
					Paso 2. 
							Elijo todas las transacciones con soporte superior al minimo elegido. Filtramos por soporte. Si tenes 20 peliculas, calculamos el soporte de las 20 peliculas y elegimos aquellas que el soporte supere el minimo elegido. (No implica cruces).
							
					Paso 3. 
							Elijo todas las transacciones del subconjunto del Paso 2 con nivel de confianza superior al minimo elegido. Filtramos por nivel de confianza. Hacemos los cruces para aquellas peliculas cuyo soporte supere el nivel de soporte, luego calculamos la confianza de los cruces, finalmente, elegimos aquellas confianzas que superen el minimo elegido. (Implica curces).
					
					Paso 4.
							Se ordenan las reglas resultantes (los dos procesos de filtrados) por Lift descendiente (relevancia de la reglas). Se colocan todas las reglas con mayor impacto primero.
							
							
					Amazon, facebook, Netflix son ejemplo de aplicacion de reglas apriori.
					
	Aplicacion en Lenguaje R:
	
	Recurso adicional: cómo representar las reglas de asociación en un grafo 
	
	# ------------------------------------------------------------------------
	# GOAL: show how to create html widgets with transaction rules
	# ------------------------------------------------------------------------
	 
	# libraries --------------------------------------------------------------
	library(arules)
	library(arulesViz)
	 
	# data -------------------------------------------------------------------
	path <- "~/Downloads/P14-Part5-Association-Rule-Learning/Section 28 - Apriori/"
	trans <- read.transactions(
	file = paste0(path, "R/Market_Basket_Optimisation.csv"),
	sep = ",",
	rm.duplicates = TRUE
	)
	 
	# apriori algoirthm ------------------------------------------------------
	rules <- apriori(
	data = trans,
	parameter = list(support = 0.004, confidence = 0.2)
	)
	 
	# visualizations ---------------------------------------------------------
	plot(rules, method = "graph", engine = "htmlwidget")
	
	
	El problema con matrices con valores vacios es que será difícil llegar a entender el dataset en el sentido clásico, lo que necesitamos es establecerlo como una matriz dispersa, una matriz Sparse, que es una matriz dispersa en realidad, una matriz Sparse que contiene muchos ceros y por tanto, muchas veces, en lugar de reservar espacio en memoria para la celda, no le hace falta reservarlo, de modo que se guarda internamente. Al ser una matriz con tan pocos valores distintos de cero, se optimiza muchísimo el espacio de memoria y la forma de trabajar con ello. Internamente se guarda en formato de tres columnas: A qué fila primera? A qué columna segunda corresponde? y  Qué valor tercera? Es la forma interna en la que se guardan las matrices de todas las matrices dispersas.	
	
	Lo que se hace es tomar todo ese conjunto de datos. Supongamos que hay un total de  120 productos diferentes en este caso y lo que podemos hacer es crear una serie de columnas, cada una de las cuales sea un producto. Luego podemos tener filas correspondiendo a los clientes y finalmente, valores dentro de la matriz que sea ese usuario compra o compra ese artículo.
    Por ejemplo, de modo que tendremos en las columnas agua mineral, huevos, pollo, chocolate, patatas fritas, miel, brownies, lo que sea. Tendremos un total de 120 columnas, correspondiendo a todos y cada uno de los 120 productos que hay en este dataset. En cada una de las filas aparecerán los usuarios que llevan a cabo las compras.
	
	Productos en columna, clientes en fila es la forma más estándar de declarar una matrices, luego internamente se guarda,  primera columna Ídem de usuario, segunda columna Ídem de producto, tercero compra o no compra, que básicamente es presencia,
	ausencia del valor.
	
	En R se usa "arules" libreria de Association Rules o Reglas de asociación.
	
	libray(asrules)
	dataset = read.transactions("file_name_dataset" , sep="," , rm.duplicates=true ) Esto es una sparse matriz.
	summary(dataset) Ver el resultado
	
	dispersion: indica el % de valores cubiertos. la diferencia con el 100% son el valor cero.
	element length distribution: cantidad de compras que tenian 1 o 2 o ... n productos.
	
	crear el algoritmo apriori:
      rules = apriori(dataset, 
	           parameter = list (support=0.1, confidence=0,8))	NOTA: limite minimo de soporte y nivel de confianza. Su seleccion depende del objetivo buscado.
			   
			   support y confidence bajo mas de reglas. Si las reglas generadas no son suficientes podemos subirlos.
			   
			   Soporte: 
				   Para decidir el valor de soporte tenemos que preguntarnos cuántos de los elementos diferentes de nuestro dataset queremos que aparezcan como reglas significativas de aprendizaje. Se puede realizar un top de los 10 elementos mas fecuentes (con itemFrecquencyPlot(dataset, topN=10)). Sin emargo, para optimizar las ventas nesitamos definir las ventas de los items que se venden mas. Esto se puede ver en el grafico construido en el valor en la barra vertical llamada "item frequency (relative)". Alli se puede identificar el valor de soporte a usar (valor en la barra vertical llamada item frequency (relative)). Sus valores se define segun lo buscado por el negocio.  Con el soporte defino que items voy a trabajar o seran mi base de las futuras reglas.		   
				   En el ejemplo se definio:
						Items que se compran 3 veces al dia (los datos del dataset son ventas de una semana), 7500 son los registros totales del dataset.
						
						3 * 7  / 7500 = 0,003 (valor del soporte)
						
						El soporte se puede definir sobre las necesidades de la tienda.
												
			   Confianza:
			   	   La confianza puede comenzarse por el defecto (0,8). Confianza baja se optienen reglas que representan ventas muy raras. Depénde de los objetivos de la empresa. Es arbitraria. Un nivel de confianza demasiado pequeño, obtendremos algunas reglas que ocurren muy de vez en cuando y que no tendrían sentido. Por ejemplo, comprar chocolate. A la vez que compro el champú, he ido a por champú porque me he quedado sin champú y me ha pegado antojo de chocolate. Eso no es una regla de asociación, es una casualidad o una coincidencia. Ese es el tipo de reglas que se obtienen de forma esporádica, con poca confianza. Se puede jugar con el valor del nivel de confoanza.
				   
				   Confianza del 0,8 (80%) significa que las reglas que cree el algoritmo de apriori tiene que ser válida al menos en el 80% de las transacciones estudiadas. El 80% de las transacciones que se llevan a cabo es un número bastante elevado, significa que tiene que ser verdadera en cuatro de cada cinco cestas de la compra debe ocurrir la regla estudiada. Entonces, por eso es que las reglas de asociación con un nivel de confianza tan alto es muy restrictiva. Lo que sí se puede hacer es tomar el nivel de confianza que teníamos anteriormente (0,8) y empezar por dividir por dos, por tres, por cuatro. Ir bajando ese nivel de confianza si lo disminuyo a la mitad.
				   Lo que estoy investigando son reglas de asociación que ocurran en el 40% de las cestas de la compra donde se incluyen esos items.
			   
	Como resultado de ejecucion de la linea anterior, el algoritmo retorna la cantidad de reglas de asociacion. "writing ... [x rules(s)]. Donde x es el numero de reglas.
	
	NOTA: Cambiar el soporte o el nivelde confianza si las reglas no tienen sentido.
	
	Soporte: 0.003 (0,3%)
	Confianza: 0,8 (80%)
	
	RECORDEMOS 
		Se filtran los items de acuerdo al soporte establecido y luego, al nivel de confianza.
	
		El algoritmo filtrara sólo items con soporte superior al mínimo establecido que aparezcan al menos en un 0,3% de las cestas de la compra del supermercado. Y a continuación se extraen las reglas que aparezcan, al menos en el 40% de las cestas de la compra. El paso dos y el paso tres, en R se hacen automáticamente, solo se define el soporte y el nivel de confianza. Finalmente, se ordenan por su lift de mayor A MENOR.
		
		
	inspect(rules[1:10]) /las diez primeras reglas- extrae la fila 1 a la 10
	
	Retorna un conjunto de 10 reglas de recomendacion:
		inspect(sort(rules, by='lift')[1:10]))/Ordena descendiente por lift y extrae las 10 reglas de mayor lift
				lift - mayor peso.
				
		Ejemplo de salida:
				{mineral water, pasta} => {olive oil}
				
				Analisis: Si comro agua y pasta podemos recomendar aceite de oliva.
				          Tambien, que se coloquen en la misma gondola los tres productos.

		Las reglas resultantes se aplican y prueban, en caso de que alguna no sea adecuada se cambian por otras o se ajusta el soporte o nivel de confianza.				  
    
	Aplicacion en Lenguaje PYTHON:


	Las empresas han estado utilizando las reglas de asociación para saber dónde colocar los productos en la tienda que colocar al lado de cada uno de los ítems.

	Por poner un ejemplo muy simple si alguien compra unos cereales, es muy probable que esta persona también quiera comprar la leche para no tomarlo solos.
	
	Entonces el colocar los cereales cerca de la leche es una técnica muy útil para que el cliente que inicialmente había ido con intención de comprar cereales, compre también la leche, se lleve otro artículo y la empresa gane más dinero de forma más genérica.
	
	Supongamos que un cliente quiere comprar un producto y para eso va a la tienda. Si ese producto puede asociarse correctamente a un producto b, se puede hacer que una persona entra a comprar el producto A y de regalo, colocándolo al lado. También se lleve el producto B.
	
	En el ejemplo, cada observación es un cliente con su cesta de la compra y se lleva una serie de productos específicos.
	tengo 7500 cestas de la compra o 7500 usuarios.
	
	Soporte:
		se trata del número de transacciones o la proporción de transacciones que contienen un determinado ítem y dividido por el número total de transacciones. Es un argumento que nos indica con qué presencia mínima debe estar un item para ser considerado como el objetivo de una regla de asociación. Entonces, lo que me interesa es que aparezcan reglas con un soporte más elevado que un mínimo que yo establezca aquí, que realmente no me salgan reglas de asociación de un item que se compra solo una vez
		lo que me interesa es que aparezcan reglas con un soporte más elevado que un mínimo que yo
		establezca aquí, que realmente no me salgan reglas de asociación de un item que se compra solo una
		vez a la semana
		
		necesito que sean relevantes
		
		Entonces esto va a depender del objetivo de la empresa.

		Esto hay que hablarlo con el jefe de la empresa para decidir qué ítems son los que vamos a proceder
		a estudiar y cuáles son los que el cliente suele colocar dentro de su cesta de la compra, al menos
		un mínimo de veces.
		
		Los datos de python corresponden a ventas semanales.
		
		El soporte, pues quiero ítems que se compren un mínimo de tres veces al día, por ejemplo, 3% son
		21 veces a la semana.
		Por tanto, en este caso el soporte será el cociente entre 7X3 21 dividido sobre el número total de
		transacciones.
		Busco aquellos ítems que aparecen por lo menos siete por tres 21 dividido 7500 veces que tienen una
		relevancia superior por lo menos a ese mínimo.

		Es decir, que existan al menos tres transacciones al día por siete.
		21.
		Por qué tenemos el dataset con una semana de ventas del centro comercial?
		Entonces busco la presencia de ítems que salgan al menos en 21 cestas de la compra sobre el total del
		data set
		Y eso evidentemente es un porcentaje, el soporte, que era un porcentaje y que en nuestro caso lo podemos
		calcular directamente.
		Simplemente hay que dividir la frecuencia total de los ítems con respecto al total del dataset.
		De acuerdo, lo voy a hacer aquí en la zona de la consola.
		Simplemente tres por siete dividido 7500 es la frecuencia que estoy intentando observar.
		
		Esto nos daría un soporte mínimo de 0,0028. redondeado a 0,003
		 es una proporción.

	nivel de confianza
	
				Diferentes niveles de confianza nos darán cestas diferentes si somos muy poco permisivos, si queremos
		que la confianza sea muy elevada de los elementos con soporte mínimo, sólo me voy a quedar con los
		que realmente muchísimas, muchísimas veces, aparezcan juntos en la cesta de la compra.
		Lo normal es empezar con un nivel de confianza alto que ocurra en el 80% de las cestas de la compra
		y luego ir bajando para ver cómo van apareciendo reglas de asociación y cuáles de ellas son buenas o
		cuáles no, evidentemente se pondría en producción.
		Miraríamos, hay un impacto en los ingresos y luego podríamos volver, cambiar el soporte y o el nivel
		de confianza, volver a experimentar con nuevas reglas de asociación más fuertes y así llegar a la forma
		de optimizar, en definitiva, nuestra tienda.
		
		En R fue empezar con un valor alto, con un nivel de confianza de 0,8.
		
		Nos dimos cuenta de que no había reglas de asociación con el nivel de confianza, ya que era demasiado
		elevado.
		Lo bajamos a un nivel inferior, de hecho lo baje a 0,4 y al bajar simplemente lo que permití es que
		en lugar de buscar reglas que aparecieran el 80% de las cestas de la compra que podéis imaginar, 80%
		de las cestas de la compra son muchas.
		Entonces, al bajar el nivel de confianza, no necesito que esta información aparezca o esta asociación
		aparezca en tantas cestas de la compra.
		De modo que la forma de razonar es esa de que sabiendo que los ítems se compran un mínimo de 21 veces
		por semana, lo que quiero es qué porcentaje de cesta de la compra aparecen esos ítems juntos.
		
		Se busca asociaciones lógicas entre esos productos, no casualidades, 
		no cosas irrelevantes.
		Entonces para esto sí que hay que intentar jugar con el nivel de confianza.

		Si estableces una confianza muy alta, pues no obtendréis ninguna regla de asociación.
		Y si sois demasiado permisivos, evidentemente encontraréis correlaciones entre los productos de la
		cesta de la compra, un poco inútiles.
		
		Usamos el valor de R un nivel de confianza de 0,2.
		
		Lift:
		
		lift, es el cociente entre soporte y nivel de confianza
		
		las reglas de asociación lo único que nos indican es el cociente, nos hacen un ranking.
		De qué tan buena es esa asociación de productos

		La idea es establecer un mínimo para que solo se me imprima las reglas más relevantes en lugar de un
		monto reglas de las cuales la mitad son inútiles.
		
		esto dependerá del problema tratado.
		
		Se busca por Lift decendente.
		
		El Lift era la medida de la fuerza de una regla de asociación.
		
		Salida de Python
		
					RelationRecord(items=frozenset({'chicken', 'light cream'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)])
					
					Analisis:
							support=0.004532728969470737
									0,45 % de las compras incluyeron el pollo.
							confidence=0.29059829059829057
									29% de las compras incluyeron el pollo y la crema ligera.
							lift=4.84395061728395
									Lift proporcion entre soporte y confianza es muy alta.
									
					RelationRecord(items=frozenset({'escalope', 'mushroom cream sauce'}), support=0.005732568990801226, ordered_statistics=[OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)])
							support=0.005732568990801226
								el 0,57 de las compras incluyen el escalope
							confidence=0.3006993006993007
								30% de las compras incluyen escalope y mushroom cream sauce
					
					RelationRecord(items=frozenset({'escalope', 'pasta'}), support=0.005865884548726837, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)])
					
					RelationRecord(items=frozenset({'fromage blanc', 'honey'}), support=0.003332888948140248, ordered_statistics=[OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)])
		

- Eclat
		
  Es una version simplificada de las reglas apriori.
  
  Las reglas de asociación resolvía el problema de la gente que compró este articulo tambien, compro este otro.
  
  De modo que a base de mirar cómo se comportan los usuarios, buscamos reglas generales que nos indiquen  que la gente que ha visto la película A, también ve la película B, o que en general toda la gente que ve la película A, le interesa ver también la película B. Se establecen relaciones fuertes de que el haber hecho algo, el haber visto en esta película, implique también ver esta otra película. Como ejemplo.
  
  Cada una de estas reglas puede tener diferente probabilidad, diferente fuerza.
  Nos interesa buscar las reglas más fuertes.
  
  En el algoritmo analizaremos qué tan probable es que un conjunto venga a la par, que dos películas se vean a la vez o que tres peliculas se vean a la vez o que un paquete de peliculas vayan a la vez
  
  No hace falta hablar del lift o confianza como en apriori. Simplemente vamos a utilizar el soporte de un conjunto. Vamos hablar de conjunto de cosas. 
  
  A las relaciones que salen del algoritmo hay que analizarlas para saber si son importantes. En Apriori usabamos el lift descendente.En Eclat hay que establecer una medida de qué tan imporante es que ocurra una cosa junto a la otra.La importancia de esa regla. Esas combinaciones son las que ahora tenemos que investigar. Eclat se basa en el soporte.
  
  Vamos a medir el soporte, mo me hará falta ni la confianza ni el lift. Medimos el soporte del conjunto y no de un item como en Apriori.
  
  Se define la importancia con el nivel de soporte de un conjunto. El modelo de eclat no tiene sentido ser evaluado para item.Miramos conjunto de items. No nos hace falta la confianza ni el lift.
  
  En eclat vemos cuál es la frecuencia de que los ítems vengan empaquetados, que vengan juntos. Eso es lo que va a calcular el soporte del conjunto.
  
  Ejemplo, qué tan probable es o qué tan juntas suelen ir la película interestelar con la película El naufrago. Qué porcentaje de los usuarios ven esas dos películas a la vez o que les ha gustado las dos peliculas a la vez, que sean un pack. Es decir, si hipotéticamente tenemos 100 usuarios en nuestro dataset, cuántos de esos usuarios han visto las dos películas, que se hayan interesado a la vez por Interestelar o por El naufrago. Empaquetamos las cosas. Si, por ejemplo, tuviéramos un 80% de los usuarios de la base de datos han visto esas dos películas, tenemos una muy alta probabilidad de que esas dos películas vayan en pares (juntas).Por tanto, si uno ha visto una, automáticamente podemos recomendar la otra.
  
  Si lo compran juntos, si es probable que aparezcan juntos.  Los que han comprado una cosa podemos recomendarle la otra.
  
  Apriori es mas potente. Eclat es mas simple y rapido (solo tiene el soporte)
  
  Algoritmo.
  
		Paso 1. Se selecciona un soporte minimo a utilizar.
		Paso 2. Tomar todas las operaciones con soporte superior al minimo.
		Paso 3. Ordenar por soporte descendente. Elegir las mas relevantes. Por ejemplo Top 10.
		
	En el lenguaje R:
	
		Tiene un solo parametro (el soporte). Las reglas que me van a salir en esta clase no van a ser las mismas que las que he obtenido con el modelo apriori.
		
		La diferencia está que ahora voy a tener la posibilidad de hablar de conjuntos de productos comprados en pack, no que un producto implique la compra de otro, sino que productos que aparecen en pack será como una especie de nube de productos que aparezcan juntos.
		
		rules = eclat(data = dataset, 
                parameter = list(support = 0.003, minlen = 2))
				
				minlen: indica que minimamente cada transaccion tenga dos elementos. Evita los singleton. Que no sean transacciones de un item. Por lo menos dos. 
				
		Lo que tengo no son reglas de asociación, sino que son subconjuntos de elementos que tienden a aparecer juntos. 
  
  
		Así aparecerán los diez soportes más altos, los diez subconjuntos de elementos que más seguro estamos de que aparecen en conjunto. No hay A implica B como en apriori. Aqui tenes subconjuntos de artículos comprados con más frecuencia.
		
		Por ejemplo, el que más aparece conjuntamente es el agua mineral con los espaguetis para hervir. Tiene un soporte cercano al 6% (soporte 0,059). Significa que aparece por lo menos en seis, en el 6% del total de celdas de cestas de la compra y representa 448 cestas de la compra con respecto al total.



------------------------------------
Parte 6 - Reinforcement Learning (o aprendizaje por refuerzo)
------------------------------------

Reinforcement Learning (o aprendizaje por refuerzo) es una rama del Machine Learning, llamada también Online Learning. Se utiliza para resolver problemas de interacción, donde los datos observados hasta el momento t son considerados para decidir qué acción se lleva a cabo en el momento t + 1.

En Inteligencia Artificial cuando se entrenan máquinas capaces de hacer tareas como caminar, o conducción de coches inteligentes.

Un resultado esperado proporciona una recompensa a la IA, mientras que uno no deseado le otorga un castigo. En este caso las Máquinas aprenden a través de ensayo y error.

Algoritmos de Reinforcement Learning (aprendizaje por refuerzo):

- Upper Confidence Bound (UCB)

- Muestreo Thompson


- El problema del bandido multibrazo:
	
		Hay diferentes formas.
		
		Este problema no es el único que se puede resolver con aprendizaje por refuerzo.
		
		Es un ejemplo generico.
		
		Los algoritmos de aprendizaje por refuerzo permiten por ejemplo que un perro robot sea capaz de caminar y no chocar con una pared, de subir un escalón o cosas de este estilo. En el caso del robot es súper fácil de entender, porque para enseñarle a caminar hay que enseñarles que primero hay que poner el pie derecho y luego el pie izquierdo de delante, luego el pie derecho de atrás, izquierdo de atrás y en base a ir combinando esos movimientos uno tras otro, es cómo se va enseñando y educando a un robot en forma de perro, a una tarea concreta que es el caminar. Básicamente le voy dando las acciones que puede tomar mover pierna derecha, izquierda, delante, detrás y le enseño cuando mover una pierna es algo positivo y cuando no, cuando da un paso adelante y no se cae, le doy una recompensa. Cada vez que se cae le doy un castigo.
		
		Normalmente la recompensa es el número +1 y el castigo suele ser cero o -1.
		De modo que lo que quiere el perro es maximizar las recompensas. Lo que hace es recordar qué acciones han sido buenas y las intenta repetir cada vez.
		
		Y así es como se enseña a caminar a un perro. De modo que no hay que programar. 
		
		Esto se mete dentro de una rama diferente, ya no es machine learning, ya es inteligencia artificial.
		
		- El bandido multibrazo. Podemos pensar en un ladrón que va al banco y que tiene toda una banda organizada, múltiples brazos operativos para poder atracar correctamente el banco. Esto podemos simplificarlo bastante, porque básicamente un bandido multi brazo no es ni más ni menos que una tragamonedas.
	
		El problema del bandido multi brazo es una especie de juego psicológico. Cuando entras en una sala de casino en lugar de tener una tragamonedas, tenéis muchas tragamonedas. La idea es que cada una de estas máquinas tragamonedas tendrá una programación diferente y llevará más o menos tiempo jugando, más o menos tiempo dando vueltas la barra de los premios. Se llama multi brazo porque es un problema que sirve para resolver o elegir la mejor tragamoneda que maximiza la victoria del jugador.
		
		Esto tendrá una aplicación directa al mundo de los problemas de una empresa cuando haya elecciones múltiples que queremos elegir una de ellas para maximizar.
		
		El problema es descubrir cuál es la distribución de premios de cada una de estas máquinas. Qué distribución tienen por detrás, una distribución de números, de beneficios, de premios a partir del uso de las mismas y así poder elegir cuál de todas estas máquinas tiene un resultado mejor.
		
		Cada una de ellas tendrá una distribución diferente. De antemano no sé cuál es la distribución de estas máquinas, se complica un poco. No puedo ir jugando en cada una de ellas porque terminare arruinado para descubrir su distribucion de premios. El objetivo es determinar cuál es la mejor distribución que podemos elegir entre las tragamonedas. Cada una de ellas tiene una distribucion diferente. La idea es elegir la mejor maquina que tenga la mejor distribucion. Como determinar la mejor distribucion.
		
		Cada maquina tiene diferentes distribuciones de probabilidad de pago de premio. Mas a la derecha mayor el premio (cola mas larga a la izquierda). Mas alto el pico mayor la probabilidad de pago. La D5 es la mejor conociendo esas distribuciones. Se busca obtener ese resultado con el algoritmo de Reinforcement Learning. Se buscan los limites de confianza hasta donde estamos dispuestos a llegar. Si se supera ese limite se incurrira en una frustracion. El objetivo es encontrar la mejor distribucion psando el menor tiempo posible 	jugando. Mayor tiempo mayor perdida de dinero.
		
		Otro ejemplo, imaginemos una campaña de coca-cola con varios anuncios. Cada uno de los cuales generan datos en base a una campaña de marketing. Son 5 anuncion cada uno con sus correspondientes campañas de marketing. El objetivo es buscar que anuncio funciona mejor para maximizar las ventas. No tenemos una distribucion al inicio de la campaña. Las distribuciones de cada campaña las conozco despues de que miles de personas miren los anuncios en sus correspondientes campañas. Que se hace, basicamente se toman los 5 anuncios y se hace un test AB con multiples pruebas A y B hasta tener una muestra grande y concluir con cierto nivel de confianza que anuncio otorga mejor resultado. El problema es que se suele gastar mucho dinero y tiempo. Las Test AB no es la mejor opcion que se puede aplicar practicamente. Si podemos conocer las distribuciones de cada anuncio en lugar de un test AB podemos obtener mejores resultados en el menor tiempo posible. Mejor de un test AB aplicar las tecnicas de Reinforcement Learning

---------------------------------
- Upper Confidence Bound (UCB)
---------------------------------

		Limite de confianza superior (UCB) 
		
		Intenta solucionar el problema del bandido multi brazo con múltiples brazos, que básicamente se resumía en que teníamos cinco máquinas tragamonedas y teniamos que averiguar como Cómo para maximizar las devoluciones de las máquinas en base a intentar descubrir las distribuciones. Hay una distribución para cada una de ellas y la desconocemos. No sabemos muy bien cuál de todas es la óptima. Por tanto, necesito combinar la exploración de estas máquinas haciendo apuestas para intentar extraer información.
		
		La aplicación moderna de este problema es entrar dentro del mundo de la publicidad, de modo que si tenéis cinco o diez, 15, 500 anuncios diferentes, hay que averiguar cuál de ellos es el mejor. Se intenta averiguar cuales son las distribuciones. Quiero combinar la exploración de los datos con las técnicas de learning para obtener el resultado más óptimo e intentar maximizar todo el dinero de la empresa para que no se arruine.
		
		Algoritmo:
		
			Cómo se puede afrontar este problema?
			Tenemos de brazos. Por ejemplo, los brazos pueden ser los anuncios que mostramos a los usuarios cuando se conectan a una página web o podrían ser cada una de las máquinas tragamonedas.
			
			Cada vez que un usuario se conecta debo elegir qué anuncio tengo que mostrar cada vez o a qué máquina tragamonedas tengo que jugar cada vez.
			
			Para ello, En cada ronda N se elige uno de los anuncios a ser mostrado o una de las tragamonedas  a ser jugado.
			
			En cada ronda que juegue o cada anuncio que muestre, voy a otorgar una recompensa, el anuncio y dará una recompensa en forma de número cero o uno.
			
			De acuerdo, si el usuario hace clic en el anuncio o si el usuario tiene un beneficio en la máquina tragamonedas pues la recompensa para la  ronda N será uno y cero cuando no. Por tanto, cuantificado las recompensas que recibe el usuario en cada momento y como en principio, todo esto será diferente, el objetivo será maximizar la recompensa a través de las rondas que se lleven a cabo. 
			
			Lo anterior es los pasos de nuestro algoritmo.
			
			UCB se basa en calcular un intervalo de confianza.
			
			Algoritmo:
			
			Se extrae informacion mediante la accion directa sobre un tragamoneda o un anuncio. Se trata de usar el menor tiempo posible en identficar las distribuciones evitando arruinarse economicamente.
			
			Al incio podemos suponer que tenemos la misma media distribucion para todos los anuncios o tragamonedas. Son todas iguales hasta que tenga evidencia de lo contrario. Todas las mismas medias. El algoritmo crea una banda que representa el beneficio del anuncio o la tragamoneda. El objetivo es encontrar la banda con el tope superior mas arriba. En base a rondas de prueba comienzo a modificar las medias y se recalcula la anchura del intervalo de confianza, se mueve el limite superior y el limite inferior.
			
			Partimos de una misma media y distribucion (General) para todos y el intervalo de confianza es grande. 
			
			PASO 1: Ronda n. Elijo una tragamonedas o anuncio cualquiera. Si paga o el usuario hace click obtengo un beneficio 1 en otro caso 0. Si no hace click en el anuncio baja la media con respecto a la general y la amplitud del intervalo de confianza (tamaño de la caja).   
			
			Se agrega toda la información de todo lo observado con respecto a la máquina elegida en la ronda. Al otorgar 0 porque no es seleccionado el anuncio, el valor de puntos discontinuos (representa la media) baja porque la media es ligeramente inferior a como era anteriormente. Cuando n sea bastante grande, la media converjira a la media de la distribución. Cuanto mayor sea la n ira bajando el tamaño de la caja paulatinamente. Ira bajando el intervalo de confianza. A mayor n, se reduce la amplitud del intervalo de confianza. El proceso de converjencia sera muy lento.
			
			PASO 2: Se elige otra maquina y se repite el experimento. Si se otorga 1  porque es seleccionado el anuncio, el valor de puntos discontinuos (representa la media) sube porque la media es ligeramente superior a como era anteriormente.  La amplitud del intervalo de confianza (tamaño de la caja) reduce tambien.
			
			PASO 3: Se realizan cientos o miles de iteraciones. Se eligen la maquina o anuncio con mayor limite superior. Sin embargo, a veces hay que pasar a otra maquina o anuncio con menor limite superior porque puede afectar los experimentos negativamente pero al final puede mejorar el limite del intervalo de confianza.
			
			PASO 4: Luego de repetir las iteraciones, se elige la de mayor limite de confianza. Aunque se debe alternar con las otras alternativas para evitar correr riesgo, ya que puede afectar los experimentos negativamente pero al final puede mejorar el limite del intervalo de confianza. Entonces habrá una estrategia específicamente diseñada para que en cada ronda de los resultados previos que hemos observado, influyan en qué vamos a mostrar en el siguiente. Y la clave para entender acerca del aprendizaje por refuerzo es precisamente que tenemos unos diez anuncios, lo que ocurre es que el algoritmo observará paulatinamente los resultados obtenidos durante las primeras rondas, durante las primeras diez rondas, las primeras diez veces que se muestre el anuncio y en base a esto decidiremos qué anuncio es el que se volverá a mostrar al usuario. En realidad como correríamos esto simultáneamente, mostraríamos el anuncio y el algoritmo evaluaría, no tendríamos un dataset como tal, sino que iríamos teniendo datos a medida que el usuario mira un anuncio. Básicamente, el usuario irá viendo cada vez los anuncios con mayor recompensa con los que tienen mayor CTR, mayor índice de clics. Recordemos, que el presupuesto es limitado.
			
			La seleccion aleatoria cambia el resultado a ser re-ejecutado el experimento. Evidentemente si vuelvo a ejecutar aquí el resultado es 1234. Lo interesante aquí es que el valor final tampoco supera demasiado. Esta entre 1200 y 1400. La idea es que gracias a nuestro algoritmo más avanzado, el confidence mount o el muestreo Thomson, quiero obtener un resultado mejor que 1200 o 1300, que es lo que estoy obteniendo hasta el momento con seleccion aleatoria.
			
			En el ejercicio en python con el algoritmo UCB casi hemos duplicado (2178) la recompensa total obtenida por el algoritmo de selección aleatoria (1200 a 1300), de modo que los anuncios mostrados a los usuarios ahora son mucho más optimizados.Se redoble el numero de ventas.
			
			En python se elige el nro 4.
			
			
			
