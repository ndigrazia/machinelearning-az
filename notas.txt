https://docs.google.com/spreadsheets/d/1I6hcZlYZqGIQEsnKMoXMVsccITrSsbFdnLyJyAk55-Q/edit?pli=1#gid=119576526

https://github.com/joanby/tensorflow2

Machine Learning de la A a la Z (LIBRO)
https://joanby.github.io/bookdown-mlaz/clasificaci%C3%B3n.html#regresi%C3%B3n-log%C3%ADstica

Machine Learning A-Z: Download Codes and Datasets
https://www.superdatascience.com/pages/machine-learning

Repositorio del Curso Machine Learning de A a la Z: R y Python para Data Science
https://github.com/joanby/machinelearning-az

https://discord.com/invite/Gq5NX6a	

Bienvenido a la Parte 2: Regresión
Los modelos de regresión (tanto lineal como no lineal) se utilizan muchísimo para predecir valores numéricos como por ejemplo el sueldo. Si nuestra variable independiente es tiempo entonces podemos hacer predicciones de valores futuros, sin embargo nuestro modelo puede predecir también valores desconocidos del presente.  Las técnicas de Regresión son muy variadas, desde la regresión lineal hasta la SVR o la Regresión con Bosques Aleatorios.

En esta parte, vamos a entender e implementar los siguientes modelos de Regresión dentro del Machine Learning:

Simple Linear Regression

Multiple Linear Regression

Polynomial Regression

Support Vector for Regression (SVR)

Decision Tree Classification

Random Forest Classification

-----------
Regression
-----------

Modelos de regresion lineal

Modelos de regresion lineal tienen una series de restricciones que se deben comprobar antes de aplicar el modelo de regresion:
	-linealidad
	-homocedasticidad
	-Normalidad multivariable
	-Independencia de errores
	-ausencia de multicolinealidad
	
Si no se da alguna de las restricciones anteriores el modelo de regresion lineal no tiene sentido.

-	Simple Linear Regression (SLR)

	En el grafico las variables independientes van en el eje de las x y la dependiente en el eje de las Y.

	y = b0 + b1 * x1
		b0 constante (ordenada all origen)
		b1 coficiente
		x1 variable independiente
		y variable dependiente (a predecir)
		
		Para identificar la recta se usa el "metodo de los minimos cuadrados"
						  ^   2
			min	SUM (yi - yi)
				   i
		(SLR) no requiere scalado
		
		La recta de regresion es la misma para los datos entrenados como los datos de test. Se deja el conjunto de entrenamiento.
		

- Multiple Linear Regression	

		(MLR) no requiere scalado
		
		y = b0 + b1 * x1 + b2 * x2 + ... + bn *xn
			b0 constante (ordenada all origen)
			bi coficientes
			xi variables independientes
			y variable dependiente (a predecir)
			
			notar:
				bi coficientes positivos(+) aportan valor a la y 
				bi coficientes negativos(-) restan valor a la y 
			
			Dummy: propiedad  categorica (no ordinal). Ejemplo: Ciudades. 
				
			Es contra producente agregar todas las columnas dummy. Se deben añadir todas las columnas dummy salvo una. Evita el efecto de multicolinealidad.
			
			y = b0 + b1 * x1 + b2 * x2 +  b4 * d1
				d: variable dummy
			
			Trampa de variables dummy:
			
				Al incluir todas las variables dummy obtenemos una multicolinealidad. Esto debe evitarse. El modelo es incapaz de determinar los efectos de todas las variables dummy
					y = b0 + b1 * x1 + b2 * x2 +  b4 * d1 + b5 * d2
							El modelo no puede determinar los efectos o impacto de d1 y de d2.
							d2 = 1 - d1
				
				SIEMPRE DEBEMOS OMITIR UNA VARIABLE Dummy. Esto evita el efecto de multicolinealidad.
				
				Si tuvieramos dos variable dummy, deberia quitar un valor para cada variable dummy. Por ejemplo, pais y sector. Quito un valor (de todos los valores posibles) para pais y uno (de todos los valores posibles) para sector.	
				
				https://www.wikihow.com/Calculate-P-Value
				https://www.mathbootcamps.com/what-is-a-p-value/
				
				Entre varios modelos con variables independientes, preferimos el que tenga la menos cantidad de variables independientes que ayuden a predecir la variable dependiente.
				 Porque?
					1- Puede ser que añadir mas variables no aporte a predecir la variable dependiente.
					2- Mayor variables independientes hace que complejo el modelo. (dificulta la explicacion del modelo)
					
					SOLO MANTENER LAS VARIABLES INDEPENDIENTES IMPORTANTES CON CAPACIDADES DE PREDECIR EL VALOR DE LA VARIABLE DEPENDIENTE. SE debe seleccionar las variables importantes capaces de predecir algo:
						5 Modelos de seleccion:
						https://www.youtube.com/watch?v=tCXc2zl3dew
							1- Exhaustivo (all-in).
							2- Eliminacion hacia atras.   -------------
							3- Seleccion hacia adelante.			   --------	Regresion paso a paso los incluye 
							4- Eliminacion bidireccional.  ------------
							5- Comparacion de scores.
												
							1- Exhaustivo (all-in)
				
								La usamos por:
								
								1. Metemos todas la variables y vemos que sucede, ya que conocemos (por el conocimiento del negocio o por necesidad) que todas son predictoras.
								
								o
								
								2. Preparacion previa a la Eliminacion hacia atras. 	
									
			
							2- Eliminacion hacia atras. (Backward)
								Elimina cualquier variable independiente que no sea significativa.
									Paso 1. 
									Elegimos un nivel de significacion para que una variable permanezca en el modelo. (Standard 0.05)
									
									Paso 2.
										Se calcula el modelo usando todas las variables como predictoras.
										
									Paso 3.
										Se considera la variable predictora con el p-valor mas grande. Si p-valor > Standard vamos al paso 4 sino a fin.
										
									Paso 4.
										Se elimina la variable con p-valor > Standard
									
									Paso 5.
										Se ajusta el modelo sin la variable con p-valor > Standard
									
									FIN - El modelo esta listo!
									
									Repetimos del paso 3 al paso 5 hasta no tener variables con p-valor > Standard. Nos quedamos con ese modelo.
							
									WARNING: Para aplicar esta seleccion en Python, se agrega una columna de todos unos a las columnas de variables independientes. Esta representa la ordenada al origen. PAra analizar si tiene significado la ordenada al origen en la funcion a calcular.
							
							3- Seleccion hacia adelante. (Forward)						
									Paso 1. 
									Elegimos un nivel de significacion para que una variable pueda entrar en el modelo. (Standard 0.05)
									
									Paso 2.
										Ajustamos todos los modelos de regresion lineal simple y ~ Xn (hacemos todos los modelos de regresion linial simple para cada variable independiente con la dependiente). Elegimos el que tiene menor p-valor.
									
									Paso 3.
										Se conserva esta variable, y ajustamos todos los posibles modelos con una variable extra añadida a la(s) que ya tenga(s) el modelo hasta el momento.
										
									Paso 4.
										Se considera la variable predictora con el menor p-valor. Si p-valor < Standard vamos al paso 3 sino a fin.
										
									FIN	- Conservar  el modelo anterior.
									
									Repetimos el paso 3 hasta  que la variable añadida supere el p-valor.  Nos quedamos con el modelo anterior.
									
							4- Eliminacion bidireccional (stepwise)
							https://www.youtube.com/watch?v=6wpTEwaFbY0
									Combina: Seleccion hacia adelante y Eliminacion hacia atras.
									
									Paso 1. 
										- Elegimos un nivel de significacion para que una variable pueda entrar en el modelo. (Standard 0.05) SLENTER
										- Elegimos un nivel de significacion para que una variable permanezca en el modelo. (Standard 0.05) SLSTAY
										
									Paso 2.
										Inicia como forward.
										Llevar a cabo el Paso de selccion hacia adelante (con las nuevas variables con p < SLENTER para entrar)
										
									Paso 3. 
										Se lleva adelante la eliminacion hacia atras (variables antiguas con p-valor < SLSTAY para quedarse)
										
										Repetir de paso 2 hasta 3 que no queden variables para añadir.
										
									El mejor de los tres modelos.
									
									FIN- Modelo esta listo.
									
							5- Fuerza bruta: (Comparacion de scores.)
							
									Genera todos los modelos de regresion posibles. Todos los de una variable, Todos los de dos variables, Todos los de test variables, etc.
									
									Paso 1.
									Seleccionar un criterio de bondad de ajuste. Ej: Vallesiano, Akaike,etc.
									
									Paso 2.
									Construir todos los modelos de regresion posibles. (2 elevado a la N) - 1. N numero de variables. Por ejemplo: Con 10 variables -> (2 elevado a 10) - 1 = 1023 modelos.
									
									Paso 3.
									Seleccionamos el modelo basado el criterio elegido. El mejor modelo.
									
									FIN- Modelo esta listo.
									
									
							Conclusion:
									No hay un modelo mejor que otro. Hay que conocer todos los metodos.
									
									Si cualquiera de los coeficientes es cercano a cero no es necesario agregarlo a la formula de regresion.
									 OLS-> ordinary list square (minimo de cuadrados ordinarios)
								
									Cuanto mas cercano es el R-Cuadrado Ajustado al 1 (uno) mejor se explica el modelo de la regresion lineal (Es el mejor modelo)
									R-Cuadrado Ajustado > 0.7 se suele aceptar qu el modelo es lineal.

									WARNING: No Dividir el conjunto de datos entre entrenamiento y testing cuando son pocos casos o datos.
									
Regresión Lineal Múltiple en Python - Eliminación hacia atrás automática

Si estás interesado en implementaciones automáticas de la Eliminación hacia atrás en Python, aquí te presentamos dos de ellas. Se ha adaptado el código para que utilice la transformación .tolist() sobre el ndarray y así se adapte a Python 3.7.

Eliminación hacia atrás utilizando solamente p-valores:

import statsmodels.formula.api as sm
def backwardElimination(x, sl):    
    numVars = len(x[0])    
    for i in range(0, numVars):        
        regressor_OLS = sm.OLS(y, x.tolist()).fit()        
        maxVar = max(regressor_OLS.pvalues).astype(float)        
        if maxVar > sl:            
            for j in range(0, numVars - i):                
                if (regressor_OLS.pvalues[j].astype(float) == maxVar):                    
                    x = np.delete(x, j, 1)    
    regressor_OLS.summary()    
    return x 
 
SL = 0.05
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_Modeled = backwardElimination(X_opt, SL)

Eliminación hacia atrás utilizando  p-valores y el valor de  R Cuadrado Ajustado:

import statsmodels.formula.api as sm
def backwardElimination(x, SL):    
    numVars = len(x[0])    
    temp = np.zeros((50,6)).astype(int)    
    for i in range(0, numVars):        
        regressor_OLS = sm.OLS(y, x.tolist()).fit()        
        maxVar = max(regressor_OLS.pvalues).astype(float)        
        adjR_before = regressor_OLS.rsquared_adj.astype(float)        
        if maxVar > SL:            
            for j in range(0, numVars - i):                
                if (regressor_OLS.pvalues[j].astype(float) == maxVar):                    
                    temp[:,j] = x[:, j]                    
                    x = np.delete(x, j, 1)                    
                    tmp_regressor = sm.OLS(y, x.tolist()).fit()                    
                    adjR_after = tmp_regressor.rsquared_adj.astype(float)                    
                    if (adjR_before >= adjR_after):                        
                        x_rollback = np.hstack((x, temp[:,[0,j]]))                        
                        x_rollback = np.delete(x_rollback, j, 1)     
                        print (regressor_OLS.summary())                        
                        return x_rollback                    
                    else:                        
                        continue    
    regressor_OLS.summary()    
    return x 
 
SL = 0.05
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_Modeled = backwardElimination(X_opt, SL)


Instalar la nueva versión de ElemStatsLearn en R
Los que tengáis problemas al instalar ElemStatLearn desde R probad con esta línea de código

install.packages("https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/ElemStatLearn_2015.6.26.2.tar.gz",repos=NULL, type="source")
									
						
Regresión Lineal Múltiple en R - Eliminación hacia atrás automática
Si quieres tener una implementación automática de la eliminación hacia atrás en R, aquí te la dejo:

backwardElimination <- function(x, sl) {
  numVars = length(x)
  for (i in c(1:numVars)){
    regressor = lm(formula = Profit ~ ., data = x)
    maxVar = max(coef(summary(regressor))[c(2:numVars), "Pr(>|t|)"])
    if (maxVar > sl){
      j = which(coef(summary(regressor))[c(2:numVars), "Pr(>|t|)"] == maxVar)
      x = x[, -j]
    }
    numVars = numVars - 1
  }
  return(summary(regressor))
}
 
SL = 0.05
dataset = dataset[, c(1,2,3,4,5)]
backwardElimination(training_set, SL)


- Regresion Polinomica (RP)

	Esta regresion tiene una unica variable independiente.-

	Regresion lineal -> 	y = b0 + b1X1
	Regresion multiple -> 	y = b0 + b1X1 + b2X2 + ... + bnXn
	Regresion polinomica -> y = b0 + b1X1 + b2X1^2 + ... + bnX1^n (tenes una misma variable (x1) con potencias sucesivas) 
	
	Cuando usar:
	
	Cuando los datos relevados siguen una parabola o una curva (no una recta).
	
	Regresion polinomica de grado 2 -> y = b0 + b1X1 + b2X1^2
	
	En la RP debemos elegir hasta que grado queremos. Grado 2 o Grado 3 o ... o Grado n. El grado tiene que ver con el exponente o potencia.
	
	¿Porque se llama Regresion lineal Polinomica?
		Es lineal por los coeficientes. Los coeficientes tienen una relacion lineal con las variables. 
		En la regresion lineal las incognitas son los coeficientes.
		La variable y se predise como combinacion lineal de esos coeficientes con las variables.
		

	WARNING: No se escalan los datos porque la relacion no lineal podria perderse.
	
	poly_reg = PolynomialFeatures(degree = 4) - degree representa grado del polinomio . Hasta que grado deseo tener las caracteristicas polimonialies de mi matriz original. Se elige el deseado.
	
	# Training the Linear Regression model on the whole dataset
	from sklearn.linear_model import LinearRegression
	lin_reg = LinearRegression()
	lin_reg.fit(X, y)

	# Training the Polynomial Regression model on the whole dataset
	from sklearn.preprocessing import PolynomialFeatures
	poly_reg = PolynomialFeatures(degree = 2)
	X_poly = poly_reg.fit_transform(X) 
	lin_reg_2 = LinearRegression()
	lin_reg_2.fit(X_poly, y)    
	
	El codigo anterior genera una matriz llamada X_poly con tres columnas. La primera el termino independiente (ordenada al origen - todos unos), la segunda columna el valor de la variable independiente y la tercer columna el cuadrado de la variable independiente.
	
	WARNING: En un modelo polinomial nunca se representa una recta.- Es una Parabola o curva. 
	
	
-	Regresion con Maquinas de Soporte Vectorial (SVR)

	Estos métodos están propiamente relacionados con problemas de clasificación y regresión. 

	Sirven para Regresiones lineales (Kernel lineal )como no lineales (rbf, etc...). Dependen de un nucleo (Kernel). El Kernel determina el tipo de regresion que lleva a cabo la maquina de soporte vectorial.
		
	Para regresion hay que ajustar el mayor corredor (o calle) posible entre dos clases. Intentando que en el interior de ese pasillo o clase queden la mayoria de los puntos. Hay que limitar la anchura (margen maximo). 
	Imaginemos una recta o calle entorno a la misma dos pasillos (uno por arriba y otro por debajo) donde queden la mayoria de los puntos. 
	
	En el caso de clasificacion se intenta que la anchura del pasillo separe las dos clases lo mas posible. En la Regresion encontrar la anchura que incluya la mayoria de los puntos. 

	Como controlamos la anchura del pasillo. Se establece un parametro llamado epsilon. A mayor valor de epsilon mayor anchura.	
	
	epsilon: Distancia entre las lineas de soporte vectorial y la linea de hiperplano. 
	
	La calle se llama hiperplano. Hay que obtener la ecuacion del hiperplano. Los puntos que hacen cero a la ecuacion estan en el hiperplano. Si el son distintos de cero partenecen a una clase o a otra. Para que funcione hay que encontrar la ecuacion que represente al mejor hiperplano (aquel que permite que no haya sesgo entre una u otra categoria). Es decir que permita realizar la mejor separacion. Las maquinas de soporte vectorial ayudan a encontrar el mejor hiperplano.
	
	El algoritmo detecta los puntos mas cercanos entre una clase y otra. Luego, encuentra la linea que los conecta. Finalmente, traza una perpendicular que divide esta linea en dos. Esta linea es el hiperplano optimo. Se busca maximizar el margen. 
	
	Vocabularion:
	
		Hiperplano = recta de regresion.
		
		Los vectores de soporte: los puntos mas cercanos entre una clase y otra. 
		Margen: distancia entre los vectores de soporte.
		Hiperplano optimo: frontera de separacion que consige la mayor separacion entre una clase y otra. 
		
		Problema:
			Si aparece un dato excepcional o valor a tipico (outlier) que lleva a reducir el margen y por lo tanto a producir un overfitting (clasificacion incorrecta de un dato nuevo por el algoritmo). Para solucionar este problema al alogritmo tradicional (Hard margin) se le agrega un parametro C (Soft margin) para flexibilizar el margen. Este valor de C es elegido por el diseñador en el entrenamiento. A menor C  mayor margen o a mayor C menor margen. El valor se escoge de manera emperica analizando el error obtenido en la clasificacion comparado con diferentes valores de C. Se escoge el menor error posible. Es decir, menor C.
			
	La idea de las Maquinas de Soporte Vectorial es obtener el mayor margen posible entre una clase y otra.
	
	Problema: 
		Maquinas de Soporte Vectorial (SVR) permiten obtener fronteras de clasificacion o hiperplanos lineales. A veces es necesario en aplicacion reales obtener hiperplanos o fronteras no lineales. Como solucion una alternativa es agregar mas dimensiones a cada dato para separar las dos categorias. Es decir, usamos Maquinas de Soporte Vectorial para obtener hiperplanos en mas dimensiones. Como agregamos mas dimensiones a los datos para poder clasificarlos: Kernel.
		
		Podemos pensar que cada punto de datos de entrenamiento representara su propia dimension. 
		
		Kernel: Toma el dataset de datos original y lo mapea a un espacio de mayor dimensiones usando una funcion no lineal. Con esta tranformacion el dataset es linealmente separable. Luego, se aplica Maquinas de Soporte Vectorial (SVR) para obtener el hiperplano optimo. Luego, obtenido el hiperplano optimo se vuelve al espacio original y finalmente, se realiza la clasificacion. Se puede representar la funcion en el espacio original.
		
		Kernel: Mapea y realiza el calculo del hiperplano. Para el mapeo usa funciones polinomiales o de gaussianas.
		
		Las Maquinas de Soporte Vectorial sirven para aumentar la dimensionalidad del problema y calcular la regresion en ese espacio de dimension superior. Luego ese funcion se puede volver al espacio de dimension original y proyectar la linea. 
		
		Objetivo de las Maquinas de Soporte Vectorial es encontrar una funcion de regresion a partir de los puntos de entrenamientos y que los errores no superen el umbral establecido (el epsilon)
		
	Algoritmo SVR:
	
		Analizar si se escalan los datos de X (independientes) y Y (dependiente)
	
		1. Elegir el conjunto de entrenamiento. (variables independiente y dependiente)
		2. Elegir la funcion de nucleo y sus parametros. Tambien, analizar si hay que realizar una regulacion adicional que elimine el ruido en el conjunto de entrenamiento. 
		3. Crear la matriz de correlaciones (K).  
		4. Entrenamos el modelo en forma exacta o aproximada para obtener los coeficientes de contraccion para cada uno de los datos.
		5. Con los coeficientes de contraccion creamos un estimador.	
		
		Se debe elegir el nucleo:
				- Lineal
				- No lineal - (Gaussiano - Es el defecto) Es el la constante rbf en python.
				
		Regulacion para eliminar el ruido de los datos.
		
		IMPORTANTE: Considerar escalar ya que el algoritmo se basa en distancias euclideas.
			
- Regresion con Arboles de desicion

		Los saltos en los arboles son discretos. Hay saltos en el diagrama del arbol.
		
		Puede tener varias variables independientes.
		
		En los arboles de desicon no suele ser necesario ningun tipo de escalado.
		
		Escalar si el algoritmo usa distancias euclideas. EL ARbol de desicion no utiliza distancias euclideas.
		
		
		CART
			Classification & Regression Trees
				Hay dos Tipos:
					---> Arboles de Clasificacion
					---> Arboles de regresion para predecir valor.
					
		Arboles de Regresion:
				 Dos variables independientes x1 y x2. Objetivo predecir una tercer variable dependiente Y.
				 
				 Predice no necesariamente de forma lineal. 
				 
				 El algoritmo divide los puntos (nuestros datos) en un conjunto de secciones. 	
				 El algoritmo mira la entropia. Como de juntos o dispersos estan los puntos (similitudes entre los puntos)-
				 Agrupa los puntos en comunes basado la entropia. Hay que establecer los puntos que se quedan en un nodo hoja (Ejemplo 5% de datos).
				 Se puede usar otra regla para definir los puntos que quedan en la hoja.
				 El algoritmo encuentra las divisiones optimas del conjunto de datos.
					
				 Alg ejemplo:
				 
					1. desicion X1 < 20
							Si -> X2 < 200
									Si -> 
									No -> 
							No -> X2 < 170
									Si -> X1 < 40 
											Si -> La proyeccion o prediccion es el promedio de los puntos que conforman la hoja o seccion de division. Para un mismo nodo hoja retorna la misma prediccion.
											No -> 
									No -> 
		Python:
				Objeto en python para representar un arbol de desicion es DecisionTreeRegressor.
				Cuando se crea se establece el criterios de division de los datos. Se suele utilizar la medida del "error cuadrado medio" como forma de minimizar la diferencia entre la prediccion y el resultado. Este criterio busca cual de las formas de cortar minimiza los cuadrados de los errores. Este es el criterio por defecto.
				
				regressor = DecisionTreeRegressor(random_state = 0)
				
				Enfoques mas avanzados suelen definir enfoques que corten por mas de un rasgo a la vez. En el constructor del objeto DecisionTreeRegressor se definen las caracteristicas o parametros del arbol de desicion. Tambien se puede definir el numero maximo de nodos hojas, numeros maximos de elementos que forman el nodo hoja, etc. Elige que parametros pasar al objeto.
				
				
	WARNING: hay que escalar o no? Por defecto no escalar y ver que buena es la prediccion y luego, ver como es la prediccion con los datos escalados. Escalar si el algoritmo usa distancias euclideas. No se usa en Arboles de desicion. 
	
	Cuando tenes una linea horizontal (en grafico) que abarca todo el conjunto de datos podria pasar que los datos estan mal interpretados. Puede ser que el arbol tuviera unas restricciones a la hora de dividir una rama en nodos hojas. Es decir, que una rama no se divide en nodos hojas si no hay suficientes datos para formar parte del nodo hoja. Tambien, puede ocurrir que no se dividen las ramas a menos que se establezcan ciertas condiciones o las condiciones de division estan mal establecidas. Por lo tanto, se agrupan muchos datos en una sola hoja y se otorga el promedio de todos los datos de la hoja a las proyecciones que caen en ese nodo hoja. Con parametros se puede cambier los criterios de division de ramas. En R npar.control(minsplit=1) Establece que una rama puede tener un dato para en su hoja. 

	 		
- Regresion con Bosques aleatorios

	Se usan para regresion y para clasificacion.
	
	los saltos en los arboles son discretos.
	
	Los arboles cuando hacen la prediccion lo hacen sobre un conjunto discreto de datos.

	Este algoritmo entra en Aprendizaje en Conjunto. Basicamente, toma un algoritmo y lo replica n veces o toma varios algoritmos diferentes y los junta para obtener un mejor algotimo.  Aprendizaje en Conjunto permite tener algoritmos mas estables.
	
	Muchos arboles de desicion conforman un bosque aleatorio.
	
	En este caso toma un algorimto de arboles de desicion y los replica n veces.
	
	Pasos:
	
	Paso 1. Se elige un numero K (numero aleatorio). Representa un subconjunto de los datos del conjunto de datos de entrenamiento. En lugar de dividir los datos en conjunto de entrenamiento y conjunto de test. Elegimos un subconjunto de datos de los datos de entrenamiento.
	
	Paso 2. Construir un arbol de desicion para los K puntos de datos. Lo que hace que el arbol tenga una vision parcial de los datos.
	
	Paso 3. Se define el numero de arboles de desicion y se repite el paso 1 y 2. Cada nuevo arbol tiene un subconjunto de datos del total de datos de entrenamiento.
	
	Paso 4. Cuando llegamos a construir el numero de arboles deseados. Se puede usar todo el bosque para hacer la prediccion. A cada arbol se le indica que realice la prediccion de Y. Finalmente, la prediccion final es el promedio de las predicciones Y dadas por los arboles construidos.
	
	Este algoritmo mejor la prediccion de un solo arbol de desicion. 
	
	Cuando hay valores outlier (valores extremos) podemos tomar la mediana en lugar de la media en cada arbol. Tambien se puede usar la media recortada.
		
	Python:
		RandomForestRegressor(n_estimators = 10, random_state = 0)	
			Parametros de ForestRegressor
					n_estimators -> cantidad de arboles del bosque.
					criterion -> criterio para dividir una rama en dos subramas o nodos hojas.
								Criterio de division: MSE (error cuadrado medio) diferencia entre la prediccion y el valor real al cuadrado. Se intenta minimizar. Se divide la rama en dos subramas intentando que el MSE de cada rama sea menor al MSE de la rama que se divide. Tambien, hay otros criterios de division.
					max_features -> caracteristicas tenidas en cuenta a la hora de generar las divisiones.
					random_state -> reproductivilidad para obtener el mismo bosque.
					
			
	
	 IMPORTANTE: Cuando se conbinan diferentes modelos o algoritmo (Aprendizaje en Conjunto) en varias maquinas sale a relucir mejores predicciones.
	 

Generalidades:

	 R Cuadrado:
	 
		Buscamos el modelo que predise el menor error posible con respecto a los datos dados.
		
		R 2 = 1- SSE/SST 
		
		SSE es la suma de cuadrados de error o residuos - Se calcula sumando las diferencias al cuadrado entre los valores predichos y los valores reales de la variable dependiente.
						
		SST es la suma de cuadrados total - Es la suma de los cuadrados de las diferencias entre cada punto de datos (valores reales de la variable dependiente-valores observados) y la media de todo el conjunto de datos
		
		Cuanto mejor sea mi modelo mas cerca estara R Cuadrado del numero uno. Mide que tan bueno es el modelo que elijo con respecto a una prediccion de media para todos los valores del conjunto de datos.
		
		Si el R Cuadrado es negativo el resultado es un pesimo modelo de prediccion.
		
		Los algoritmos o tecnicas de regresion se enfocan en mejorar el R Cuadrado para indicar lo bueno que es mi modelo para predecir los datos.
		
		En R se llama "multiple R-squared"
		
		El valor mas grande que puede tomar es uno. 
		
		R cuadrado no es un estimador muy exacto de la bondad de ajuste para algoritmos de variables multiples. Siempre sera mejor o mayor cuanta mas variables agreguemos. Necesitamos usar un valor u otro indicador que no este sesgado. Usamos R Cuadrado Ajustado. (regresion lineal multiple)
	
	 R Cuadrado Ajustado:
	
		Regresion Lineal multiple: El R Cuadrado se sesga y no sirve para medir o evaluar modelos con numero de variables diferentes. Es decir, si agregar una variable mas al modelo lo mejora y en consecuencia, mejora la prediccion. (R2 nunca decrece, se incrementa). En ese caso hay que usar el R Cuadrado Ajustado para evaluar si agregar una variable mejora nuestro modelo.
		
		Adj R 2  = 1 - (1 - R2) * (n - 1) / (n - p - 1)
		
		R2 es el R cuadrado
		n tamaño de la muestra.
		p numero de variables independientes (o variables regresoras) en nuestro modelo.
		
		Al agregar mas variables independientes hace que el R 2 decrezca, ya que penaliza el cociente.	
		
		Adj R 2 Evalua si gana mas el valor del (1 - R2) o el cociente ((n - 1) / (n - p - 1)) por agregar esa nueva variable.
		
		
		A veces el umbral de rechazo de una variable para pertenecer al modelo (regresion multiple), el p valor, es muy cercano al nivel que tomemos como umbral. Si no señimos al algoritmo deberiamos rechazar la varible pero como su valor es muy cercano podriamos tener presente otros valores. Dar una segunda opinion si eliminamos o no ese variable.  
		
		Nos quedamos con el modelo de mayor R Cuadrado Ajustado cuando comparamos varios modelos en regresion multiple para determinar si agregar o eliminar una variable aporta al modelos. Cuando comenzas a sacar variables del modelo y cae el R Cuadrado Ajustado debemos para de eliminar variables (metodo de reduccion hacia atras).
		
     
	 Interpretar Coeficientes de Regresion Lineal
	 
		1. Signo del coeficiente. (Negativo) aporta negativamente a la prediccion.  (Positivo) aporta positivamente a la prediccion.  
		2. La magnitud del coeficiente. (Ojo con las unidades de los coeficientes) Analizar si son diferentes sus escalas. SIEWMPRE Analizar las unidades de los coeficientes. Hacer transformaciones para que todo cobre sentido.
		
		Interpretar el coeficiente como el incremento o decremento "por unidad de" ... (Dolar, Empleado, distancia, etc.). La unidad de la variable independiente que aporta positivamente o negativamente al modelo.

Fin de la Parte 2 - Regresión

¿Cómo sé qué modelo debo elegir para resolver mi problema?

Paso 1.

Averiguar si el problema es una prediccion (regresion), clasificacion o agrupamiento (clustering). Para ello analizamos la variable dependiente.

Si no existe la variable dependiente es un agrupamiento.
Si tiene la variable dependiente y la variable es continua o discreta es una regresion. Finalmente, si la variable dependiente es categorica es una clasificación.

Paso 2. Preguntarse si tu problema es o no es lineal. Se puede responder con la tecnica Grid Search.

Una vez lo sepas, Si tu problema es lineal, deberás intentar crear un modelo de Regresión Lineal Simple si solo tienes una variable independiente o un modelo de Regresión Lineal Múltiple en el caso de tener varias.

Si tu problema no es lineal, entonces tenemos varias técnicas donde elegir, como la Regresión Polinómica, SVR, Árboles de Decisión y Bosques Aleatorios. ¿Cuál te funcionará mejor? El método en este caso consiste en utilizar una técnica muy útil para evaluar modelos llamada k-Fold Cross Validation, y elegir el modelo que demuestre mejores resultados. 
 
¿Cómo puedo mejorar cada uno de estos modelos?

Existen dos tipos de parámetros en nuestros modelos:

- los parámetros que el modelo aprende, como los coeficientes de la Regresión Lineal,

- los hiper parámetros del algoritmo. En este último caso, los hiper parámetros son parámetros que el algoritmo no aprende, si no que son fijos y forman parte de las ecuaciones de los modelos. Por ejemplo, el parámetro lambda de regularización o el factor de penalización C son hiper parámetros.

---------------------
Seleccion de modelos.
---------------------

- - - 
K-Fold Cross Validation

Se utiliza para proporcionar una evaluacion relevante de la eficacia de nuestro modelo. 
Permite comparar diferentes algoritmos (Logistica, vectorial, KNN)y obtener una idea de cuan bien trabajaran.

Incrementa el rendimiento del modelo.

En la practica es comun dividir los datos en 10 bloques (Fold) - Python (cross_val_score (cv = nro. bloques)

Se muestrea varias veces nuestro dataset de partida. Incrementa el rendimiento del modelo porque todos los datos se usan para entrenar y evaluar.

Juzgar el rendimiento de nuestro modelo con un solo conjunto de datos de prueba no es el mejor enfoque (problema de varianza de los datos). No es la mejor forma de evaluar el rendimiento de un modelo.  K-Fold Cross Validation soluciona el problema de las varianzas entre un conjunto de datos de prueba.

Compensar el sesgo de la varianza

	Sesgo Bajo: diferencia baja entre la prediccion y el valor real observado. 
	Sesgo Alta: diferencia alat entre la prediccion y el valor real observado. 
		
	Varianza baja: Ejecutamos varias veces el modelo y la prediccion no varia demasiado.
	Varianza alta: Ejecutamos varias veces el modelo y la prediccion varia demasiado.
	
	En Python el resultado de aplicar este modelo es un array con la presicion de cada una de las corridas. Resultado final es la media de los resultados obtenidos y el desvio estandard.
	
- - - 
Grid search

Optimiza los hiperparametros. 

Nos permite identificar si es mejor elegir un modelo lineal o no lineal.

Trata de encontar los valores optimos de los hiperparametros. Hiperparametros son los que elige el usuario.

Realizamos primero la evaluacion deñ renmdimiento del algoritmo (K-Fold Cross Validation) y luego, ajustamos los hiperparametros para optimizarlos.

La optimizacion depende de los parametros del algoritmo utilizado. DEbe elegir los parametros que quiero optimizar y para ello debo conocerlos desde el algoritmo que voy a emplear. WARNING : No exagerar con la cantidad de parametros.


-------------
Clasificacion
-------------

Importante: Los algoritmos con distancia euclideas hay que escalarlos.

A diferencia de la regresión donde se predice un valor continuo, se utiliza la clasificación para predecir una categoría. Existen una gran amalgama de aplicaciones del proceso de clasificación desde medicina hasta marketing. Los modelos de clasificación incluyen desde modelos lineales como la Regresión Logística, SVM, así como otros no lineales como K-NN, Kernel SVM y Bosques Aleatorios.

Clasificar y catalogar.

- Regresion Logistica

	Es buena cuando los datos pueden ser separados por medio de una recta.
	
	Busca el mejor separador lineal recto (la mejor recta) posible en el proceso de clasificacion.
	
	En el grafico las variables independientes van en el eje de las x y la dependiente en el eje de las Y.
	
	Se usa para clasificacion. Es un clasificador lineal. Devuelve probabilidades.
	
	Se  predice una accion. Por ejemplo, compra o no compra. Abre o no abre.
	
	Accion: Comprar o no comprar un producto.
			Abrir o no abrir un correo electronico. 
			
			Es decir llevar a cabo una accion.
			
			La logistica utiliza la regresion lineal, en 2D (Sueldo y Edad por ejemplo), el separador es una recta. En 3D el separador es un plano. En multiple dimensiones el separador lineal es un hiper plano.
			
			Ejemplo, 
				Compra en funcion de la edad
				Abre el correo en funcion de la edad
				
			En lugar dee predecir lo que vaa suceder podriamos predecir que tan probable es un cliente acepte la oferta.
			Probabilidad entre cero y uno. Todo lo que esta por debajo de cero o cero sera no compra, todo lo que sea uno o supere a uno sera compra. Lo intermedio sera un probabilidad de que compre o no. La idea es encontrar esas franjas de edades y cuantificar la probabilidad que compre o no. 
			
			No se puede usar la regresion lineal porque los datos no siguen esa funcion. Si en lugar se le aplica una funcion sigmoide (por ejemplo para predecir la venta dada la edad) para despejar el valor de la variable Y, obtendriamos:

				funcion lineal 
			     		y = b0 + b1 * x
				
				funcion sigmoide 
							p = 1 /(1 + e^-y) 
				
					al despejar la y obtendriamos:
						 p = 1 /(1 + e^-y) => y = ln(p/1-p) 
						
				Finalmente, 
				
					ln(p/1-p) = b0 + b1 * x
								
			En resumen, a una regresion lineal se le aplica una funcion sigmoide para trasnformar el valor final de la prediccion en una probabilidad.
			
				Regresion logistica o sigmoide
						ln(p/1-p) = b0 + b1 * x
							
							Es un tuneo  de una regresion lineal. 
							
							El eje y se trasnforma en una probabilidad. (en la regresion lineal es un prediccion)
							Probabilidad entre 0 y 1. 
							
							Se busca la funcion sigmoide (o logistica) que mejor se ajusta con respecto al conjunto de funciones sigmoides. Objetivo final, si hay probabilidad de comprar o no (Si se realiza el suceso o no)
							
							En el grafico la recta X es la variable independiente y la recta Y la probabilidad de compar o no. (Si lleva a cabo la accion o no)
							
							Se toma valores aleatorios para la variable independiente X y se analiza la probabilidad del suceso dado ese valor de X en funcion de la regresion logistica. Luego, en funcion de un "valor central" se decide si ocurre o no el suceso (si compra o no). Ese valor central generalmente es 0,5. Define si ocurre o no el suceso dada la probabilidad obtenida. El valor central se puede elegir arbitrariamente.
							
							Si la probabilidad obtenida del modelo es menor o igual que 0,5 no ocurre el suceso, de lo contrario si ocurre.
							
							En resumenn, el calculo de probabilidades es necesario como herramienta intermedia para realizar la clasificacion (compra o no).
				
				Matriz de confusion
				
						Una matriz de confusión es una herramienta que permite la visualización del desempeño de un algoritmo que se emplea en aprendizaje supervisado. Cada columna de la matriz representa el número de predicciones de cada clase, mientras que cada fila representa a las instancias en la clase real. Uno de los beneficios de las matrices de confusión es que facilitan ver si el sistema está confundiendo dos clases.
		
			En R:
			
				# Ajustar el modelo de regresión logística con el conjunto de entrenamiento.
				classifier = glm(formula = Purchased ~ .,
							data = training_set, 
								family = binomial) Es (family = binomial) binomial porque compra o no compra es lo que queremos predecir.
				
					CUIDADO - Cambios en la sintaxis de las librerías

					En la clase instalo la librería ElemStatLearn. Si te sale un error debido a cambios en la librería, debes hacer la instalación indicada en los siguiente pasos:


					https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/


					1. Descarga la librería desde este link o utilizando el enlace anterior.

					2. A continuación abre R-Studio. Selecciona el menú de herramientas y haz click en instalar paquetes.

					3. Desde 'Install from:' elige el la opción package install file(.zip; .tar,etc)

					4. Selecciona el paquete descargado anteriormente.

					También puedes instalarlo haciendo uso de la linea de comandos con la instrucción : R CMD install <pkg>.

- K-Nearest neighbors (K-NN)

	Es usado para categorizacion.
	
	No es un clasificador lineal. Los datos pueden ser separados sin la existencia de una recta o linea.
	
	Como lo hace?
	
		Cuatro pasos:
		
				Paso 1. 
						Elegir numero k de vecinos mas cercanos que tenemos en cuenta para el proceso de clasificacion.
						Elegir K en impar para no tener empate. K = 5 es el mas utilizado generalmente.
						
				Paso 2.
						Tomar los k vecinos mas cercanos del nuevo dato utilizando la distancia Euclidea. Es decir, se seleccionan los k vecinos mas cercanos utilizando la distancia euclidea.
						No es necesario utilizar la distancia euclidea, se puede usar otra (ej, manhattan, metrica del infinito, etc..) pero euclidea es la mas utilizada.
						
						La distancia euclidea entre dos puntos es basicamente el teorema de pitagora:
								Distancia entre P1 y P2  = SQRT((x2-x1)^2 + (y2-y1)^2)
						
				Paso 3.
						Entre los k vecinos contamos la cantidad de puntos que pertenecen a cada una de las categorias.
				
				Paso 4.				
						Asignar el nuevo punto de dato a la categoria que tenga mas vecinos en ella. 
						
				Paso 5.
						El modelo ya esta elavorado.
											
	En Python:
	
		from sklearn.neighbors import KNeighborsClassifier
		classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
		classifier.fit(X_train, y_train)
		
		n_neighbors: nro de vecinos
		
		metric: metrica para calcular la distancia de los puntos. (minkowski)
						(minkowski) Familia de distancias.
									
		p: (1) manhattan (2) euclideana
		

- SVN (support vector machine)

		Detecta los puntos mas cercanos entre las categorias. Esto se llama el margen maximo. Distancia maxima para equivocarse. Luego,  traza una linea x distante de esos puntos. La suma de las distancias entre  los puntos mas cercanos y la linea se busca maximizarla.  Se busca crear ese margen lo mas grande posible. Margen la distancia entre los puntos (o vectores) mas cercanos. Los puntos mas cercanos entre las categorias se llaman vectores. La linea separadora se llama Hiperplano. El Hiperplano busca maximizar el margen. Las rectas asociadas a los vectores se llaman hiperplano positivo e hipleplano negativo.
		
		Python.
		
		from sklearn.svm import SVC
		classifier = SVC(kernel = "linear", random_state = 0)
		classifier.fit(X_train, y_train)
		
		kernel: define la forma de separacion. Hay varios: (rbf: Gausiano, linear: lineal, etc.)
		
		Kernel lineal: linear. Basicamente es como una regresion logistica.	 Es lineal. Se genera una linea de separacion.
		Kernel no lineales: rbf
		
		Parametro degree: se usa en un kernel polinomico.
		Parametro gamma: se usa en nucleo gauseano, polinomico o sigmoide.
		
		Kernel SVN:
		
			Idea de Kernel:
			
				Que paso con nucleos no lineales?
					
				No siempre el limite de separacion entre las categorias es una recta.
				Hay datos que no son linealmente separables.
				
				La SVN tiene una hipotesis la cual es, que el limite de separacion (o la forma de separacion) debe ser detectada o definida por el humano. La separacion puede ser por esferas, circulos, elipses, etc. 
				
				Cuando no son lineales puedo representarlos en otras dimensiones para ver que sucede. Por ejemplo, en 3D puede ser que los puntos esten al nivel del suelo o los otros puntos al nivel del techo. Alli puedo encontar un plano o una linea que los separe. 
				
				Datos no separables en forma lineal.
				
				Hay que tranformarlos en un espacio de nivel superior y obtener un conjunto de datos separables en forma lineal. Luego, se retorna al espacio original para obtener la separacion correcta.
				
				El algoritmo toma datos en una dimension que no son separables linealmente, los transforma en otra dimension, mediante una funcion, tratando de separarlos linealmente, intentando hallar un hiperplano, en la nueva dimension. Encontrada la linea de separacion lineal (o hiperplano) se retorna a la dimension original mediante una proyeccion y se representa, en la dimension original, el separador que es no lineal.
				Usar esta mecanismo de transformación tiene un precio. Al subir a una dimension el calculo computacional puede ser mas costoso.(puede requerirse un equipo con muchos recursos o puede tardarse en clasificar y dar una respuesta)
				
				Truco del Kernel:
				
					Kernel RBF (Gausiano) : No lineal. Se púede usar sin necesidad de entrar en un espacio de nivel superior.
				
				Tipos de Kernel:
				
						- Gausiano (RBF) Muy util cuando los datos deben ser separados como circulos 
						- Sigmoide 
						- Polinomico
						Hay muchos mas...
				
				NOTA: Cuando no quiero que una variable domine las demas variables aplico la estandarizacion. Tambien, en algoritmos que trabajan con distancia euclidea.
				
- Naive Bayes

		- Teorema de Bayes

				P(A|B) = P(B|A) * P(A) / P(B)
				
				A: condicionado
				B: Suceso condicionante
				P: 
				
				Maquina 1: 30 herramientas/hora - Las herramientas son marcadas indicando la maquina que la genero. Maquina 1
				Maquina 2: 20 herramientas/hora - Las herramientas son marcadas indicando la maquina que la genero. Maquina 2
				
				Total de herramientas/hora: 50 herramientas (30+20).
						
				De todas las herramientas producidas se observa que el 1% es defectuoso.
				
				De todas las herramientas defectuosas se observa que:
					El 50% de herramientas defectosas proviene de la maquina 1.
					El 50% de herramientas defectosas proviene de la maquina 2.
					
				Pregunta:

				¿Cual es la probabilidad que una herramienta producida por la maquina 2 sea defectuosa?
				La respuesta es el teorema de bayer.
				
				Probabilidad = Casos posibles / Total de casos.
				
				Si elijo una herramienta cualquiera la probabilidad que sea de una u otra maquina es:
				
				P(Match1) = 30/50 = 0,60 - 60% - Probabilidad de que la herramienta sea producida por la maquina 1 del total de herramientas
				P(Match2) = 20/50 = 0,40 - 40% - Probabilidad de que la herramienta sea producida por la maquina 2 del total de herramientas
				
				P(Defectuosa) = 1%
				
				P(Match1| Defectuosa) = 50% - Sabiendo que es defectuosa cual es la probabilidad de que la herramienta sea de la maquina 1
				P(Match2| Defectuosa) = 50% - Sabiendo que es defectuosa cual es la probabilidad que que la herramienta sea de la maquina 2
				
				Las anteriores son probabilidades condicionadas.
				
				¿Cual es la probabilidad que una herramienta producida por la maquina 2 sea defectuosa?
				
				P(Defectuosa|Match2) = ?
				
				La respuesta es el teorema de bayer.
				
				Teorema de bayer.
				
						P(A|B) = P(B|A) * P(A) / P(B)
				 
						P(Defectuosa|Match2) = P(Match2|Defectuosa) * P(Defectuosa) / P(Match2)
						
						P(Defectuosa|Match2) =  0,50 * 0,01 / 0,40 = 0,0125 (1,25%) De cada 1000 herramientas producidas de la maquina = 12,5 son defectuosas.
				
				En general, la probabilidad condicionada por un suceso se gira el condicionante y el condicionado, multiplicamos por la probabilidad del condicionado y dividimos por la probabilidad del condicionante.
				
				Veamos otro ejemplo:
				
					Calculo frecuentista:
						Total de herramientas: 1000 herramientas 
						Herramientas maquina 2: 400 herramientas 
						1% tienen defecto: 10 herramientas defectuosas.
						Herramientas defectuosas de producidas por la maquina 2: 5 herramientas 
						porcentaje de herramientas defectuosas provenientes de la maquina 2: 5 * 1 / 400 =  1,25%
						
					Porque no realizamos el calculo frecuentista(contamos las herramientas)?
							- Podria ser que tome mucho tiempo contar las herramientas por cada maquina e identificar las defectuosas.
							- Puede ser que no se tenga acceso a los datos o la informacion.
							
							
					Ejercicio rapido:
					
						P(Defectuosa|Match1) = P(Match1|Defectuosa) * P(Defectuosa) / P(Match1)
											 = 0,50 * 0,01 / 0,60 =
	
		- Naive Bayes.
		
				- Porque Naive?
					Naive( se traduce ingenuo)
					Supone una independencia entre los casos que aparencen dentro de la probabilidades. Que las variables independientes sean independientes es decir, no exista correlacion entre ellas. A veces, puede que no exista esa independencia. Es decir, el teorema de Bayes requiere que la Edad y el sueldo sean independientes entre si.
					Se puede aplicar igual, y se obtiene resultados bastantes buenos. Por eso se llama ingenuo.
					
				El ejemplo se trabajo con dos variables pero puede haber + variables.
				
				- Variables independientas: Sueldo y Edad. Vector: (Sueldo, Edad).
				- Categorias: "Camina al trabajo" o "Conduce al trabajo"
				
				Plan de Ataque:
					
					Se aplica dos veces. 
					
					(1) La primera es para conocer cual es la probabilidad de que la persona camine:
					 
							X: Es el vector de caracteristicas (Sueldo, Edad). Ej: (U$S30000, 25). (El dato lo observo o lo consulto).
							
							P(camine/X) = P(X/camine) * P(camine) / P(X)
							
								P(camine): Se llama Probabilidad Previa o a Priori. Conocer cuanta gente va caminando al trabajo. Probabilidad de que una persona camine. 
								Se cuenta todas las personas y se cuenta cuantas caminan.
								P(camine) = Cant. individuos que caminan/Total de individuos.
														
								P(X): Se llama Probabilidad Marginal con respecto a esas caracteristicas. Indivudos que presentan esas caracteristicas del total de individuos. Que probabilidad tiene una persona en presentar esas caracteristicas.
								Como se cualcula? La probabilidad de que un individuo seleccionado al azar presente esas caracteristicas.
								Para calcularla se selecciona un radio de variabilidad, se dibuja un circulo con ese radio cerca del individuo (punto) a clasificar. El individuo o punto se usa como centro. Luego, se cuenta la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. Es decir, cuantos individuos tienen caracteristicas similares al individuo a clasificar.
								El radio es un parametro de entrada del algoritmo. El radio define la cantidad de individuos o puntos forman parte de la muestra a ser usada para contar la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. 
								Todos los individuos o puntos que estan dentro del circulo se consideran similar al individuo a clasificar. Por lo tanto, todos los puntos dentro del circulo son similares. 
								Entra el concepto de distancia: No es lo mismo la distancia euclidea o manhattan. 
														
								P(X/camine): Se llama Probabilidad condicionada por un suceso.
								Del total de personas que camina cuantas presenta esas caracteristicas.
								Para realizar el calculo se usa el mismo circulo definido en el calculo de P(X). La diferencia es que dentro del circulo se cuenta el total de individuos que caminan.	
														
								P(camine/X): Se llama probabilidad a posteriori.

					(2) La segunda es para conocer cual es la probabilidad de que la persona conduce:
					 
							X: Es el vector de caracteristicas (Sueldo, Edad). Ej: (U$S30000, 25). (El dato lo observo o lo consulto).
							
							P(conducen/X) = P(X/conducen) * P(conducen) / P(X)
							
								P(conducen): Se llama Probabilidad Previa o a Priori. Conocer cuanta gente conduce al trabajo. Probabilidad de que una persona conduzca. 
								Se cuenta todas las personas y se cuenta cuantas personas conducen.
								P(conducen) = Cant. individuos que conducen/Total de individuos.
														
								P(X): Se llama Probabilidad Marginal con respecto a esas caracteristicas. Indivudos que presentan esas caracteristicas del total de individuos. Que probabilidad tiene una persona en presentar esas caracteristicas.
								Como se cualcula? La probabilidad de que un individuo seleccionado al azar presente esas caracteristicas.
								Para calcularla se selecciona un radio de variabilidad, se dibuja un circulo con ese radio cerca del individuo (punto) a clasificar. El individuo o punto se usa como centro. Luego, se cuenta la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. Es decir, cuantos individuos tienen caracteristicas similares al individuo a clasificar.
								El radio es un parametro de entrada del algoritmo. El radio define la cantidad de individuos o puntos forman parte de la muestra a ser usada para contar la cantidad de individuos (puntos) que en ese circulo se parecen al individuo a clasificar. 
								Todos los individuos o puntos que estan dentro del circulo se consideran similar al individuo a clasificar. Por lo tanto, todos los puntos dentro del circulo son similares. 
								Entra el concepto de distancia: No es lo mismo la distancia euclidea o manhattan. 
																				
								P(X/conducen): Se llama Probabilidad condicionada por un suceso.
								Del total de personas que conducen cuantas presenta esas caracteristicas.
								Para realizar el calculo se usa el mismo circulo definido en el calculo de P(X). La diferencia es que dentro del circulo se cuenta el total de individuos que conducen.	
								
								P(conducen/X): Se llama probabilidad a posteriori.
					
				   (3) - Paso 3. Compara las probabilidades. La mayor gana.
				   
							Naives Bayes compara las dos probabilidades y obtiene la mas probable.

							Compara P(camine/X) vs P(conducen/X) y obtiene la de mayor probabilidad.
							
					
					Ejemplo:
							
							Total de personas: 30
							Personas que conducen: 20 
							Personas que caminan: 10
							
							P(Camine): 10/30
							P(conducen): 20/30
							
							P(X): Como se cualcula? 
							Se selecciona un radio y luego se define un circulo tomando como centro el individuo a clasificar (sus caracteristicas.Ej: Edad, sueldo). Finalmente, se cuenta la cantidad de puntos o individuos dentro del circulo. En nuesto ejemplo son 4.
							
							Personas dentro del circulo: 4 (Individuos similares al individuo a clasificar)
							Personas dentro del circulo que caminan: 3
							Personas dentro del circulo que conducen: 1
							
							P(X): Cant. individuos dentro del circulo/Total de individuos.
							P(X): 4/30
							
							P(X/camine): Cant. individuos dentro del circulo que caminan/Total de individuos que caminan.
							P(X/camine): 3/10
							
							P(X/conducen): Cant. individuos dentro del circulo que conducen/Total de individuos que conducen.
							P(X/conducen): 1/20
												
							P(camine/X) = P(X/camine) * P(Camine) / P(X)
							P(camine/X) = (3/10 * 10/30) / (4/30) = 0,75
							
							P(conducen/X) = P(X/conducen) * P(conducen) / P(X)
							P(conducen/X) =	(1/20 * 20/30) / (4/30) = 0,25
							
							P(camine/X) vs  P(conducen/X) = 0,75 vs 0,25 => 0,75 > 0,25 => El individuo a catalogar tiene una mayor probabilidad de caminar. Finalmente, se categoriza como Camina. Es una probabilidad.
								Tiene el punto un 75% de que Camina
								Tiene el punto un 25% de que Conduce
							
					NOTA: Como P(X) es usado en ambas formulas, podemos eliminarlo en ambas formulas y el resultado es el mismo.
				
						P(camine/X) = P(X/camine) * P(Camine) 
						P(camine/X) = (3/10 * 10/30) = 0,1
						
						P(conducen/X) = P(X/conducen) * P(conducen) 
						P(conducen/X) =	(1/20 * 20/30) = 0,033
						
						P(camine/X) vs  P(conducen/X) = 0,1 > 0,033
						El punto a clasificar: Camina. No es una probabilidad. Solo me permite comparar cual es el valor mayor. Es una modificacion ligera. NOTA: No sirve para saber la probabilidad. La mas segura es realizar el calculo completo (usando el denominador P(X)).
					
					NOTA: Que pasa con mas de dos caracteriisticas independientes?
						
						Para el calculo de probabilidades se aplica Bayes a cada caracteristica independiente. 
						
						Recuerda que las probaabilidades no puede superar el 1. Con lo cual si tenemos una caracteristica con una probabilidad mayor al 0,5. El resto de las probabilidades sera menor y esa gana.
									
						Si son mas de tres caracteristicas independientes puedo calcular las probabilidades de dos caracteristicas y ver sus probabilidades viendo que si supera sus sumas el 0,5. La mayor de ellas es la que gana.

			Resumen: Se calculan las probabilidades condicionadas para cada una de las caracteristicas.
			
			NOTA: 
				En R hay que factorizar la variable dependiente si no es un factor. Hacerlo antes de aplicar el algoritmo de Naive Bayes.
				
				Un factor es una variable categórica con un número finito de valores o niveles (etiqueta). En R los factores se utilizan habitualmente para realizar clasificaciones de los datos, estableciendo su pertenencia a los grupos o categorías determinados por los niveles del factor.
	
	
- Arboles de desicion
		
		NOTA: Los arboles no utilizan el concepto de la distancia euclidea. No necesitan escalar.
		NOTA: Si el algotimo usa distancia euclidea, es necesario escalar. Si no, no es necesario.
				
		CART. (Classification and Regresion Trees)
			---> Arboles de Clasificacion. Basado en variables categoricas. Categorias ej, hombre o mujer, compra o no compra
			---> Arboles de Desicion. Basado en variables numericas. Predice un valor.
	
		Como sabe el algoritmo donde ir cortando los datos?
			Ejemplo:
				Rojo = compra
				Verde = no compra 
			Se hace de tal modo que se maximiza el numero de categorias (o puntos) dentro de cada una de las divisiones. Por ejmeplo, quremos que las categorias (o puntos) queden en un lado cuanto mas puntos rojos mejor y al otro lado cuanto mas puntos verdes.
			Concretamente, en la separacion se usa el concepto de entropia. La entropia mide el desorden de una serie de datos. Es un concepto matematico. 
			
		Despues de la separacion tendriamos nodos terminales u hojas con los puntos de separacion para la regresion o le asigna rojo o verde dependiendo el caso. 
		
		x1 y x2: variables independientes.
		
		Luego de la separacion:
		
				Si x2 > 60
						Si x1 < 50
								"Es Verde"
						else
								"Es Rojo"
				else 
					if x1 > 70
						si x2 < 20
							"Es Rojo"
						else
							"Es Verde" 
					else
						"Es Rojo"
						
				
		Un Arbol puede tener mas variables independientes.
		
		NOTA: La tecnica es muy vieja y no son muy utilizados. Son Simple. Se mejoran con otros algoritmos o tecnicas. Las nuevas tecnicas que los mejoran son varias por ejemplo, la tecnica de bosques aleatorios, Gradient Boosting, etc.
			  
		En python el objeto es DecisionTreeClassifier, sus parametros son:
		
				criterion: Criterio de division. Por defecto: "Gini". Tambien, esta la "entropy". Ambos son usados para la division. Generalmente se usa la entropia ("entropy"). Busca la homogenea en la separacion de los datos.
				Entropia 0 la rama es homogenea.


- Bosques aleatorios

		Aprendizaje en conjunto: Combinan la potencia de diferentes algoritmos de prediccion. Son varios algoritmos trabajandp en paralelo y luego juntan la informacion. Objetivo: Elevar el resultado.
	
		NOTA: En el ejemplo se escalo para que quede bonito el grafico. Los arboles no tienen necesidad de escalar. No usan distancia euclidea.

		NOTA: Puede ser que cuando agrego mas arboles al bosque la prediccion puede que no mejore por el overfitting.
		
		Pasos:
	
			Paso 1. Se elige un numero K (numero aleatorio). Representa un subconjunto de los datos del conjunto de datos de entrenamiento. En lugar de dividir los datos en conjunto de entrenamiento y conjunto de test. Elegimos un subconjunto de datos de los datos de entrenamiento.
			
			Paso 2. Construir un arbol de desicion para los K puntos de datos. Lo que hace que el arbol tenga una vision parcial de los datos.
			
			Paso 3. Se define el numero de arboles de desicion y se repite el paso 1 y 2. Cada nuevo arbol tiene un subconjunto de datos del total de datos de entrenamiento.
			
			Paso 4. Cuando llegamos a construir el numero de arboles deseados. Se puede usar todo el bosque para hacer la prediccion. A cada arbol se le indica que realice la prediccion de la categoria a la que pertenece el nuevo punto. Finalmente, la prediccion final es asignar al nuevo punto la categoria con mas votos. 
			
			Este algoritmo mejor la prediccion de un solo arbol de desicion. 
				
			Python:
				RandomForestRegressor(n_estimators = 10, random_state = 0)	
					Parametros de ForestRegressor
							n_estimators -> cantidad de arboles del bosque. Defecto 10 arboles.
							criterion -> criterio para dividir una rama en dos subramas o nodos hojas.
										Criterio de division: MSE (error cuadrado medio) diferencia entre la prediccion y el valor real al cuadrado. Se intenta minimizar. Se divide la rama en dos subramas intentando que el MSE de cada rama sea menor al MSE de la rama que se divide. Tambien, hay otros criterios de division como la entropia (Se minimiza la entropia). 
							max_features -> caracteristicas tenidas en cuenta a la hora de generar las divisiones.
							random_state -> reproductivilidad para obtener el mismo bosque.
					  

- Evaluar la eficacia de los modelos de clasificacion.

	- Falsos positivos y falsos negativos.
		
			Falsos positivos o errores de tipo 1. Ejemplo, Usuarios que no compraban, la observacion real del dato lo muestra, pero mi modelo indica que compran.
			
			Falsos negativos o errores de tipo 2. Ejemplo, Usuarios que compraban, la observacion real del dato lo muestra, pero mi modelo indica que no compran.
			
			De estos dos hay uno peor que otro. Falsos positivos o errores de tipo 1 son cuidado. Mientras, Falsos negativos o errores de tipo 2 son PELIGRO DE MUERTE (Son malos) ES EL PEOR DE AMBOS. En ciertas cuestiones medicas o biologicas puede provocar daños a las personas. Ej, No hay un tsunami (pero lo hubo).
			
			
	- Matriz de confusion: 
		
			Ejemplo Matriz de confunsion
									predichos
								
								0				1
						0		35				5	-> (falso positivo)
			observados
			(real)		1		10				50
								 |
								 V
							   (falso negativo)
	
	       calcular dos prporciones:
		   
				Ratio de presicion (RP) = Correctos / Total =  85/100 = 85 %
				Ratio de error (ER) = Incorrecto / Total = 15 / 100 = 15 %
				
				
	- Paradoja de presicion:
	
			Ejemplo Matriz de confunsion
									predichos
								
								0				1
						0		9700			150	-> (falso positivo)
			observados
			(real)		1		50				100
								 |
								 V
							   (falso negativo)		
							   
			Ratio de presicion (RP) = Correctos / Total =  9800/10000 = 98 %
			Ratio de error (ER) = Incorrecto / Total = 200 / 10000 = 2 %
			
			Segun la matriz de confusion la gran mayoria de datos son marcados como Cero (9700). Hay muchos datos marcado como Cero. Dejamos el modelo y marco el dato como Cero. Muevo los datos que eran predichos como 1 a 0. La razon es porque es mas probable un 0 que un 1:
			
											predichos
										
										0				1
								0		9850			0	-> (falso positivo)
					observados
					(real)		1		150				0
										 |
										 V
									   (falso negativo)		
									
					Ratio de presicion (RP) = Correctos / Total =  9850/10000 = 98,5 %
					Ratio de error (ER) = Incorrecto / Total = 200 / 10000 = 2 %
			
			Se aumenta la presicion sin haber usado el modelo. Esto es la paradoja de la presicion. Cuando tenes un suceso raro o muy raro que ocurra, lo mejor es no usar modelos. Por ejemplo, si un 0,1 % de la poblacion tiene una enfermedad, lo mas normal es que nadie la tenga. Esto puede mostrar que en sucesos raros los modelos a veces son inutiles.
		
		
	- Curva CAP (Perfil de Presicion Acumulado):
			
			Como se sabe que un Perfil acumulado es mejor que otro? El area que queda entre la curva de color rojo y la recta azul. Cuanto mayor es esa area, mejor resultado se obtiene. Lave la pena de representar cuan se lleven a cavo algoritmos de clasificacion para ver que tambien el algoritmo funciona. Generalmente, se traduce a porcentajes: Del 100% de personas (o datos de nuestro dataset) suceptibles a ser contactadas, a que porcentaje tengo que atacar para conseguir el 50% o 60 % de las ventas. Si empleamos varios algoritmos de clasificacion y generamos el Perfil de Presicion Acumulado podemos compararlos. 
			
			Perfil de Presicion Acumulado es adecuado para comparar algoritmos de Clasificacion. Al compara los algoritmos con "(Perfil de Presicion Acumulado)" podemos saber que tanto mas ganamos de un modelo a otro o con respecto al modelo aleatorio (tomar individuos al azar). 
			Existe una curva llamada "Bola de cristal(o Modelo Perfecto)" que es el mejor modelo. Con el se puede contactar el 10% de los clientes y que los mismos realicen el 100% de las compras. Se sabe a quien contactar y que realizara todas las compras. No ocurre casi nunca pero es adonde intentamos ir.
			
			
	- Analisis de las curvas CAP		
	
			La linea del Modelo Buen es obtenida desde la salida del algotimo de clasificacion.
			
			Para evaluar modelos de clasificacion. Tenemos varias lineas que representan las salidas de nuestro modelos. Sin embargo, existen 3 lineas principales: La linea del modelo perfecto, la linea del buen modelo y la linea del modelo aleatorio. Lo que realizamos es cociente de areas, el limite al que podemos llegar.Como de cerca estamos de la linea perfecto o buen modelo (Bola de cristal). Para ello tomamos el area que va desde la linea del Modelo Perfecto hasta la linea del Modelo Aleatorio. Esta es el area del Modelo Perfecto (Ap). Esta es el area maximo que puede conseguir un modelo de clasificacion. Del mismo modo calculamos el area que va desde la linea del Modelo Bueno hasta la linea del Modelo aleatorio. La llamamos (Ar). Lo que queremos saber que porcentaje del area del Modelo Perfecto (AP) cubre el area del Modelo Bueno.
					
					AR = Ar/Ap . 
					
					Esto nos da un valor entre 0 y 1. Cuanto mas cerca de 1 mejor ajusta al Modelo Perfecto. Cuanto mas lejos de 1 sera peor. Si el valor es 0 es lo mismo realizar elecciones con el modelo o realizarlas aleatoriamente.
					
					El calculo de AR no se suele realizar a mano. Suele haber librerias que ayuden. La curva es dificil de calcular. Normalmente se realiza una serie de muestreos, se calcula con el modelo con el 20% de los datos, con el 40% de los datos, con el 60% de los datos. Se obtiene un poligono que ajusta el area de nuestro modelo y se calcula el area basado en este poligono. 
					
					Otra forma de ver que tan bueno es nuestro modelo es establecer una cota del total de contactados. Tipicamente la cota suele ser 50%. Luego se mira que porcentaje de Ventas se consigue en el modelo con el 50% de compradores. Busco el 50% de compradores y analizo cuanto me pronostica el modelo (X %). Se usa la siguiente regla para clasificar el modelo:
					
							90% < X < 100% 	Demasiado Bueno.
							80% < X < 90% 	Muy Bueno.
							70% < X < 80% 	Bueno.
							60% < X < 70% 	Malo.
							60% < X 		Muy malo.
					
							Entre 70% y el 80% es un modelo bastante acertado. No presenta problemas de overfitting
							Entre 80% y el 90% es muy bueno. No presenta problemas de overfitting. Si se presente es Muy bueno.
							EWntre 90% < X < 100% 	Demasiado Bueno. Puede presentar overfitting. Estudiarlo. Puede ser que en un 100% la variable dependiente forma parte de las independientes. Tambien, puede haber una relacion (o correlacion) entre las variables independientes.
							
							NOTA: Ir con  cuidado cuando los modelos superen el 90% de presicion. Es imposible conseguir el 100% de prediccion. Seguramente existe overfitting.
							
 - Conclusion - Clasificación


		¿Cuales son los pros y los contras de cada modelo?

		¿Cómo sé qué modelo debo elegir para resolver mi problema?

		¿Cómo puedo mejorar cada uno de estos modelos ?


		Resolvamos cada pregunta una por una:

		1. ¿Cuales son los pros y los contras de cada modelo?

		En esta clase encontrarás un documento que te dará todos los pros y contras de cada modelo de clasificación.


		2. ¿Cómo sé qué modelo debo elegir para resolver mi problema?

		Al igual que con los modelos de regresión, primero tendrás que averiguar si tu problema es o no es lineal.

		Una vez lo sepas, Si tu problema es lineal, deberás intentar crear un modelo de Regresión Logística o bien SVM.

		Si tu problema no es lineal, entonces tenemos varias técnicas donde elegir, como K-NN, Naïve Bayes, Árboles de Decisión o Random Forest.

		Desde un punto de vista empresarial, entonces deberías usar:

		Regresión Logística o Naïve Bayes cuando quieras ordenar tus predicciones por probabilidad. Por ejemplo, deberías usar estas técnicas si quieres crear un ranking de clientes desde el más probable al menos probable que compre un producto. Esto permite crear objetivos específicos para las campañas de marketing, por ejemplo. Y por supuesto, si tu problema de empresa es lineal, mejor utiliza la regresión logística, y si no lo es, intenta con Naïve Bayes.

		SVM  cuando quieras predecir a qué segmento pertenece un cliente. Los segmentos pueden ser cualquier conjunto de características que definan a los clientes, como los que identificaremos en la Parte 4 - Clustering.

		Los árboles de decisión cuando necesites tener una interpretación clara de los resultados modelizados.

		Y por último, Random Forest cuando busques un mejor resultado de predicción y te preocupe menos la interpretación de los modelos.


		3. ¿Cómo puedo mejorar cada uno de estos modelos ?

		Igual que en la parte 2, en la Parte 10 - Selección de Modelos, la segunda sección está dedicada a los Ajustes de Parámetros que permite mejorar la eficacia de nuestros modelos ajustando los valores de los parámetros. Como habrás comprobado, existen dos tipos de parámetros en nuestros modelos:

		los parámetros que el modelo aprende, como los coeficientes de la Regresión Lineal,

		los hiper parámetros del algoritmo.

		En este último caso, los hiper parámetros  son parámetros que el algoritmo no aprende, si no que son fijos y forman parte de las ecuaciones de los modelos. Por ejemplo, el parámetro lambda de regularización o el factor de penalización C son hiper parámetros. Hasta el momento hemos tomado los valores por defecto y no nos hemos preocupados de afinar su valor óptimo para mejorar la eficacia del modelo. Entontrar el valor óptimo es parte del Ajuste de Parámetros, así que si estás interesado en descubrir cómo hacerlo, te recomiendo ir directamente a la Parte 10 del curso donde veremos juntos cómo hacerlo.
						
					
-------------------------
Parte 4 - Clustering o Segmentacion
-------------------------

		Clustering es un proceso similar al de clasificación, pero con un fundamento diferente. En el Clustering no sabes qué categorías estás buscando, si no que intentas crear una segmentación de tus propios datos en grupos más o menos homogéneos. Cuando utilizamos algoritmos de clustering en el data set, surgen de entre los datos cosas inesperadas como estructuras, clusters o agrupaciones que un humano no se habría imaginado pero la máquina crea por nosotros.

		Algoritmos de Clustering para Machine Learning:

			Clustering con K-Means

			Clustering Jerárquico
			
- K-Means

		Se usaron en el ejemplo 2 variables pero se pueden tener mas variables (3, 4, etc.) para identificar los puntos.
		
		Tenemos un conjunto de puntos que queremos segmentarlos o clasificarlos. La diferencia con Clasificacion es que no conocemos las categorias a asignar a cada conjunto. Los datos se deben segmentar. No sabemos los grupos a segmentar. Los datos se agrupan por homogéneos. No hay categorias definidas conocidas previamente. Por ejemplo, compra o no compra.
		
		Como funciona:
		
			Paso 1. Elegir el numero k de cluster. (NOTA: Ver como calcular el numero K de cluster)
				
			Paso 2. Se seleccionan K puntos. Son los baricentros iniciales. (no necesariamente tienen que ser de nuestro dataset). Seran los hipoteticos centros geometricos de cada cluster. Los baricentros son lo puntos en el medio de cada cluster. Se seleccionan al azar. 

			Paso 3. Se asignan cada uno de los puntos del dataset al baricentro mas cercano. Esto forma K cluster.

			Paso 4. Se calcula el nuevo baricentro de cada cluster usando los puntos de cada cluster. Calculo la media geometrica de cada coordenadas (variables o caracteristicas de los datos. Ejemplo X1, x2, etc.) del cluster. Los baricentros originales quedan recalculado. Si en el calculo del nuevo baricentro uso distancia euclidea, los cluster quedaran redondos o elipse. En cambio si uso la distancia manhattan los cluster serian cuadrados. 
			NOTA: La distancia que elijamos es importante. La euclidea es la mas comun. Que distancia debo utilizar? Depende del problema a resolver. IMPORTANTE: Investigar sobre la forma de calcular las distancias y cuales son sus concecuencias en la forma del cluster.
			
			Paso 5. Se reasignan cada uno de los puntos del dataset al baricentro mas cercano. Si se realizan nuevas asignaciones ir al Paso 4 sino ir a Fin.
			
			FIN. Modelo listo!
			
			
		- La trampa de la incializacion aleatoria:
		
			Al inicio se eligen baricentros al azar para comenzar el algoritmo. Segun como se inicialicen esos baricentros el resultado puede ser bastante diferentes. Hay que tener cuidado con la eleccion de baricentros.
			
			¿Que pasa si elegimos mal los baricentros?
			
				Segun como inicialicemos nuestros baricentros al comienzo de algoritmo podemos sesgar el resultado final. Esto no es bueno porque una eleccion aleatoria puede llevar a una segmentacion incorrecta. Como corregimos esto? No es sencillo. Existe una modificacion del algoritmo de  K-Means que permite seleccionar bien los baricentros al incio del algoritmo. Se llama K-means ++ (plus plus). Es bastante dificil de entender pero se puede leer en otras fuentes. El algoritmo de K-means ++ esta implementado en python o R y no hace falta entender el fundamento del algoritmo K-means ++ para utilizarlo en esos lenguajes. NOTA. La seleccion de los baricentro iniciales puede sesgar la clasificacion. La respuesta:  K-means ++.
				
				
		- Como seleccionar el numero correcto de K cluster?
		
			K indica la cantidad de grupos que salen de la segmentacion.
			
			Dependiendo del numero K elegido cambia completamente el resultado del algoritmo.
		
			La eleccion depende de una metrica concreta. La metrica se llama la "Suma de los Cuadrados del Centro del Cluster" (WCSS). La formula es la suma de los cuadrados de las distancia de cada punto con respecto al cluster que pertenece (al centro geometrico del cluster). Luego, se suman todos los resultados de cada cluster.
			
			WCSS1 = Σ d(Pi, C1)^2
					Pi E Cluster 1
				
			WCSS2 = Σ d(Pi, C1)^2  +  Σ d(Pi, C2)^2  
					Pi E Cluster 1	 Pi E Cluster 2	 
		
			WCSS3 = Σ d(Pi, C1)^2  +  Σ d(Pi, C2)^2  + Σ d(Pi, C3)^2  
					Pi E Cluster 1	 Pi E Cluster 2	   Pi E Cluster 3
					
			Se busca minimizar el WCSS. A medida que se aumenta el numero de cluster las Sumas de los Cuadrados (WCSS) se reducen.
					
			¿Hasta donde se disminuyen? 
			 
			El decremento no sera constante. Al principio decrece rapidamente pero existira un momento en que al agregar mas cluster, los puntos estan tan cerca que no existe una reduccion o mejora considerable. Se traba de buscar el mejor equilibrio, ya que en un extremo puedo tener tantos cluster como puntos (esa es la menor distancia) pero no sirve para el modelo. Se busca al agregar un nuevo cluster una disminucion palpable de la metrica WCSS. Para ello, se usa la tecnica del codo, el cual es un metodo visual que se basa en decidir a partir de que punto, añadir un nuevo cluster a nuestro algoritmo de K-means no aporta una mejora. Es decir, disminuir la suma de los cuadrados de las distancias entro los puntos y el cluster al que pertenecen. 
			Se hace el grafico del codo y de forma visual detectar el cambio en la tendencia de disminucion del valor WCSS. 
			Nro. optimo de cluster = Tecnica del Codo. A veces es dificil detectar el numero K correcto en el grafico. No existe una formula matematica perfecta que permita sacar el valor correcto de K. 	La Tecnica del Codo es la mas pactada que funciona. Al final, es nuestra desicion como Analista elegir el K numero de cluster.
			
			Python:
			
			# Método del codo para averiguar el número óptimo de clusters
			from sklearn.cluster import KMeans
			wcss = []
			for i in range(1, 11): # Hace de 1 a 10
				kmeans = KMeans(n_clusters = i, init = "k-means++", max_iter = 300, n_init = 10, random_state = 0)
				kmeans.fit(X)
				wcss.append(kmeans.inertia_) #kmeans.inertia_: Trae la Suma de los Cuadrados de las distancias(WCSS).
				
			# Parametros:
			# n_clusters: Nro. de cluster
			# init: inicializacion de los baricentro. Como no queremos caer en la trampa usamos el k-means++.
			# max_iter: Puede ser que el algoritmo no finalice, moviendo el centro de un lado al otro. Se recomienda indicar el numero maximo de iteraciones. Indica que luego de n iteraciones finalice el algoritmo. Default 300. 
			# n_init: Inicializacion aleatoria. Defecto 10.
			
			plt.plot(range(1,11), wcss)
			plt.title("Método del codo")
			plt.xlabel("Número de Clusters")
			plt.ylabel("WCSS(k)")
			plt.show()
			
			En R:
			
				NOTA: El algoritmo estandariza los valoes de las variables. Cambia la escala.
				EL objeto "clusplot" de R es para usarlo en dos dimensiones (no mas de dos variables).
				
				
- Clustering Jerarquico
        
		No son apropiado para grandes datasets
		
		Como funciona?
		
			El enfoque y el proceso es diferente al de k-means. El resultado puede ser exactamente igual al de k-means.
			
			Existen dos tipo de agrupaciones jerarquicas: Aglomerativo y divisitivo. No es lo mismo un cluster jerarquico Aglomerativo que un cluster jerarquico Divisitivo. El aglomerativo es un enfoque de abajo hacia arriba. En el curso vemos el aglomerativo. Comenzamos desde los elementos (desde abajo) y construimos nuestro cluster jearquico. Comenzamos desde los elementos y vamos juntando objetos similares para crear conglomerados. 
			
			Cluster jerarquico Divisitivo: Partimos del total de elementos y comenzamos a partir por donde nos interesa para crear el cluster.
			
			Trabajamos con Cluster jerarquico Aglomerativo:
			
				- Como funciona el algoritmo?
				
						Paso 1: 
							Hacer que cada punto sea un cluster (Asi tenemos tantos cluster como puntos). (N Clusteres)
							
						Paso 2:
							Elegimos los dos puntos mas cercanos y los juntamos en un unico cluster. (N-1 Clusteres)
							Por que los puntos mas cercanos? Porque el algoritmo aglomerativo se basa en distancias. 
							
							OJO que dependiendo como calculo la distancia (euclidea, manhattan, etc) afecta al resultado del algoritmo. Ecuclidea es la mas usada.
							
							Se repite, es decir se van juntando los dos puntos mas cercanos en cluster hasta no poder armar mas clusteres.
							
							Los puntos mas alejados son mas diferentes.
							
							Pueden quedar cluster con un unico punto dado que ese/esos punto/puntos esta/estan mas alejado/alejados del resto de los puntos.
							
						Paso 3:
							Elegimos los dos clusteres mas cercanos y los juntamos en un unico cluster. (N-2 Clusteres)
							La clave es elegir simpre los clusteres mas cercanos. Como calculo la distancia entre dos clusteres? 
							
							Puede haber varias opciones para seleccionar los puntos de referencia que representan a cada cluster:
							
							(1) Distancia entre los puntos mas cercanos de cada cluster.
							(2) Distancia entre los puntos mas alejados de cada cluster.
							(3) Distancia media. Distancia entre todas las combinaciones de puntos entre un cluster y otro. Finalmente, se hace la media de todas esas distancias (de todos esos resultados).
							(4) Distancia entre sus baricentros. Se busca el centro geometico y se calcula la distancia entre esos centros.
					
							OJO que dependiendo como selecciono los puntos (usando cualquiera de las tecnicas anteriores) se afecta al resultado del algoritmo. Se elige dependiendo del problema a tratar.
							
							Luego de elegir la opcion para seleccionar los puntos de referencia, se calcula la distancia entre esos puntos de referencia usando euclidea, manhattan, etc. Defecto euclidea.
									
						Paso 4:
							Repetir el Paso 3. Es decir, ir juntando los clusteres mas cercanos hasta tener un unico cluster.
							
						FIN.
				
				
				- Como funcionan el Dendrograma?
				
						Dendrograma es una arbol.
										
						Para saber el numero de clusteres que van a salir necesitamos pasar por una herramienta visual llamada dendograma. 
						
						Paso 1 .
						
							Se representan cada punto en el eje X indicando (P1, P2, .., Px) P1. Punto 1, P2 Punto 2,... Pn Punto n. En el eje de las Y se indican valores que representan las futuras distancias.
							
							Este es el grafico de Dendrograma.
							
						Paso 2. Se incia el  Cluster jerarquico Aglomerativo:
						
								Paso 2.1: 
									Hacer que cada punto sea un cluster (Asi tenemos tantos cluster como puntos). (N Clusteres)
						
								Paso 2.2:
									Elegimos los dos puntos mas cercanos y los juntamos en un unico cluster. (N-1 Clusteres)
									Se calcula la distancia entre esos puntos (Euclidea, manhattan, etc. ). Euclidea es la que mas se usa. Se indica una raya horizontal que une esos dos puntos en el grafico Dendrograma, tomando como Y el valor de sus diferencias. Luego, se une la raya horizontal con el eje de las X (donde se representan los puntos). Esto se hace para cada punto cercano hasta no poder formar mas clusteres.
									
									La barra resultante es mayor a mayor distnacia entre los puntos.
									
								Paso 2.3:
									Elegimos los dos clusteres mas cercanos y los juntamos en un unico cluster.	Se calcula la distancia entre esos clusteres (usando alguno de las opciones de calculo). Se indica una raya horizontal que une esos dos clusteres en el grafico Dendrograma, tomando como Y el valor de sus diferencias.
									
								Paso 2.4:
									Repetir el Paso 2.3. Es decir, ir juntando los clusteres mas cercanos hasta tener un unico cluster. Se indica una raya horizontal que une todos los clusteres en el grafico Dendrograma, tomando como Y el valor de sus diferencias.
							
								FIN.	
							
						El Dendograma representa las uniones y jerarquias entre los clusteres a media que se van construyendo.
	
	
				- Como utilizar los Dendograma?
						
						Dendograma: Permite identificar el numero adecuado de clusteres.
						
						Se miran los niveles horizontales. Se define una distancia para cortar el dendograma horizontalmente. Ese umbral (Ej. una distancia de 1,7) me permite separar los clusteres. Luego, marcar la raya horizontal que representa el umbral en el dendograma. Como resultado, voy a tener clusteres separados. En el ejemplo, todo lo que esta por debajo del umbral se conforman en cluster que tiene una diferencia no superior a 1,7.
						
						NOTA: La cantidad de lineas verticales que corta la linea horizontal que representa el umbral es la cantidad de clusteres.
						
						Segun como seleccionemos el umbral tendremos diferentes numeros de clusteres. Cual es la forma optima de identificar el numero de clusteres? Hay una regla que se puede usar y es que la recta vertical mas larga hasta que cruza a una linea horizontal (las que representan las uniones entre las agrupaciones), es la que debemos cortar con la raya horizontal del umbral. Se anlizan las lineas horizontales que salen de una horizontal hasta otra horizontal (imaginariamente). La mas larga es la que se corta. Una linea vertical mas estrecha marca menor distancia entre los elementos. Sin embargo, a mayor linea vertical mayor distancia entre los elementos (agrupaciones o puntos). Se corta por cualquier punto de la recta vertical mas larga. NOTA: Cuando se corta con la linea horizontal (umbral) no se debe tocar las rectas horizontales que representan las uniones de los elementos (puntos o grupos). La cantidad de rectas verticales que se cortan con el recta horizontal (umbral) es la cantidad de cluster a usar.
						
						
						Python:
												
							# Utilizar el dendrograma para encontrar el número óptimo de clusters
							import scipy.cluster.hierarchy as sch
							dendrogram = sch.dendrogram(sch.linkage(X, method = "ward"))
							plt.title("Dendrograma")
							plt.xlabel("Clientes")
							plt.ylabel("Distancia Euclídea")
							plt.show()
									
							# sch.linkage: Representa el algoritmo Aglomerativo
							# Parametros
								# X: La lista de variables (matriz) a segregar.
								# method: metodo usado para encontrar los cluster o armar los clusteres. Ward: Intenta minimizar la varianza entre los puntos que exiten en los cluster. Se minimiza la varianza intracluster. En R hay varios tipos de "ward". Se usa "ward.D", ward.D2 es una mejora
							
							# Ajustar el clustetring jerárquico a nuestro conjunto de datos
							from sklearn.cluster import AgglomerativeClustering
							hc = AgglomerativeClustering(n_clusters = 5, affinity = "euclidean", linkage = "ward")
							y_hc = hc.fit_predict(X)
							
							# Parametros
							# n_clusters: nros. de clusteres.
							# affinity: distancia a utilizar.
							# linkage: metodo usado para encontrar los cluster o armar los clusteres. Ward: Intenta minimizar la varianza entre los puntos que exiten en los cluster. Se minimiza la varianza intracluster.
							#El metodo usado en linkage debe ser el mismo al metodo usado en el dendograma (parametro "method").
					
					NOTA: El grafico es para dos dimensiones. Si se trabaja con mas dimensiones. Comentar el codigo del dibujo, ya que es imposible realizarlo. Se puede usar ACP para la reduccion de dimensiones y reducirlas tal vez hasta conservar solo dos y poder representar el grafico.-


Conclusión de la Parte 4 - Clustering

Hemos aprendido dos modelos de clustering: K-Means y el Clustering Jerárquico.

Adjunta en esta clase encontrarás una hoja de ayuda con los pros y contras de los dos modelos de clustering.



------------------------------------
Parte 5 - Aprendizaje con Reglas de Asociación
------------------------------------

NOTA: Trata el problema de recomendaciones.
Busca asoiaciones entre transacciones u operaciones.

Premisa: 

	La gente que compre este producto, tambien compro este otro producto.
	La gente que hizo esto, tambien hizo esto otro.
				 
	Analiza el comportamiento emparejado de los usuarios.
	
	Lo compra o no lo compra. No hablamos de cantidades. (Presencia o Ausencia	)
	
	Sirve para realizar recomendaciones. Por ejemplo, si compro miel y manteca, se puede recomendar que compore gallettitas. Esta recomendacion se extrae de los datos del set de datos trabajado.
	
Dos modelos de reglas de asociación:

-(1) Apriori
-(2) Eclat


- Apriori

		Ejemplo: Sistema de recomendacion de peliculas(Netflix), Optimizacion de Cesta de Compra (Amazon)
		
		Como funciona?
				
				Se dividi en tres partes.
				
				(1) Soporte de la regla de asociacion:
				
						Recomendacion de Peliculas
						
								Sop(M) = |Usuarios que vieron M|/|usuarios|
								Usuarios que van a favor de lo que estamos evaluando/total de usuarios	
				
						Optimizacion de Cesta de Compra
								Sop(I) = |transacciones que contiene I|/|transacciones|
								Transacciones que van a favor de lo que estamos evaluando/total de transacciones
					
				(2)	Confianza
				
						Recomendacion de Peliculas
						
								Conf(M1=>M2) = |Usuarios que vieron M1 y M2|/|Usuarios que vieron M1|
								
				
						Optimizacion de Cesta de Compra
					
								Conf(I1=>I2) = |transacciones que contiene I1 y I2|/|transacciones que contiene I1|
						
						Que tan seguros estamos de que ocurren dos cosas a la vez. (Una especie de pprobabilidad condicionada) Ej. Que usuario consumio un item y otro al mismo tiempo?.  Que confianza hay cuando vemos una pelicula y luego ver otra?. Numero de  usuarios que ven una pelicula y luego otra, dividido el numero total de usuarios que ven la primera de ellas. Ej. Numero de personas que ven Interestelar y Luego ven ex-maquina, dividido el numero de personas que ven interestelar. Cantidad de usuarios que compran el item 1, luego compran el item 2, dividido la cantidad de uarios que compran el item 1.
										
				(3)	Lift
				
						Recomendacion de Peliculas
						   Lift(M1=>M2) = Conf(M1=>M2)/Sop(M2)
						
						Optimizacion de Cesta de Compra
							Lift(I1=>I2) = Conf(I1=>I2)/Sop(I2)
							
				Prob = Casos posibles / casos totales.
							
				Ejemplo General:
				
						- Total de poblacion de personas: 100.
						
						- Total de personas que vieron ex-maquina del total de la poblacion = 10.
						
						- Soporte de Ex-maquina: Nivel superior al 10 % (Elegido por caso de uso)
							Sop(ex-maquina) = 10 / 100 = 0,1 = 10%
							10 personas vieron ex-maquina del total de la poblacion.
							
					    - Total de personas que vieron Interestelar del total de la poblacion: 40
						
						- Total de personas que vieron Interestelar y Luego	ex-maquina: 7

						- Confianza(Interestelar=>ex-maquina) - Nivel superior al 10 % (Elegido por caso de uso)
							Conf(Interestelar=>ex-maquina) = 7 / 40 = 0,175 = 17,5%
							
						-  Lift(Interestelar=>ex-maquina) = 0,175 / 0,1  = 1,75 
						
							Si vieste Interestelar, recomendar ver ex-maquina es mejor que recomendar sin saber nada. 
				
		Algoritmo Apriori:

					Paso 1. Decidir el soporte y el nivel de confianza minimo.(Se decide de acuerdo al problema a tratar - Subjetivo)
							Para evitar que el algoritmo crezca y se tranforme en algo dificil de tratar, que no converja, se establece un soporte minimo. Por ejemplo, no queremos es tratar productos que tengan un soporte menor al 20% (por decir algo). Porque sera una perdida de tiempo construir algo para ese valor porque seguramente sera dificil que la gente vea esa recomendacion o es insignificante. Se debe jugar con esos umbrales.
							
							- Si busco valores muy altos de soporte y nivel de confianza, el sistema no recomendara nada.  
							
							- Si busco valores muy bajos de soporte y nivel de confianza, el sistema realizara muchas recomendaciones y lo probable es que el usuario no lo utilice. 

							Sera un tira y afloje.
					
					Paso 2. 
							Elijo todas las transacciones con soporte superior al minimo elegido. Filtramos por soporte. Si tenes 20 peliculas, calculamos el soporte de las 20 peliculas y elegimos aquellas que el soporte supere el minimo elegido. (No implica cruces).
							
					Paso 3. 
							Elijo todas las transacciones del subconjunto del Paso 2 con nivel de confianza superior al minimo elegido. Filtramos por nivel de confianza. Hacemos los cruces para aquellas peliculas cuyo soporte supere el nivel de soporte, luego calculamos la confianza de los cruces, finalmente, elegimos aquellas confianzas que superen el minimo elegido. (Implica curces).
					
					Paso 4.
							Se ordenan las reglas resultantes (los dos procesos de filtrados) por Lift descendiente (relevancia de la reglas). Se colocan todas las reglas con mayor impacto primero.
							
							
					Amazon, facebook, Netflix son ejemplo de aplicacion de reglas apriori.
					
	Aplicacion en Lenguaje R:
	
	Recurso adicional: cómo representar las reglas de asociación en un grafo 
	
	# ------------------------------------------------------------------------
	# GOAL: show how to create html widgets with transaction rules
	# ------------------------------------------------------------------------
	 
	# libraries --------------------------------------------------------------
	library(arules)
	library(arulesViz)
	 
	# data -------------------------------------------------------------------
	path <- "~/Downloads/P14-Part5-Association-Rule-Learning/Section 28 - Apriori/"
	trans <- read.transactions(
	file = paste0(path, "R/Market_Basket_Optimisation.csv"),
	sep = ",",
	rm.duplicates = TRUE
	)
	 
	# apriori algoirthm ------------------------------------------------------
	rules <- apriori(
	data = trans,
	parameter = list(support = 0.004, confidence = 0.2)
	)
	 
	# visualizations ---------------------------------------------------------
	plot(rules, method = "graph", engine = "htmlwidget")
	
	
	El problema con matrices con valores vacios es que será difícil llegar a entender el dataset en el sentido clásico, lo que necesitamos es establecerlo como una matriz dispersa, una matriz Sparse, que es una matriz dispersa en realidad, una matriz Sparse que contiene muchos ceros y por tanto, muchas veces, en lugar de reservar espacio en memoria para la celda, no le hace falta reservarlo, de modo que se guarda internamente. Al ser una matriz con tan pocos valores distintos de cero, se optimiza muchísimo el espacio de memoria y la forma de trabajar con ello. Internamente se guarda en formato de tres columnas: A qué fila primera? A qué columna segunda corresponde? y  Qué valor tercera? Es la forma interna en la que se guardan las matrices de todas las matrices dispersas.	
	
	Lo que se hace es tomar todo ese conjunto de datos. Supongamos que hay un total de  120 productos diferentes en este caso y lo que podemos hacer es crear una serie de columnas, cada una de las cuales sea un producto. Luego podemos tener filas correspondiendo a los clientes y finalmente, valores dentro de la matriz que sea ese usuario compra o compra ese artículo.
    Por ejemplo, de modo que tendremos en las columnas agua mineral, huevos, pollo, chocolate, patatas fritas, miel, brownies, lo que sea. Tendremos un total de 120 columnas, correspondiendo a todos y cada uno de los 120 productos que hay en este dataset. En cada una de las filas aparecerán los usuarios que llevan a cabo las compras.
	
	Productos en columna, clientes en fila es la forma más estándar de declarar una matrices, luego internamente se guarda,  primera columna Ídem de usuario, segunda columna Ídem de producto, tercero compra o no compra, que básicamente es presencia,
	ausencia del valor.
	
	En R se usa "arules" libreria de Association Rules o Reglas de asociación.
	
	libray(asrules)
	dataset = read.transactions("file_name_dataset" , sep="," , rm.duplicates=true ) Esto es una sparse matriz.
	summary(dataset) Ver el resultado
	
	dispersion: indica el % de valores cubiertos. la diferencia con el 100% son el valor cero.
	element length distribution: cantidad de compras que tenian 1 o 2 o ... n productos.
	
	crear el algoritmo apriori:
      rules = apriori(dataset, 
	           parameter = list (support=0.1, confidence=0,8))	NOTA: limite minimo de soporte y nivel de confianza. Su seleccion depende del objetivo buscado.
			   
			   support y confidence bajo mas de reglas. Si las reglas generadas no son suficientes podemos subirlos.
			   
			   Soporte: 
				   Para decidir el valor de soporte tenemos que preguntarnos cuántos de los elementos diferentes de nuestro dataset queremos que aparezcan como reglas significativas de aprendizaje. Se puede realizar un top de los 10 elementos mas fecuentes (con itemFrecquencyPlot(dataset, topN=10)). Sin emargo, para optimizar las ventas nesitamos definir las ventas de los items que se venden mas. Esto se puede ver en el grafico construido en el valor en la barra vertical llamada "item frequency (relative)". Alli se puede identificar el valor de soporte a usar (valor en la barra vertical llamada item frequency (relative)). Sus valores se define segun lo buscado por el negocio.  Con el soporte defino que items voy a trabajar o seran mi base de las futuras reglas.		   
				   En el ejemplo se definio:
						Items que se compran 3 veces al dia (los datos del dataset son ventas de una semana), 7500 son los registros totales del dataset.
						
						3 * 7  / 7500 = 0,003 (valor del soporte)
						
						El soporte se puede definir sobre las necesidades de la tienda.
												
			   Confianza:
			   	   La confianza puede comenzarse por el defecto (0,8). Confianza baja se optienen reglas que representan ventas muy raras. Depénde de los objetivos de la empresa. Es arbitraria. Un nivel de confianza demasiado pequeño, obtendremos algunas reglas que ocurren muy de vez en cuando y que no tendrían sentido. Por ejemplo, comprar chocolate. A la vez que compro el champú, he ido a por champú porque me he quedado sin champú y me ha pegado antojo de chocolate. Eso no es una regla de asociación, es una casualidad o una coincidencia. Ese es el tipo de reglas que se obtienen de forma esporádica, con poca confianza. Se puede jugar con el valor del nivel de confoanza.
				   
				   Confianza del 0,8 (80%) significa que las reglas que cree el algoritmo de apriori tiene que ser válida al menos en el 80% de las transacciones estudiadas. El 80% de las transacciones que se llevan a cabo es un número bastante elevado, significa que tiene que ser verdadera en cuatro de cada cinco cestas de la compra debe ocurrir la regla estudiada. Entonces, por eso es que las reglas de asociación con un nivel de confianza tan alto es muy restrictiva. Lo que sí se puede hacer es tomar el nivel de confianza que teníamos anteriormente (0,8) y empezar por dividir por dos, por tres, por cuatro. Ir bajando ese nivel de confianza si lo disminuyo a la mitad.
				   Lo que estoy investigando son reglas de asociación que ocurran en el 40% de las cestas de la compra donde se incluyen esos items.
			   
	Como resultado de ejecucion de la linea anterior, el algoritmo retorna la cantidad de reglas de asociacion. "writing ... [x rules(s)]. Donde x es el numero de reglas.
	
	NOTA: Cambiar el soporte o el nivelde confianza si las reglas no tienen sentido.
	
	Soporte: 0.003 (0,3%)
	Confianza: 0,8 (80%)
	
	RECORDEMOS 
		Se filtran los items de acuerdo al soporte establecido y luego, al nivel de confianza.
	
		El algoritmo filtrara sólo items con soporte superior al mínimo establecido que aparezcan al menos en un 0,3% de las cestas de la compra del supermercado. Y a continuación se extraen las reglas que aparezcan, al menos en el 40% de las cestas de la compra. El paso dos y el paso tres, en R se hacen automáticamente, solo se define el soporte y el nivel de confianza. Finalmente, se ordenan por su lift de mayor A MENOR.
		
		
	inspect(rules[1:10]) /las diez primeras reglas- extrae la fila 1 a la 10
	
	Retorna un conjunto de 10 reglas de recomendacion:
		inspect(sort(rules, by='lift')[1:10]))/Ordena descendiente por lift y extrae las 10 reglas de mayor lift
				lift - mayor peso.
				
		Ejemplo de salida:
				{mineral water, pasta} => {olive oil}
				
				Analisis: Si comro agua y pasta podemos recomendar aceite de oliva.
				          Tambien, que se coloquen en la misma gondola los tres productos.

		Las reglas resultantes se aplican y prueban, en caso de que alguna no sea adecuada se cambian por otras o se ajusta el soporte o nivel de confianza.				  
    
	Aplicacion en Lenguaje PYTHON:


	Las empresas han estado utilizando las reglas de asociación para saber dónde colocar los productos en la tienda que colocar al lado de cada uno de los ítems.

	Por poner un ejemplo muy simple si alguien compra unos cereales, es muy probable que esta persona también quiera comprar la leche para no tomarlo solos.
	
	Entonces el colocar los cereales cerca de la leche es una técnica muy útil para que el cliente que inicialmente había ido con intención de comprar cereales, compre también la leche, se lleve otro artículo y la empresa gane más dinero de forma más genérica.
	
	Supongamos que un cliente quiere comprar un producto y para eso va a la tienda. Si ese producto puede asociarse correctamente a un producto b, se puede hacer que una persona entra a comprar el producto A y de regalo, colocándolo al lado. También se lleve el producto B.
	
	En el ejemplo, cada observación es un cliente con su cesta de la compra y se lleva una serie de productos específicos.
	tengo 7500 cestas de la compra o 7500 usuarios.
	
	Soporte:
		se trata del número de transacciones o la proporción de transacciones que contienen un determinado ítem y dividido por el número total de transacciones. Es un argumento que nos indica con qué presencia mínima debe estar un item para ser considerado como el objetivo de una regla de asociación. Entonces, lo que me interesa es que aparezcan reglas con un soporte más elevado que un mínimo que yo establezca aquí, que realmente no me salgan reglas de asociación de un item que se compra solo una vez
		lo que me interesa es que aparezcan reglas con un soporte más elevado que un mínimo que yo
		establezca aquí, que realmente no me salgan reglas de asociación de un item que se compra solo una
		vez a la semana
		
		necesito que sean relevantes
		
		Entonces esto va a depender del objetivo de la empresa.

		Esto hay que hablarlo con el jefe de la empresa para decidir qué ítems son los que vamos a proceder
		a estudiar y cuáles son los que el cliente suele colocar dentro de su cesta de la compra, al menos
		un mínimo de veces.
		
		Los datos de python corresponden a ventas semanales.
		
		El soporte, pues quiero ítems que se compren un mínimo de tres veces al día, por ejemplo, 3% son
		21 veces a la semana.
		Por tanto, en este caso el soporte será el cociente entre 7X3 21 dividido sobre el número total de
		transacciones.
		Busco aquellos ítems que aparecen por lo menos siete por tres 21 dividido 7500 veces que tienen una
		relevancia superior por lo menos a ese mínimo.

		Es decir, que existan al menos tres transacciones al día por siete.
		21.
		Por qué tenemos el dataset con una semana de ventas del centro comercial?
		Entonces busco la presencia de ítems que salgan al menos en 21 cestas de la compra sobre el total del
		data set
		Y eso evidentemente es un porcentaje, el soporte, que era un porcentaje y que en nuestro caso lo podemos
		calcular directamente.
		Simplemente hay que dividir la frecuencia total de los ítems con respecto al total del dataset.
		De acuerdo, lo voy a hacer aquí en la zona de la consola.
		Simplemente tres por siete dividido 7500 es la frecuencia que estoy intentando observar.
		
		Esto nos daría un soporte mínimo de 0,0028. redondeado a 0,003
		 es una proporción.

	nivel de confianza
	
				Diferentes niveles de confianza nos darán cestas diferentes si somos muy poco permisivos, si queremos
		que la confianza sea muy elevada de los elementos con soporte mínimo, sólo me voy a quedar con los
		que realmente muchísimas, muchísimas veces, aparezcan juntos en la cesta de la compra.
		Lo normal es empezar con un nivel de confianza alto que ocurra en el 80% de las cestas de la compra
		y luego ir bajando para ver cómo van apareciendo reglas de asociación y cuáles de ellas son buenas o
		cuáles no, evidentemente se pondría en producción.
		Miraríamos, hay un impacto en los ingresos y luego podríamos volver, cambiar el soporte y o el nivel
		de confianza, volver a experimentar con nuevas reglas de asociación más fuertes y así llegar a la forma
		de optimizar, en definitiva, nuestra tienda.
		
		En R fue empezar con un valor alto, con un nivel de confianza de 0,8.
		
		Nos dimos cuenta de que no había reglas de asociación con el nivel de confianza, ya que era demasiado
		elevado.
		Lo bajamos a un nivel inferior, de hecho lo baje a 0,4 y al bajar simplemente lo que permití es que
		en lugar de buscar reglas que aparecieran el 80% de las cestas de la compra que podéis imaginar, 80%
		de las cestas de la compra son muchas.
		Entonces, al bajar el nivel de confianza, no necesito que esta información aparezca o esta asociación
		aparezca en tantas cestas de la compra.
		De modo que la forma de razonar es esa de que sabiendo que los ítems se compran un mínimo de 21 veces
		por semana, lo que quiero es qué porcentaje de cesta de la compra aparecen esos ítems juntos.
		
		Se busca asociaciones lógicas entre esos productos, no casualidades, 
		no cosas irrelevantes.
		Entonces para esto sí que hay que intentar jugar con el nivel de confianza.

		Si estableces una confianza muy alta, pues no obtendréis ninguna regla de asociación.
		Y si sois demasiado permisivos, evidentemente encontraréis correlaciones entre los productos de la
		cesta de la compra, un poco inútiles.
		
		Usamos el valor de R un nivel de confianza de 0,2.
		
		Lift:
		
		lift, es el cociente entre soporte y nivel de confianza
		
		las reglas de asociación lo único que nos indican es el cociente, nos hacen un ranking.
		De qué tan buena es esa asociación de productos

		La idea es establecer un mínimo para que solo se me imprima las reglas más relevantes en lugar de un
		monto reglas de las cuales la mitad son inútiles.
		
		esto dependerá del problema tratado.
		
		Se busca por Lift decendente.
		
		El Lift era la medida de la fuerza de una regla de asociación.
		
		Salida de Python
		
					RelationRecord(items=frozenset({'chicken', 'light cream'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)])
					
					Analisis:
							support=0.004532728969470737
									0,45 % de las compras incluyeron el pollo.
							confidence=0.29059829059829057
									29% de las compras incluyeron el pollo y la crema ligera.
							lift=4.84395061728395
									Lift proporcion entre soporte y confianza es muy alta.
									
					RelationRecord(items=frozenset({'escalope', 'mushroom cream sauce'}), support=0.005732568990801226, ordered_statistics=[OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)])
							support=0.005732568990801226
								el 0,57 de las compras incluyen el escalope
							confidence=0.3006993006993007
								30% de las compras incluyen escalope y mushroom cream sauce
					
					RelationRecord(items=frozenset({'escalope', 'pasta'}), support=0.005865884548726837, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)])
					
					RelationRecord(items=frozenset({'fromage blanc', 'honey'}), support=0.003332888948140248, ordered_statistics=[OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)])
		

- Eclat
		
  Es una version simplificada de las reglas apriori.
  
  Las reglas de asociación resolvía el problema de la gente que compró este articulo tambien, compro este otro.
  
  De modo que a base de mirar cómo se comportan los usuarios, buscamos reglas generales que nos indiquen  que la gente que ha visto la película A, también ve la película B, o que en general toda la gente que ve la película A, le interesa ver también la película B. Se establecen relaciones fuertes de que el haber hecho algo, el haber visto en esta película, implique también ver esta otra película. Como ejemplo.
  
  Cada una de estas reglas puede tener diferente probabilidad, diferente fuerza.
  Nos interesa buscar las reglas más fuertes.
  
  En el algoritmo analizaremos qué tan probable es que un conjunto venga a la par, que dos películas se vean a la vez o que tres peliculas se vean a la vez o que un paquete de peliculas vayan a la vez
  
  No hace falta hablar del lift o confianza como en apriori. Simplemente vamos a utilizar el soporte de un conjunto. Vamos hablar de conjunto de cosas. 
  
  A las relaciones que salen del algoritmo hay que analizarlas para saber si son importantes. En Apriori usabamos el lift descendente.En Eclat hay que establecer una medida de qué tan imporante es que ocurra una cosa junto a la otra.La importancia de esa regla. Esas combinaciones son las que ahora tenemos que investigar. Eclat se basa en el soporte.
  
  Vamos a medir el soporte, mo me hará falta ni la confianza ni el lift. Medimos el soporte del conjunto y no de un item como en Apriori.
  
  Se define la importancia con el nivel de soporte de un conjunto. El modelo de eclat no tiene sentido ser evaluado para item.Miramos conjunto de items. No nos hace falta la confianza ni el lift.
  
  En eclat vemos cuál es la frecuencia de que los ítems vengan empaquetados, que vengan juntos. Eso es lo que va a calcular el soporte del conjunto.
  
  Ejemplo, qué tan probable es o qué tan juntas suelen ir la película interestelar con la película El naufrago. Qué porcentaje de los usuarios ven esas dos películas a la vez o que les ha gustado las dos peliculas a la vez, que sean un pack. Es decir, si hipotéticamente tenemos 100 usuarios en nuestro dataset, cuántos de esos usuarios han visto las dos películas, que se hayan interesado a la vez por Interestelar o por El naufrago. Empaquetamos las cosas. Si, por ejemplo, tuviéramos un 80% de los usuarios de la base de datos han visto esas dos películas, tenemos una muy alta probabilidad de que esas dos películas vayan en pares (juntas).Por tanto, si uno ha visto una, automáticamente podemos recomendar la otra.
  
  Si lo compran juntos, si es probable que aparezcan juntos.  Los que han comprado una cosa podemos recomendarle la otra.
  
  Apriori es mas potente. Eclat es mas simple y rapido (solo tiene el soporte)
  
  Algoritmo.
  
		Paso 1. Se selecciona un soporte minimo a utilizar.
		Paso 2. Tomar todas las operaciones con soporte superior al minimo.
		Paso 3. Ordenar por soporte descendente. Elegir las mas relevantes. Por ejemplo Top 10.
		
	En el lenguaje R:
	
		Tiene un solo parametro (el soporte). Las reglas que me van a salir en esta clase no van a ser las mismas que las que he obtenido con el modelo apriori.
		
		La diferencia está que ahora voy a tener la posibilidad de hablar de conjuntos de productos comprados en pack, no que un producto implique la compra de otro, sino que productos que aparecen en pack será como una especie de nube de productos que aparezcan juntos.
		
		rules = eclat(data = dataset, 
                parameter = list(support = 0.003, minlen = 2))
				
				minlen: indica que minimamente cada transaccion tenga dos elementos. Evita los singleton. Que no sean transacciones de un item. Por lo menos dos. 
				
		Lo que tengo no son reglas de asociación, sino que son subconjuntos de elementos que tienden a aparecer juntos. 
  
  
		Así aparecerán los diez soportes más altos, los diez subconjuntos de elementos que más seguro estamos de que aparecen en conjunto. No hay A implica B como en apriori. Aqui tenes subconjuntos de artículos comprados con más frecuencia.
		
		Por ejemplo, el que más aparece conjuntamente es el agua mineral con los espaguetis para hervir. Tiene un soporte cercano al 6% (soporte 0,059). Significa que aparece por lo menos en seis, en el 6% del total de celdas de cestas de la compra y representa 448 cestas de la compra con respecto al total.



------------------------------------
Parte 6 - Reinforcement Learning (o aprendizaje por refuerzo)
------------------------------------

Reinforcement Learning (o aprendizaje por refuerzo) es una rama del Machine Learning, llamada también Online Learning. Se utiliza para resolver problemas de interacción, donde los datos observados hasta el momento t son considerados para decidir qué acción se lleva a cabo en el momento t + 1.

En Inteligencia Artificial cuando se entrenan máquinas capaces de hacer tareas como caminar, o conducción de coches inteligentes.

Un resultado esperado proporciona una recompensa a la IA, mientras que uno no deseado le otorga un castigo. En este caso las Máquinas aprenden a través de ensayo y error.

Algoritmos de Reinforcement Learning (aprendizaje por refuerzo):

- Upper Confidence Bound (UCB)

- Muestreo Thompson


- El problema del bandido multibrazo:
	
		Hay diferentes formas.
		
		Este problema no es el único que se puede resolver con aprendizaje por refuerzo.
		
		Es un ejemplo generico.
		
		Los algoritmos de aprendizaje por refuerzo permiten por ejemplo que un perro robot sea capaz de caminar y no chocar con una pared, de subir un escalón o cosas de este estilo. En el caso del robot es súper fácil de entender, porque para enseñarle a caminar hay que enseñarles que primero hay que poner el pie derecho y luego el pie izquierdo de delante, luego el pie derecho de atrás, izquierdo de atrás y en base a ir combinando esos movimientos uno tras otro, es cómo se va enseñando y educando a un robot en forma de perro, a una tarea concreta que es el caminar. Básicamente le voy dando las acciones que puede tomar mover pierna derecha, izquierda, delante, detrás y le enseño cuando mover una pierna es algo positivo y cuando no, cuando da un paso adelante y no se cae, le doy una recompensa. Cada vez que se cae le doy un castigo.
		
		Normalmente la recompensa es el número +1 y el castigo suele ser cero o -1.
		De modo que lo que quiere el perro es maximizar las recompensas. Lo que hace es recordar qué acciones han sido buenas y las intenta repetir cada vez.
		
		Y así es como se enseña a caminar a un perro. De modo que no hay que programar. 
		
		Esto se mete dentro de una rama diferente, ya no es machine learning, ya es inteligencia artificial.
		
		- El bandido multibrazo. Podemos pensar en un ladrón que va al banco y que tiene toda una banda organizada, múltiples brazos operativos para poder atracar correctamente el banco. Esto podemos simplificarlo bastante, porque básicamente un bandido multi brazo no es ni más ni menos que una tragamonedas.
	
		El problema del bandido multi brazo es una especie de juego psicológico. Cuando entras en una sala de casino en lugar de tener una tragamonedas, tenéis muchas tragamonedas. La idea es que cada una de estas máquinas tragamonedas tendrá una programación diferente y llevará más o menos tiempo jugando, más o menos tiempo dando vueltas la barra de los premios. Se llama multi brazo porque es un problema que sirve para resolver o elegir la mejor tragamoneda que maximiza la victoria del jugador.
		
		Esto tendrá una aplicación directa al mundo de los problemas de una empresa cuando haya elecciones múltiples que queremos elegir una de ellas para maximizar.
		
		El problema es descubrir cuál es la distribución de premios de cada una de estas máquinas. Qué distribución tienen por detrás, una distribución de números, de beneficios, de premios a partir del uso de las mismas y así poder elegir cuál de todas estas máquinas tiene un resultado mejor.
		
		Cada una de ellas tendrá una distribución diferente. De antemano no sé cuál es la distribución de estas máquinas, se complica un poco. No puedo ir jugando en cada una de ellas porque terminare arruinado para descubrir su distribucion de premios. El objetivo es determinar cuál es la mejor distribución que podemos elegir entre las tragamonedas. Cada una de ellas tiene una distribucion diferente. La idea es elegir la mejor maquina que tenga la mejor distribucion. Como determinar la mejor distribucion.
		
		Cada maquina tiene diferentes distribuciones de probabilidad de pago de premio. Mas a la derecha mayor el premio (cola mas larga a la izquierda). Mas alto el pico mayor la probabilidad de pago. La D5 es la mejor conociendo esas distribuciones. Se busca obtener ese resultado con el algoritmo de Reinforcement Learning. Se buscan los limites de confianza hasta donde estamos dispuestos a llegar. Si se supera ese limite se incurrira en una frustracion. El objetivo es encontrar la mejor distribucion psando el menor tiempo posible 	jugando. Mayor tiempo mayor perdida de dinero.
		
		Otro ejemplo, imaginemos una campaña de coca-cola con varios anuncios. Cada uno de los cuales generan datos en base a una campaña de marketing. Son 5 anuncion cada uno con sus correspondientes campañas de marketing. El objetivo es buscar que anuncio funciona mejor para maximizar las ventas. No tenemos una distribucion al inicio de la campaña. Las distribuciones de cada campaña las conozco despues de que miles de personas miren los anuncios en sus correspondientes campañas. Que se hace, basicamente se toman los 5 anuncios y se hace un test AB con multiples pruebas A y B hasta tener una muestra grande y concluir con cierto nivel de confianza que anuncio otorga mejor resultado. El problema es que se suele gastar mucho dinero y tiempo. Las Test AB no es la mejor opcion que se puede aplicar practicamente. Si podemos conocer las distribuciones de cada anuncio en lugar de un test AB podemos obtener mejores resultados en el menor tiempo posible. Mejor de un test AB aplicar las tecnicas de Reinforcement Learning

---------------------------------
- Upper Confidence Bound (UCB)
---------------------------------

		Limite de confianza superior (UCB) 
		
		Intenta solucionar el problema del bandido multi brazo con múltiples brazos, que básicamente se resumía en que teníamos cinco máquinas tragamonedas y teniamos que averiguar como maximizar las devoluciones de las máquinas en base a intentar descubrir las distribuciones. Hay una distribución para cada una de ellas y la desconocemos. No sabemos muy bien cuál de todas es la óptima. Por tanto, necesito combinar la exploración de estas máquinas haciendo apuestas para intentar extraer información.
		
		La aplicación moderna de este problema es entrar dentro del mundo de la publicidad, de modo que si tenéis cinco o diez, 15, 500 anuncios diferentes, hay que averiguar cuál de ellos es el mejor. Se intenta averiguar cuales son las distribuciones. Quiero combinar la exploración de los datos con las técnicas de reinforcement learning para obtener el resultado más óptimo e intentar maximizar todo el dinero de la empresa para que no se arruine.
		
		Algoritmo:
		
			Cómo se puede afrontar este problema?
			
			Tenemos d brazos (o anuncios). Por ejemplo, los brazos pueden ser los anuncios que mostramos a los usuarios cuando se conectan a una página web o podrían ser cada una de las máquinas tragamonedas.
			
			Cada vez que un usuario se conecta debo elegir qué anuncio tengo que mostrar cada vez o a qué máquina tragamonedas tengo que jugar cada vez.
			
			Para ello, En cada ronda N se elige uno de los anuncios a ser mostrado o una de las tragamonedas a ser jugado.
			
			En cada ronda que juegue o cada anuncio que muestre, voy a otorgar una recompensa, el anuncio y dará una recompensa en forma de número cero o uno.
			
			Si el usuario hace clic en el anuncio o si el usuario tiene un beneficio en la máquina tragamonedas pues la recompensa para la  ronda N será uno y cero cuando no. Por tanto, cuantificado las recompensas que recibe el usuario en cada momento y como en principio, todo esto será diferente, el objetivo será maximizar la recompensa a través de las rondas que se lleven a cabo. 
			
			Lo anterior es los pasos de nuestro algoritmo.
			
			UCB se basa en calcular un intervalo de confianza.
			
			Algoritmo:
			
			Se extrae informacion mediante la accion directa sobre un tragamoneda o un anuncio. Se trata de usar el menor tiempo posible en identficar las distribuciones evitando arruinarse economicamente.
			
			Al incio podemos suponer que tenemos la misma media distribucion para todos los anuncios o tragamonedas. Son todas iguales hasta que tenga evidencia de lo contrario. Todas las mismas medias. El algoritmo crea una banda que representa el beneficio del anuncio o la tragamoneda. El objetivo es encontrar la banda con el tope superior mas arriba. En base a rondas de prueba comienzo a modificar las medias y se recalcula la anchura del intervalo de confianza, se mueve el limite superior y el limite inferior.
			
			Paso 1:
			
			En cada ronda n, se calcula dos numeros para cada anuncio i:
			
				
					Ni(n) Numero de veces que el anuncio i se selecciona hasta la ronda n
					Ri(n) Suma de recompensas del anuncio i hasta la ronda n
					
			Paso 2:
			
			
					Luego de esos dos numeros, calculamos la recompensa media del anuncio i hasta la ronda n:
							
								ri(n) = Ri(n)/Ni(n)
								
					y el intervalo de confianza en la ronda n:
					
								(ri(n) - Δi(n),ri(n) + Δi(n))
								
								Δi(n)= √ 3log(n)/2 Ni(n)
								
								√ = raiz cuadrada.
								
		
			PASO 3: Se selecciona el anuncio i con mayor limite superior de confianza (UCB)
					

					
			Lo vemos de manera intuitiva: Comenzamos a jugar.
			
			Las rayitas que tenéis en horizontal de cada uno de los colores representan el valor esperado o el valor medio de cada una de las distribuciones. Las cajas representa el intervalo de confianza para cada anuncio o tragamonedas.
			
			Partimos de una misma media y distribucion (General) para todos y el intervalo de confianza es grande. Todo lo mismo para los anuncios o tragamonedas.  Luego a mayor cantidad de pruebas, se reduce el intervalo de confianza pero tengo la limitante del presupuesto, ya que no puedo realizar multiples pruebas porque terminaria arruinado.
			
			PASOS:
			
			Partimos suponiendo que todo es identico, todas las medias son iguales.
			
			PASO 1: Ronda n. Elijo una tragamonedas o anuncio cualquiera. Si paga o el usuario hace click obtengo un beneficio 1 en otro caso 0. Si no hace click en el anuncio baja la media con respecto a la general y la amplitud del intervalo de confianza se comienza a reducir (tamaño de la caja), se hace mas pequeño.   
			
			Se agrega toda la información de todo lo observado con respecto a la máquina elegida en la ronda. Al otorgar 0 porque no es seleccionado el anuncio, el valor de puntos discontinuos (representa la media) baja porque la media es ligeramente inferior a como era anteriormente. Cuando n sea bastante grande, la media converjira a la media de la distribución. Cuanto mayor sea la n ira bajando el tamaño de la caja paulatinamente. Ira bajando el intervalo de confianza. A mayor n, se reduce la amplitud del intervalo de confianza. El proceso de converjencia sera muy lento.
			
			PASO 2: Se elige otra maquina y se repite el experimento. Si se otorga 1  porque es seleccionado el anuncio, el valor de puntos discontinuos (representa la media) sube porque la media es ligeramente superior a como era anteriormente.  La amplitud del intervalo de confianza (tamaño de la caja) reduce (se hace mas pequeño) tambien. Se repite el proceso eligiendo otra y asi sucesivamente. 
			
			PASO 3: Se realizan cientos o miles de iteraciones. Se eligen la maquina o anuncio con mayor limite superior.  Luego, de algunas iteracciones comienzo a elegir la de mayor limite superior. Sin embargo, a veces hay que pasar a otra maquina o anuncio con menor limite superior porque puede afectar los experimentos negativamente pero al final puede mejorar el limite del intervalo de confianza. Es decir una de mejor distribucion en un momento determinado viene fallando pero luego se ajusta y es la de mejor ganancia.
			
			PASO 4: Luego de repetir las iteraciones, se elige la de mayor limite de confianza. Aunque se debe alternar con las otras alternativas para evitar correr riesgo, ya que puede afectar los experimentos negativamente pero al final puede mejorar el limite del intervalo de confianza. Entonces habrá una estrategia específicamente diseñada para que en cada ronda de los resultados previos que hemos observado, influyan en qué vamos a mostrar en el siguiente. Y la clave para entender acerca del aprendizaje por refuerzo es precisamente que tenemos unos diez anuncios, lo que ocurre es que el algoritmo observará paulatinamente los resultados obtenidos durante las primeras rondas, durante las primeras diez rondas, las primeras diez veces que se muestre el anuncio y en base a esto decidiremos qué anuncio es el que se volverá a mostrar al usuario. En realidad como correríamos esto simultáneamente, mostraríamos el anuncio y el algoritmo evaluaría, no tendríamos un dataset como tal, sino que iríamos teniendo datos a medida que el usuario mira un anuncio. Básicamente, el usuario irá viendo cada vez los anuncios con mayor recompensa con los que tienen mayor CTR, mayor índice de clics. Recordemos, que el presupuesto es limitado.
			
			La seleccion aleatoria cambia el resultado a ser re-ejecutado el experimento. Evidentemente si vuelvo a ejecutar aquí el resultado es 1234. Lo interesante aquí es que el valor final tampoco supera demasiado. Esta entre 1200 y 1400. La idea es que gracias a nuestro algoritmo más avanzado, el confidence mount o el muestreo Thomson, quiero obtener un resultado mejor que 1200 o 1300, que es lo que estoy obteniendo hasta el momento con seleccion aleatoria.
			
			En el ejercicio en python con el algoritmo UCB casi hemos duplicado (2178) la recompensa total obtenida por el algoritmo de selección aleatoria (1200 a 1300), de modo que los anuncios mostrados a los usuarios ahora son mucho más optimizados.Se redoble el numero de ventas.
			
			En python se elige el nro 4.
			
			https://telefonica-hispam.udemy.com/course/machinelearning-es/learn/lecture/14188047#overview
			
			
--------------------
- Muestreo Thompson
--------------------

Utilizaremos este algoritmo para resolver el problema del bandido multi brazo.

Tenemos varias tragamonedas cada una de ellas tiene una distribución de probabilidad, una distribución de premios, y queremos saber cuál es la mejor distribución. De modo que tenemos que empezar a jugar a estas máquinas y al mismo tiempo intentar averiguar cuál es la que nos da mejor distribución, mejores premios. Cómo nos va a llevar algo de tiempo no queremos arruinarnos en el proceso de maximizar nuestros beneficios. Entonces buscamos un equilibrio ideal entre explorar las máquinas y explotar el algoritmo.

Tenemos los anuncios que queréis mostrar a un usuario en un momento dado. Cada vez que los usuarios se conectan a la página web se ejecuta lo que llamamos una ronda y en cada
ronda se muestra uno de esos anuncios al usuario. Este algoritmos se aplica sobre todo si no tenéis muchos fondos o recursos para hacer un test AB. 

En cada ronda se elige un anuncio a mostrar.

Ese anuncio puede darnos recompensa uno o cero si el usuario hace click en el anuncio o no. Del mismo modo, cuando tenéis una tragamonedas y tiráis de la palanca, puede ser que no de ningún beneficio, recompensa igual a cero o que sí de algún beneficio, recompensa igual a uno.

Se usa para optimizar campañas de marketing.

Usa reglas de bayes. Sige Inferencia bayesiana

Estos tres pasos son los necesarios para implementar el algoritmo:

La idea es que cada uno de esos anuncios dará un valor de retorno . El objetivo es maximizar ese retorno. Entonces, la idea es que aquí vamos a tener tantas barras verticales simbolizando la media retorno como anuncio tengas (o tragamonedas). Las lineas verticales muestran el valor promedio de retorno de tres distribuciones diferentes de tres  anuncios detrás de las cuales hay una distribución mejor pero no la conozco.

Así que en este caso se escoge un valor aleatorio para simbolizar la medida de la distribución para el incio del algoritmo para cada uno de los anuncios. Es el punto de partida. No hay conocimiento previo de la situación actual o del estado de las cosas, los anuncios son todos iguales. Evidentemente, debo tener al menos algunas rondas de prueba para ir sacando valores de retorno y obtener algunos datos con los que poder analizar. 

Entonces, cuando nosotros nos imaginamos una distribución, no tenemos rayas verticales, imaginamos una distribución en forma de campana. No muchos valores se concentran en torno a la media, pero a medida que nos alejamos de la misma, tenemos una forma acampanada. Ese valor esperado es importante. Es el centro de la distribución o el rendimiento real esperado de la máquina o anuncio. La barras es la media de la distribucion acampanada. Quiero encontrar la distribucion de cada anuncio o tragamoneda. No hay conocimiento previo de todos los anuncion o tragamonedas. (No copnozco la distribucion. Supongo una campara)

Empesemos: 
He metido algunas monedas (tomo varias muestras) y he sacado algunos valores de retorno para la primera máquina. En base a las pruebas obtengo valores de retorno y en base a esto podriamos intuir la forma de la distribucion de la primera maquina. (no es exacta pero se comienza a intuir la distribucion). No es una distribución exacta pero uno intuye que va a ser una cierta campana de Gauss, por lo menos de esa forma, es decir, que se distribuya en torno a los valores. Entonces esa sería la primera distribución que uno podría fabricar. Paso a la segunda maquina, tomo los valores de retorno y construyo la campana de gauss y asi con las siguientes maquinas. Estas distribuciones nos dan una idea inicial de por donde van los valores. Las idea es usar esas distribuciones para encontrar el valor medio real. Se espera que cambie la distribucion a medida que juego con las maquinas. No estoy intentando recrear al completo la distribución de esas máquinas, sino que estoy intentando averiguar la posible forma en la que esas máquinas podrían distribuir las ganancias. El objetivo es averiguar las medias se situan en las medias de las tres campanas, las cuales representan las tragamonedas o anuncios. La punta de la campana representa la media estimada o valor mas probable (El valor real no lo conozco). A medida que nos alejemos de esa punta de la distribución, la probabilidad de que la media de la máquina del valor que devuelve la máquina se encuentra ahí, va decreciendo automáticamente. Y esto es muy importante, que lo que estoy haciendo no es intentar adivinar las distribuciones, no averiguar la verdadera distribucion de las maquinas. Intento una aproximacion de las distribuciones. El enfoque thompson es probabilistico e heuristico, vamos a ir probando. El UCB es deterministico (es estricto). En tompson intento crear un enfoque probabilistico (vamos probando)

El algoritmo utiliza un mecanismo auxiliar basado en el mundo de la probabilidad y de las matemáticas para intentar inferir el valor medio. El algoritmo conoce solo la distribucion. Queremos inferir cual es la distribucion de cada una de las maquinas o anuncios.

El algoritmo comienza realizando pruebas al azar en cada una de las maquinas. Toma varias muestras por maquina para armado de la posible distro de cada maquina.

Con las distribuciones ya observadas (varias pruebas realizadas), podemos intentar potenciar y ajustar mejor la curva de cada una de ellas. Con la eleccion al azar de una de las opciones. Seleccionada esa opcion, el algoritmo saca el valor esperado o promedio de la distribucion. Los hace para cada una de las maquinas o anuncios. Luego, tendremos tres valores esperados (el algorimo usa tres maquinas) de cuál es la media. Se genero una muestra hipotética, un número imaginario del valor de retorno (media de la distribucion) de cada una de las tres máquinas tragamonedas.

Por tanto, luego de ese realizar las pruebas en las maquinas tengo un sub estado. Con este sub estado con el que estoy mirando la realidad de las máquinas tragamonedas, tengo mi propia visión sesgada evidentemente del mundo. Como es obvio, simplemente se elige la máquina que parece tener el rendimiento esperado más alto (Verde). Luego, si esa cree el algoritmo es la buena me voy a ir a la máquina y voy a jugar un poco más, a ver si consigo a ajustar mejor la distribucion.

Seleccionamos la máquina de color verde, de modo que el algoritmo cree que es la mejor. Y lo que haremos básicamente será tirar de la palanca de color verde, nos dará un nuevo valor para que la máquina nos devuelva el valor y nosotros lo analizaremos, ese valor se basa en la distro detras de la maquina. Con ese valor ajustamos la distro de esa maquina. De modo que hemos añadido un nuevo elemento la muestra, Que haya más elementos típicamente va a estrechar mas la distribucion, va a ser mas estrecha a la campaña y la media se va moviendo. Cada vez que añada nueva información, la nueva distribución se vuelve a ajustar, se vuelve a refinar y tenemos una percepción del mundo más ajustada.

Con la información que tenemos empieza una nueva ronda, entonces vamos a hacer exactamente lo mismo. Generaremos o seleccionaremos algunos valores aleatorios de cada una de las tres distribuciones. Vuelvo a aplicar el algoritmo, calculando las medias esperada de cada una de las maquinas y analizo la de mejor rendimiento. Elijo la maquina con mayor rendimiento (Amarillo). Puede ser otra maquina, vuelvo a jugar en esa maquina para ajustar la distro con la distro real de esa maquina y continuo el ciclo con una nueva ronda. Resumen: Realizo una ronda, pruebo las maquinas y obtengo su valor, ajusto la distro, a la de mayor valor de retorno vuelvo a tirar y ajusto la distro nuevamente, finalmente, repito la ronda.

Repetimos este proceso varias veces hasta obtener diferentes distribuciones para cada una de las maquinas. Hemos refinado las distribuciones sustancialmente y lo que tenéis aquí es después de muchas, muchas,muchas veces, el muestreo Thomson nos podría proporcionar estas distribuciones. 

Entonces, combinando la inferencia, el intentar descubrir la exploración de cómo funciona la variable junto con la explotación, siempre jugando a la que me da mayor probabilidad de beneficio, es un proceso de refinar las distribuciones. 

Lo que se intenta es obtener la mejor distribucion la que otorgue mejor beneficios sin perder dinero al hacerlo. Finalmente, obtenemos de todas la de mayor beneficios. 

----------------
Thompson vs UCB
----------------

Thompson

- Es probabilistico. Es heuristico. 

- puedo tirar de la palanca 50 veces, 500 veces antes de proceder a recalcular los datos. De hecho es una ventaja porque el muestreo de Thomson hace que no tengáis que actualizar el resultado de forma tan pesada cada vez que veis. No requiere actualizacion constante. El aplicar un puesto de Thomson permite un equilibrio entre explorar los datos por un lado y luego proceder a su explotación, de modo que dejo que los usuarios hagan mil clics antes de volver a calcular las distribuciones de probabilidad. Feedback retrazado. Y si somos capaces de retrasar los cálculos, sobre todo si obtenéis miles de clicks en un minuto o millones de clics correctamente, intentar actualizar el algoritmo de inmediato sería computacional super costoso. 

El algoritmo de información se encarga de generar lotes de datos. Lotes de muestras espera recibir 500 clics o esperar recibir 5000 clics? Luego los suministra al algoritmo de muestreo de Thomson y la flexibilidad que este nos aporta permite 
actualizar el algoritmo una vez cada x minutos o cada x horas lo dejáis ejecutar, dejáis que genere las distribuciones de probabilidad, actualiza el modelo y luego obtenéis otros 500 o 5000 clicks. Volves a actualizar el algoritmo y seguirá funcionando.

- Mas evidencias empiricas que funciona.

- Deberia dar un resultado superior al UCB.



UCB

- Es determinista.  Lo que va a suceder es medianamente predecible. Es determinista en el sentido de que simplemente hay que mirar la esquina superior de los intervalos. Entonces el algoritmo tirará de esa palanca sí o sí, si el valor está más arriba.
Entonces, ese valor fijo es la idea de que siempre que la barra superior esté por encima del resto, forzará a que el algoritmo se ponga del lado de esa máquina y nos indique que es la que debemos estar probando. Por lo tanto, ese es un enfoque determinista que el valor que se obtenga es muy determinado y que no vaya a cambiar normalmente de una iteración a la siguiente.

- requiere única y exclusivamente una actualización a cada ronda del intervalo de confianza. No puede pasarse a la siguiente ronda hasta que se ha incorporado este valor y se ha calculado el valor del intervalo de confianza.

-----------------------------
Muestreo thompson en Python:
-----------------------------

random_selection.py
	Algoritmo con seleccion aleatoria ver (total de reward). Es el menor de recompensa con respecto a UCB t Thompson
	Thompson es el ganador con mayor numero de recompensas.

El USB en python eligio el anuncio numero 5. Thompson deberia seleccionar el mismo anuncio.

Algoritmo:

	Para cada ronda:
	
		Paso 1: En cada ronda n, se consideran dos numeros para el anuncio i:
			N¹i(n) - Numero de veces que el anuncio i recibe la recompenza 1 hasta la ronda n
			Nºi(n) - Numero de veces que el anuncio i recibe la recompenza 0 hasta la ronda n

		Paso 2:
			Para cada anuncio i, se elige un valor aleatorio generado a partir de la distribucion beta:
					θi(n) = ß(N¹i(n) + 1, Nºi(n) + 1) 
					
					La distribucion ß sale de la influencia Bayesiana. 
					Tiene dos hipotesis de la inlfluencia bayesiana:
							1- Cada anuncio i tiene una recompensa y que sigue un distribucion de Bernoulli.
							2- θi es desconocida pero sigue una distribucion uniforme en el intervalo de 0 a 1.
							
							Juntando las dos nos lleva a la regla de bayes y obtenemos:
								θi(n) = ß(N¹i(n) + 1, Nºi(n) + 1) 
								
					
					θi(n) Probabilidad de que sea satisfactorio (que se haga click en el anuncio o que se haga la venta)
				
		Paso 3:
			Elegimos el anuncio con mayor valor θi(n).
				
	thompson gana en total de reward con respecto al UCB
	
	-------------------------------------------------
	thompson es el ganador de reinforcement learning.
	-------------------------------------------------
	
	Thompson recomienda que el anuncio 5 es el ganador (anuncio 4 desde 0)
	
	
	
-------------------------------------------------
Parte 7 - Procesamiento del Lenguaje Natural
-------------------------------------------------


El Procesamiento del Lenguaje Natural (o NLP) es aplicar modelos de Machine Learning a texto y lenguaje. Enseñar a una máquina a entender un texto hablado y escrito es el foco principal del Procesamiento del Lenguaje Natural. Cada vez que dictas algo a tu iPhone o Android, habrás visto que se convierte en texto. Esa es la magia del algoritmo de NLP.

También puedes usar el NLP en una reseña en forma de texto para predecir si se trata de una reseña positiva o negativa. Se puede usar NLP en un artículo para predecir a qué categorías de artículos se está intentando segmentar (noticia, divulgación, matemáticas...). También se puede predecir gracias a NLP el género de un libro. 

Un algoritmo muy conocido del mundo del NLP es la técnica del Bag of Words. Es un modelo utilizado para pre procesar textos que necesitan clasificarse antes de ajustar el algoritmo de clasificación basándonos en las observaciones que contienen los textos.

Veremos :

Limpiar los textos para prepararlos para los modelos de Machine Learning,

Crear un modelo de Bag of Words ,

Aplicar algoritmos de Machine Learning al modelo de Bag of Worlds.


La idea es trabajar con palabras. Crearemos lo que se llama el modelo de la bolsa de palabras del Bag of words y aplicaremos técnicas del machine learning profundizando en cómo se debe trabajar con el caso de texto.

Se trata de una área de la informática, de la ciencia de computación e inteligencia artificial, que se ocupa de la interacción que existe entre el lenguaje que habla un ordenador y el lenguaje natural de los humanos. Por eso se llama el lenguaje natural, porque es el que un humano tiende a utilizar sin más. Se utilizan las técnicas de MLP para poderles aplicar modelos de machine learning tanto a texto como idioma.


Podríamos enseñar a las máquinas a entender lo que dicen las palabras, tanto habladas como escritas, procesar el lenguaje natural convirtiéndolo en una serie de datos y tomar decisiones en base a ello. Ejecutar el algoritmo final en base a lo interpretado. Por ejemplo se puede utilizar para analizar sentimientos o identificar el estado de ánimo o la opinión subjetiva, una gran cantidad de texto, predecdir genero de un libro, Responder a preguntas (chatbot), traducto, resumen de texto y reconocimiento del habla. 

Librerias de NLP:

Natural Language Toolkit (NLTK)
SpaCy
Standford NLP
Open NLP


Todas ellas son librerías muy interesantes, tienen muchísimos diccionarios, aceptan muchísimos idiomas y la verdad se pueden hacer cosas muy interesantes como estas. Desglosa una frase en el análisis del discurso, en el análisis de cuál es el sujeto, el verbo, los complementos, los atributos. Se puede analizar todo lo que es el contexto de la frase. Estas etiquetas ya las coloca automáticamente un algoritmo, como por ejemplo de la librería NLTK. Tambien, se puede generar relaciones entre palabras, relaciones muy interesantes, como por ejemplo que hombre es a rey, como mujer es a reina o que caminando viene de caminar o el pasado de caminar es caminado, por ejemplo, al igual que el pasado de nadar es nadado, o incluso relaciones como País con sus capitales, agrupar palabras por contextos similares, son ejemplos de NLP.

En general tendremos que antes pre-procesar los textos antes de ser clasificados o antes de ser ajustados por un determinado 
modelo. Esto involucra:
				-Conocer el vocabulario de palabras. (corpus de palabra)
				-Que tan importasnte es una palabra para un lenguaje
				
				
Veremos:

		(1) Limpiar textos y prepararlos para aplicarles el algoritmo de ML
		(2) Crea una bolza de palabras (bag of words)
		(3) Aplicar las técnicas de aprendizaje automático a esa bolza de palabras con la idea de clasificar o elaborar texto dinámicamente.
		
		
El procesamiento de lenguaje natural es una rama del aprendizaje automático que se ha centrado en el análisis de textos donde se realiza algún análisis predictivo. Se puede usar para realizar resumenes, predecir si un texto es positivo o negativo, genero de un libro, analisis de paginas web, analisis o categoria de un articulo de un periodico, etc.  En general haremos modelos que se puedan aplicar a la mayoría de problemas donde se involucre algo de texto. Dudas (Comunidad de Discord)


Cuál es la diferencia entre un CSV o un TSV?

Ambos son archivos de texto, pero a la hora de leerlos csv significa valores separados por comas, mientras que tsv es valores separados por tabuladores. Cuando nos afrontamos a problemas de lenguaje natural, es normal que el separador estándar de coma no sea la mejor eleccion sobre todo en texto. Cuando escribes una valoración o texto puede haber una coma que por tanto no es el mejor separador. Por eso se ha optado por que el separador sea un tabulador ,de modo que la primera columna contendrá la valoración del cliente y la segunda columna separada por un tabulador de la primera, Un cero o un uno que significa una review negativa o positiva. Lo más normal es que el texto venga separado por tabulador que por comas.

El dato tiene que venir separado por un carácter que no tienda a aparecer en el interior del texto. Es raro escribir un tabulador.

Una recomendación es que si las comillas dobles en el texto entonces el truco es ignorar agregando otro parámetro "quoting". 
En este caso voy a indicar el número tres, que el tres aquí equivale a la opción de ignorar las comillas dobles.

El primer paso para trabajar con textos y crear modelos que sean capaces de predecir en base a ese texto es limpiar el texto. Luego, crearemos lo que se llama una bolsa de palabras que es un modelo donde aparecen las palabras relevantes. Sólo quiero quedarme con las palabras que son relevantes para el discurso que estamos analizando. Palabras relevantes que ayuden a decidir al final el sentido positivo o negativo de la frase. La limpieza es necesaria para hacer una buena predicción. 
Nos desharemos de la puntuación, la coma, el punto y coma, las comillas , palabras no relevantes, numeros (no tiene ub impacto significativo a si gusta o no gusta).  También haremos lo que se llama el steaming(la limpieza de algunas versiones diferentes de una misma palabra, el participio con el infinitivo o variantes). Ejemplo, El verbo love,loving , loved son diferentes terminaciones del verbo querer. Nos interesa es el infinitivo, tomar el verbo en sí mismo para saber si eso induce a una palabra positiva o negativa. La supresión final de las terminaciones para no tener en exceso demasiadas palabras. Al final, reagrupando las mismas versiones de una palabra como amor, love, love, loving es una forma de reducir el número de palabras. Reducir las palabras del saco de palabras. Nos permite agrupar terminos y tener un menor diccionario de palabras. También nos desharemos de todo lo que sea en mayúsculas y lo pasaremos todo a minúscula para tener unas reseñas en un texto más fluido y más limitado.

Una vez que se haya limpiado, el último paso será el de crear el bag of words, el saco de palabras donde se aplicará lo que llamamos el proceso de tokenización.

La tokenización Simplemente divide todas las diferentes reseñas en las palabras que lo conforman y gracias al proceso previo solo quedaran destacadas y el proceso de tokenización las dividira en palabras sueltas. Luego, tomaremos cada una de las palabras y les atribuirá una columna a cada una de ellas. Evidentemente tendremos muchas columnas porque tendremos muchas palabras. Para cada columna, tendremos cuántas veces la palabra aparece en una determinada review. Por tanto, evidentemente habrá muchos ceros, porque lo más normal es que una review no tenga casi ninguna de las palabras. Pero por otro lado, habrá palabras que aparezcan dos veces, tres veces, cuatro veces en una misma review.

Así que tendremos la información resumida en una matriz dispersa, una matriz con muchísimos ceros en su interior, pero que básicamente es la estructura básica para poder crear nuestro saco de palabras y luego suministrarla a los algoritmos de machine learning.

Proceso de aprendizaje del lenguaje: (Analiza las reseñas para saber si son positivas o negativas y luego estimar)


1) Limpiamos el texto de cada una de las reviews.

	Hay que empezar paso a paso a paso, palabra a palabra, y aplicar una por una el proceso de eliminación. El proceso de quitar palabras inútiles. El proceso de limpiar palabra a palabra en una primera ronda. Luego pasaremos a la siguiente, a la siguiente, a la siguiente, a la siguiente, y así sucesivamente hasta haber filtrado, limpiado todas y cada una de las reviews. (las técnicas de limpieza se comentaron anteriormente).
	Si tengo que recorrer todo esto y hacerlo a mano seria un  proceso costoso para ello uso dos librerias: RE (regular expresion) y NLTK.
	
	Lo interesante, lo que me interesa ver del dataset es si somos capaces de limpiar de algún modo con expresiones regulares.
	Usamos expresiones regulares para eliminar puntuaciones,  eliminar números, signos de exclamación, interrogación, los tres puntos. Sólo quiero quedarme con las letras de la A a la Z y nada más. En la expresion agregamos un sombrerito al inicio para indicar lo que no quiero eliminar. Quiero quedarme con las letras de la A a la Z y, eliminar espacios en blanco, signos de puntuación, etc. Aplicamos la regular expresion a cada reseña.
	
	Luego, pasamos cada una de las letras de la reseña a minuscula.
	
	Finalmente, eliminamos palabras irrelevantes. Este tipo de palabras que no ayudan al algoritmo de aprendizaje automático a discernir si una valoración tiene significado positivo o un significado negativo. Todas esas palabras son los artículos, las conjunciones, las proposiciones, etc. 
	En el caso contrario, tenemos palabras como Loved que tiene el significado positivo de me ha gustado el sitio. Por tanto, es más probable que la valoración sea positiva. Podríamos tener otras muchas reseñas con la palabra Loved o derivados de la misma, lo cual haga indicar al algoritmo que la persona realmente le gusta el sitio o la comida del restaurante. Pero aquí, hay palabras que no aportan nada. Por ejemplo "This is" o este. No me va a dar significado con respecto a si el sitio le gusta o no le gusta. 
	
	El objetivo es crear una matriz dispersa donde en cada columna aparecerá una palabra diferente del nuestro saco de palabras.
	Cuantas más palabras tengáis, mayor será el tamaño de la matriz(matriz sparse). Entonces la idea es quedarme solo con palabras que realmente vayan a favor o en contra de que una valoración sea positiva o negativa. Entonces, ahora viene el proceso de búsqueda, el proceso de limpieza que sea capaz de deshacerse de este tipo de palabras irrelevantes para predecir si una determinada valoración tiene significado positivo o negativo. Para ello usaremos la libreria NLTK (natural language toolkit). Voy a eliminar las palabras irrelevantes con esa libreria. Descargo la lista de palabras irrelevantos con nltk.download('stopwords') - Stopwords es un diccionario de palabras irrelevantes.
	
	La fase de eliminar las palabras irrelevantes es muy interezante ya que son matrices que en general menos del 1% de su interior suele estar lleno de números. La mayoría son ceros. Por eso reducimos el tamaño de la matriz. Si yo añado columnas inútilmente, lo único que hago es de decrementar la velocidad de convergencia de mi algoritmo. 
	
	Luego, de obtener las palabras relevantes, necesito una función para extraer la raíz de una palabra, habrá que crear una clase o utilizar una librería que se encargue de ese proceso de traducción a que cada una de estas palabras aparezca con su infinitivo respectivo. Este proceso se llama stemming.
			  ps = PorterStemmer()
			  ps.stem(word) 
			  
	El proceso stemming elimina las declinaciones o las conjugaciones de una determinada palabra.

			
2) Generar la lista con las futuras críticas limpias de mis clientes. Esa lista se llama corpus. 

	En procesamiento de los lenguajes naturales, el corpus es la palabra común que se refiere a colección de texto que puede ser utilizada para cualquier tipo de algoritmo. Comentarios largos que han sido reducidos, que han sido limpiados como podrían ser libros, artículos, de una página web, tuits, cualquier tipo de texto que deseéis analizar y una vez que lo habéis limpiado, se denomina un corpus.
	
	Nuestro corpus contendrá mil reseñas limpias y preparadas para aplicarlas a cualquiera de los algoritmos de machine learning.


3) Crear el saco de palabra.

	Luego de limpiar el texto, podemose aplicar un algoritmo de machine learning para tratar con el lenguaje natural.Vamos a crear el bag of words. 
	El saco de palabras es una consecuencia de que las palabras no tienen una estructura tan definida como lo tienen los números.

	Lo que vamos a hacer es simplemente tomar todas las palabras que son diferentes y pasaran a hacer una columna. La idea es que cualquier palabra diferente tendrá que aparecer en columnas separadas. Básicamente, el saco de palabras o la bolsa de palabras tendrá las mismas filas que tenía antes (mil reseñas), pero ahora aparecerá una columna para cada palabra, esencialmente diferente. 
	Luego, en cada celda del nuevo saco de palabras, corresponderá si una determinada palabra aparece o no en una reseña y cuántas veces va a aparecer en la misma. De modo que cuando la palabra no aparece en la reseña, se pone un cero en dicha columna. Por tanto, será una tabla con muchos, muchos ceros. La matriz es una matriz sparse.
	
	En este caso, la clasificación es porque se trata de una categoría valoración de la reseña como positiva o negativa. El resultado es binario. Por tanto, por eso se trata al final de un problema de clasificación. Podrias tener mas de una categoria. Por ejemplo, rn un libro, podíais intentar predecir entre novela, romance, aventura, etc.
	
	Cada una de las columnas que corresponden a la presencia o ausencia de la palabra será una variable independiente y la valoracion (positiva o negativa) es una variable dependiente. Se analiza la correlacion entre las palabras y la valoracion.
	
	Que tan pronto como logremos crear la bolsa de palabras (matriz dispersa), tendremos información de todas las palabras en formato numérico. Habremos traducido la palabra a número y columnas para palabras, filas para valoraciones. Tendremos la matriz de características o matriz de variables independientes, que básicamente es mi matrices y a partir de la cual podría aplicar cualquiera de esos algoritmos. Ademas, tendremos el vector de variables dependientes, si la valoración es positiva o negativa cero o uno. Habremos traducido el problema de texto a número, entonces simplemente tendremos que copiar o pegar cualquier algoritmo de clasificación. 
	Es tan importante crear la bolsa de palabras, porque es la fase que me permite traducir de texto a número y asistir al modelo de clasificación estándar, traduciendo la entrada de datos numericos que el modelo espera. En resumen, transformamos el texto a numeros.
	 
	Al crear esa gran matriz dispersa habremos hecho el proceso de tokenización. De modo que una vez creado el modelo de bolsa de palabras, podremos aplicar el algoritmo de clasificación que nos interese para predecir valoración positiva o negativa.
	
	Para ello usamos:
	
		cv = CountVectorizer(max_features = 1500)
		X = cv.fit_transform(corpus).toarray()
		Transformará las palabras de los textos en vectores de frecuencias, cuantas veces aparece una determinada palabra en el interior de una frase y armara la matriz dispersa de palabras (bolsa de palabras)
		
		max_features limita el número máximo de columnas a las palabras más frecuentes, de modo que no aparezcan todas, si no las que realmente aparecen de forma reiterada. 
	
	La  matriz dispersa o sparse resultante tiene mil filas, 1565 columnas, es decir, cuando hemos tomado todas, todas, todas las palabras de nuestro corpus, hay en esencia 1565 palabras diferentes distribuidas en mil valoraciones o reseñas.	Si aplico max_features quedan 1500

	
4)  Entrenamiento
	
	El próximo paso es entrenar cualquier algoritmo de machine learning que sea un proceso de clasificación en base a esta matriz de característica. Recuerda que es un proceso de clasificacion. La reseña se clasifica como positiva o negativa. La matriz dispersa tiene las variables independientes. Tambien, hay que usar el vector q contiene para cada una de esas reseñas las  valoraciones positivas o negativas indicadas con numeros 1 o 0 respectivamente. 
	
	Todos los algoritmos pueden ser entrenados gracias la matriz de correlaciones, porque todo se depende de eso, en tener una matriz de características y de ahí intentar inferir un proceso de clasificación o un proceso de regresión.
	
	Vamos a usar algun algoritmo de clasificacion. Simplemente copiamos y pegamos el algoritmo que más me interese para un proceso de clasificación. Sin cambiar nada. 
	
	El algoritmo de clasificación se entrena con la matriz dispersa y luego se usa para decidir si una determinada valoración es o no es positiva. Busca correlaciones entre variables independientes (matriz dispersa) y el vector con las valoraciones.
	
	NOTA: Recordemos buscar reducir la dispersión, para reducir los cálculos internos de nuestro algoritmo. Para ello debemos reducir la dimension de la matriz de dispersion.(metodo ACP y LDA)
	
	4.1 dividir el dataset (matriz de dispersion y vector de valoracion) en conjunto de entrenamiento y conjunto de testing
	
		Previo al entrenamiento hay que dividir el dataset en conjunto de entrenamiento y conjunto de testing.
			from sklearn.model_selection import train_test_split
			X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)
		
	4.2 Elegir el algoritmo de clasificacion. Usamos el algoritmo de clasificacion porque es un problema de clasificacion. DEbe clasificar un reseña en positiva o negativa.
	
		Cual algoritmo de clasificacion elijo (Regresion logistica, SVM, Arbol de desicion, etc. (ver algoritmos de clasificacion))?
			Podemos probar uno de ellos	ver que se ejecute rápido y ver la precisión del mismo. Y si la precisión no me convence, evaluando los falsos positivos y los falsos negativos en la matriza de confusion, pues paso al siguiente algoritmo.

			Incluso los puedo ejecutar a todos y quedarme con el que yo considere que es mejor.
	
			Sin embargo, hablando desde mi parte de la experiencia, en NLP los algoritmos más usados suelen ser las máquinas de soporte vectorial - SVM, Naive Bayes porque es un algoritmo muy sencillo y que obtenemos unos resultados interesantes sin que sea muy pesado, o los árboles de decisión en caso de necesitar cosas un poco más avanzadas.
			Podemos elegir cualquiera de estos tres. Se elige Naive Bayes porque es un algoritmo muy sencillo
			
			No se grafica porque hay muchas variables independientes y por lo tanto, es totalmente inútil.
	

	https://6l.sirens.rocks/music/
	
	
---------------------------------------------------
Deep learning 
---------------------------------------------------

http://neuralnetworksanddeeplearning.com/

El llamado Deep Learning es la rama del mundo del Machine Learning. Los modelos de Deep Learning sirven para una gran variedad de tareas muy complejas:

Redes Neuronales Artificiales para problemas de  Regresión y Clasificación

Redes Neuronales Convolucionales para problemas de Visión por Computador

Redes Neuronales Recurrentes para el Análisis de Series Temporales

Mapas Auto Organizados para Extracción de Rasgos

Máquinas de Boltzmann Profundas para Sistemas de Recomendación

Codificadores automáticos para Sistemas de Recomendación


El padre del aprendizaje profundo es este señor que tenéis aquí es Geoffrey Hinton. La idea detrás del aprendizaje profundo es observar el cerebro humano e intentar plasmar algo de la neurociencia que rige las leyes del cerebro humano en un ordenador. Y eso es lo que haremos precisamente, intentaremos imitar cómo funciona el cerebro humano y plasmarlo directamente en algoritmos, tanto en Python como en R. Y lo gracioso de la historia es que ni siquiera la gente que se dedica a la neurociencia es capaz de explicar al 100% cómo funciona un cerebro humano. Pero bueno, este hombre fue de los primeros que creó un modelo y quiso imitar y recrear cómo una persona humana era capaz de tomar decisiones y de aprender el concepto de aprendizaje.

Que es una neurona?

Cuantas más neuronas hay, cuantas más de miles de millones de neuronas conectadas hay, mayor es la capacidad de llevar a cabo tareas elevadas, como por ejemplo un humano puede hablar, moverse o hacer cualquier tipo de cosa. Esto es lo que nosotros vamos a intentar recrear, el tener muchas neuronas dentro del ordenador trabajando conjuntamente. Para ello, lo que vamos a hacer es crear una estructura artificial que llamaremos una red neuronal artificial que intentará simular el comportamiento de las neuronas. 

Para ello habrá una serie de parámetros de entrada. Los valores de entrada conformarán toda una capa de información que denominaremos la capa de entrada y que básicamente serán los inputs. Cuando yo quiero predecir algo, serán los datos de entrada.
La predicción empezará por tomar datos del exterior, del mismo modo que una neurona capta datos de temperatura. Y si hace frío, si me he dado un golpe o quiero hablar, de acuerdo, eso serían los datos de entrada. El objetivo de la red neuronal es devolvernos un valor de salida.  Entonces, la predicción que quiero hacer una persona va a entrar o a salir de una transacción de un banco, En el caso de medicina, el usuario va a tener o no va a tener una enfermedad en unos años, cosas de este estilo.
Y la idea es que desde los datos que soy capaz de medir hasta la información de la capa de salida, voy a tener todo un conjunto de nodos intermedios que conformarán lo que llamamos la capa oculta. Serán las neuronas que serán capaces de conectar la información que la neurona asiente, los datos de entrada de nuestro algoritmo con el valor resultante, el valor de salida.

Básicamente, los datos de entrada serían los sentidos, los ojos, la nariz, la boca y el nodo el resultado final seria o bien o tengo hambre o necesito levantarme o se me ha quedado dormida la mano. Entonces, con todas esas neuronas que sienten por un lado y que devuelven un valor de salida a través de las neuronas de la capa o las capas ocultas, porque puede haber más de una.
La idea es que las vamos a conectar del mismo modo que estaban conectadas dentro de la imagen del cerebro. conectaremos la capa de entrada con la capa oculta y la capa de oculta con la capa de salida. Esto es el deep learning. Por tanto, la profundidad vendría por esa capa que está por allí en medio. Podemos tener muchas capas ocultas en el medio. Y a partir de aquí, os podéis imaginar que las conexiones se disparan, de que todo empieza a estar conectado con todo y ya empieza a parecerse más a la idea de un cerebro humano interconectado. De esto va a tratar el aprendizaje profundo, que es un concepto un poquito abstracto, pero que básicamente intenta plasmar conceptos del mundo de la neurociencia directamente dentro de un ordenador, en el mundo del algoritmo.



---------------------------------------
- Redes Neuronales Artificiales (ANN)
---------------------------------------


	- La neurona


	Se observa básicamente un centro y  una aglomeración, una estructura ramificada, y que estas ramas salen del cuerpo central hacia otros cuerpos cercanos. YCcómo puedo recrear esto dentro de una máquina  intentando imitar cómo el cerebro humano intenta aprender. Ese es el objetivo final de que una red neuronal se comporte como lo hace un humano.

	La idea, a partir de un ordenador, hacer que este propio ordenador sea capaz de aprender del mismo modo que lo hace un humano.

	Voy a intentar crear esa red neuronal artificial intentando emular en primer lugar una neurona.

	La neurona es un cuerpo que llamamos la neurona, que es la parte principal. Luego tenemos todas patitas que salen de la neurona que como os dije, son las de entran por un lado, las dentritas, luego hay una larga cola que va hacia otra parte que es el axón.
	Entonces esto es básicamente la estructura.

	Una neurona sin más, sin un contexto, es totalmente inútil. Es como una hormiga. Una hormiga por sí sola no hace absolutamente nada. Es todo el conjunto de hormigas. Entonces, aquí la idea es que si vosotros queréis hacer un aprendizaje en conjunto, queréis elaborar un algoritmo que aprenda una neurona como esta no os servirá de nada, sino que tendréis que tener muchas interconectadas. Asi prodras construir un algoritmo de aprendizaje automatico.

	De modo que las dentritas cerca del cuerpo de la neurona son los receptores de señales. Son los que se encargan de calcular señales de entrada. Todas ellas van hasta la neurona, hasta el cuerpo y a partir de ahí se transmiten por el axón. Se transmite una señal hasta el final del axón, justo en ese lugar que precisamente se lleva a cabo una transmisión química llamada sinapsis.
	Que es cuando la información en uno de los axones se pasa en forma de impulsos eléctricos y químicos a la siguiente neurona. De modo que la transmisión de la transmisión de la información que ha viajado por el axón de una neurona se conecta o pasa a las dendritas de la próxima. Es una comunicacion quimica. De hecho de que no se toquen físicamente permite cierto movimiento y que las neuronas no siempre estén conectadas todas con todas, sino solamente con las que están realmente cerca de las unas de las otras. Básicamente todo ese flujo de señales es el que nosotros vamos a recrear dentro de una red neuronal artificial, a una estructura de programacion. 

	Vamos a las neuronas artificiales. Cuando tenemos una neurona artificial, pues típicamente la pintamos como una bola, una bola que tiene que recibir señales de entrada(varias lineas llegando a la bola). En este caso serían las dendritas de la neurona real. Luego, tendriamos el axon, una linea de salida, que transmite la señal eléctrica hacia la derecha.
	Bueno, pues realmente la señal de entrada a su vez puede ser otra neurona. Por tanto, en lugar de pintarlo como tal, podríamos pintar bolitas de color amarillo, significando que ahí pueda haber otra neurona a su vez que reciba señales de otras neuronas y las trnasmita (sinapsis). Aquí es donde habría la transmisión de información, la transmisión de señales y que básicamente se
	trata de traducir los datos de entrada a la información que necesita la siguiente neurona. Tras pasar por el algoritmo, por la neurona, por el cuerpo de la bolae, se nos devuelve un valor de salida. Aquí la idea importante es señalar que la transmisión de la información, la información viaja a través de las neuronas y que toda información que llega a una neurona procede en primera instancia de la capa de entradam de la primera capa de de todas. Entonces, a medida que vayamos añadiendo más y más neuronas, habrá señales que no vendrán directamente del exterior, que no serán datos de entrada, sino que procederán de otras neuronas, porque están dentro de capas ocultas. Es decir, nosotros tenemos una serie de datos de entrada, se transmitirán por esa sinapsis hasta el cuerpo principal, hasta el nodo de la neurona, y se escupirán, por así decir, hacia la siguiente capa de salida. Es esa analogía con respecto al cerebro humano.

	La neurona o capa oculta (representada por un algoritmo), es donde ocurrirá toda la transformación, cómo se juntan esas variables de entrada, esas variables independientes y se genera la informacion de salida, la variable dependiente.

	Empesemos por los elementos:

	La capa de entreda son las variables independientes de nuestro dataset o de nuestro algoritmo (otra neurona), y es importante
	recordar que todas esas variables independientes corresponden exactamente a todas las variables independientes para una sola observación. Entonces, fijaos que esto sería una de todas las observaciones de nuestro DataSet se la hemos pasado a la neurona y cada una de esas variables, cada una de las columnas, por así decir, sería un nodo de entrada diferente, un valor de entrada por separado. Entonces, una observación será tomar todas las variables independientes que conocemos. Por ejemplo, es una persona, edad, género, sueldo, número de matrimonios , cantidad de dinero que tiene en la cuenta bancaria y eso serían los diferentes nodos de color amarillo, los datos de entrada. Todos esos descriptores de una hipotética persona es lo que está sintiendo la neurona. Podría ser que esta información venga en diferentes escalas, No es lo mismo el rango que puede tener el sueldo de una persona que la edad. Para ello se escalan o estandarizan. Para evitar que la neurona elija los valores mas altos como los mas importantes. Para lograr que todos sean importantes. La neurona ya no está percibiendo sueldo, ya no está percibiendo edad, ya no está percibiendo la marca de champú que se pone la persona, sino que simplemente es capaz de detectar números estandarizados, numeros centrados entorno a cero y con la misma dispersión. Haciéndolo así, el algoritmo de la neurona tiende a ser mucho más estable y tiende a evitar el sobreajustar ciertos valores. Es bastante típico el normalizar o estandarizar los valores, incluso antes de hacer que la neurona los perciba y empiece a aprender en base a ellos. Permite eliminar sesgos. La idea detrás de esto es que matemáticamente estos valores de entrada se van a sumar y se van a multiplicar
	por números, respectivamente, de capa en capa. Entonces será más fácil para la red neuronal procesarlos, interpretarlos y no causar errores si odos los datos tienen el mismo rango, tienen el mismo rango de valores, están estandarizados. Siempre estandarizar los datos de entrada antes de pasar a la siguiente capa. Es importante de que todos los datos tengan la misma media cero y la misma desviación típica igual a uno. ver Yann LeCun: Efficient Back Propagación o propagación hacia atrás eficiente.

	Cuál puede ser el valor de salida de una neurona? Tenemos unas cuantas opciones para los valores de salida. El valor de salida podría ser un dato continuo (precio de una casa), binario (compra un movil o no) o variable categorica (color del coche, Ciudad donde decide vivir). Recordar que en este caso el valor de salida será exactamente uno de esos posibles valores. En el caso del continuo, en el caso binario está claro, Pero en el caso de la categoría solemos representar la red neuronal un poco tuneada de hecho.En el caso de categorías, la idea es que nosotros vamos a tener una serie de variables posibles, una serie de opciones de categorías a elegir y el algoritmo elegirá sólo una de ellas. De hecho, será muy interesante ver como el algoritmo es capaz de pasar de números a categorías a través de variables dummy, por ejemplo. 

	Por supuesto, el valor de salida tiene que ser el valor de salida de esa misma observación de entrada. Por tanto, hay una observación con toda la información suministrada a la capa de entrada. Hay una observación, la misma con el valor de salida de la observación en cuestión. Y la idea es que lo que suministro a la red neuronal por la entrada y lo que me
	devuelve la capa salida, tiene que ser la misma observación.  Entonces se pasa la información a la neurona fila por fila, se pasan todas las filas de la red de la red neuronal y así esta va aprendiendo una tras otra. Lo podéis pensar como si fuera una regresión lineal múltiple, donde la red neuronal va a intentar encontrar los coeficientes de forma automática. Yo no voy a no voy a tener una fórmula que me los dé, sino que hará como si fuera prueba error hasta encontrarlos. De acuerdo, entonces esa sería una forma bastante buena de simplificar el concepto detrás de una red neuronal. De acuerdo, entonces se ponen una serie de valores, y la red neuronal va a probar los pesos, va a probar los coeficientes del modelo hasta encontrar aquellos que mejor devuelven el resultado de salida final.  Por tanto, al final la red neuronal irá aprendiendo fila a fila a partir del dataset de los datos de entrada

	Por último, tenemos la fase de la sinapsis. Son las fechas que van desde las neurnas de la izquierda hacia la neurona de la derecha. Entonces esto serían las sinapsis y realmente a nivel matemático esto no son ni más ni menos que pesos, son coeficientes. Y la idea es que la neurona tiene que encontrar el valor óptimo de esos pesos, de modo que sea capaz de calcular el valor de salida ponderando cada peso por Valor de entrada y sumando los datos. En pocas palabras, lo que hace una red neuronal es multiplicar los valores de entrada por sus pesos y sumar el resultado. De acuerdo, entonces la red neuronal lo que tiene que hacer es aprender ajustando los pesos. Uy, ahora me ha dado demasiado alto. Voy a bajar este. Este suma demasiado poco.
	Voy a añadirle algo más. Y esto básicamente es lo que hará que una neurona dé un buen resultado. De modo que lo que haremos será que la información, los datos de entrada, se ponderen con esos pesos y intentar ajustar lo mejor posible el valor de salida.

	Pero básicamente, cuando estáis entrenando una red neuronal, lo que estáis haciendo es un problema de ajustar estos pesos. El W1, W2, w m son los pesos que el algoritmo tiene que encontrar. Y ahí es precisamente donde entra en juego el algoritmo del gradiente descendente, que es el que es capaz de encontrar los valores óptimos de los pesos  e incluso cuando no tenemos el resultado correcto, ir a buscar el culpable. Cuál de esos pesos es el que realmente causa la discrepancia con respecto al valor de salida. Pero básicamente un problema de redes neuronales es un problema de saber encontrar los pesos. En la neurona central es donde va a ocurrir el problema de gradiente descendente,  la propagación hacia atrás y demás.

	Van a pasar por una serie de pasos. El primer paso, ya os lo he comentado, en la neurona se van a sumar los productos de cada peso por el valor de entrada respectivo. Esa será la primera fase a tener en cuenta, en la que la suma ponderada va a resumir toda la información en un único número. Una vez que tenemos ese resultado final, ese numerito se le aplica una función de activación. La función de activación servirá precisamente para ver si la neurona debe despertarse o no, debe activarse o no, si la entrada de datos es lo suficientemente interesante como para transmitir la señal por el axón. Y si ese valor es suficientemente alto, pues lo que hacemos es transmitir la información hacia la siguiente capa, hacia el valor de salida. Por tanto, fijaros que dependiendo de qué función de activación elijáis la neurona, podría transmitir o no la señal.La elección de la función de activación también será interesante Eso ocurrirá precisamente en la tercera etapa, donde si el valor es suficientemente elevado, la información que la neurona ha calculado será transmitida hacia la siguiente capa.

	Entonces se toman unos datos de entrada, se ponderan con unos pesos. Se aplica una función de activación para ver si la neurona se despierta o no, y en caso de despertarse transmite la información por el axón hasta la siguiente neurona.

	La gran cantidad de neuronas que coloquemos conectadas entre todas ellas harán que este algoritmo sea 
	capaz de aprender muchísimos, muchísimos rasgos.

	Operacion dentro de la neurona:
		  n
		Σ  wi*xi
		  i=1
		  
		  w: peso de la entrada
		  x: valor de la entrada
		  n: cantidad de entradas
		  
		  
	- La funcion de Activacion
	
	La funcion de activacion es la que determina si el valor resultante de la neurona tiene que pasar a la proxima capa o no.
	
	Básicamente veremos cuatro tipos diferentes de funciones de activación que tenemos a nuestra disposición y se utilizan dentro del mundo del machine learning.
	
	Por supuesto, existen más tipos de funciones de activación, pero estas cuatro que veremos son las predominantes.
	
	Las funciones son:
	
		- Funcion escalon o umbral:  
		
		R(x) = 1 if x >= 0 o 0 if x < 0
		
		Realmente se llama así porque parece realmente un escalón en un grafico de X y Y. En el eje de las X tenemos la entrada ya ponderada multiplicada por los respectivos pesos de la fase de sinapsis, por así decir, de la red neuronal, y el valor de salida que ésta devuelve se transforma a cero o uno respectivamente, dependiendo de si la ponderación da positiva o negativa. Si el resultado de la neurona es >=0 es 1 y  0 si el resultado es < 0.
		
		Se crea un salto entorno al valor cero (0). De modo que cuando se calculan los valores de entrada por los pesos, si el
		resultado de esa ponderación es negativo, entonces la función escalón lo transforma automáticamente a 0. Y en el caso de que la ponderación de un número positivo, automáticamente la función escalón lo transforma en uno.
	 
		Hay blancos y negros o se activa o no.
		
		- Funcion Sigmoide:
		
		Funcion con forma similar a la funcion logistica en regresion. 
		
		En caso de tener que devolver 0 o 1. Se puede establecer un valor de corte entorno a 0,5 (1/2), valor resultante de x=0. Todo por debajo de 0,5 tiene resultado 0 y todo por encima de 0,5 tiene resultado 1.
		
		R(x) = 1 / (1 + e ^-x)
		
		e: El número de Euler 
			∞
		e=∑	1/n!
			n=0
		
		Es una fórmula bastante interesante y que básicamente se trata de hacer esa misma división entre cero y uno, pero en este caso de forma continua (mas suave), con un poquito de pendiente y derivable, o sea, sin puntas en torno a ningún punto.
		
		Cuando X es un número muy negativo, pues esta función se acerca mucho a cero y cuando X es un valor muy grande, tiende a uno.
	
		Valores muy negativos los transforma a valores cercanos a 0 y valores muy positivos los transforma a valores cercanos a 1. Cuanto mas positivos es el resultado mas se aproxima a 1.
		
		Esto lo que hace el hecho de que está entre cero y uno es poderlo interpretar como una probabilidad. Qué tan probable es que la neurona se active.
		
		- Funcion Rectificador:
		
		R(x) = max(x, 0)
		
		La mitad de la función totalmente nula, luego va creciendo paulatinamente. los valores de x hasta 0 hace a la funcion nula luego, crece paulatinamente.
		
		Es una de las funciones más populares para redes neuronales artificiales, por el hecho de que transforma todo lo negativo directamente en cero, se carga todo lo que sea negativo. 
		
		Todo aquello que una vez ponderado por los pesos de la red neuronal da negativo, no interesa, desaparece.
		
		Y a partir de ahí todo lo positivo se conserva como tal. Por eso la función es el máximo entre el valor de la ponderación y el número cero. La funcion aumenta gradualmente con respecto al valor de entrada.
		
		- Funcion Tangente Hiperbolica (Tanh):

		Forma similar a la sigmoide.
		
		R(x) = 1 - (e^-2x) /  1 + (e^-2x) 
		
		Aquí la idea es que si la función sigmoide empezaba en 0 y acababa en uno. La función tangente hiperbólica empieza por debajo de cero. Empieza en -1 y acababa en 1.  Justo en 0 vale 0. Y a partir de ahí, los números positivos de la ponderación van creciendo hasta un máximo de uno. 
		
		Pede ser útil en algunas aplicaciones donde necesitamos valores negativos.
		
		Para mas detalle ver paper: Deep sparse rectificador neural networks. By Xavier Glorot. Alli se ve porque la función de rectificación es una función tan importante y popular.
		
		Cada una de las neuronas de la capa oculta pondera más o menos cada uno de los valores de entrada. Cada neurona puede tener diferentes funciones de activacion. Aplicaríamos esas funciones de activación respectivamente y transmitiría los valores a la señal de salida y justo ahí al final. Si nosotros queremos predecir qué tan probable es que el valor de salida pertenezca a una clase o contenga cierto valor, por ejemplo, se podría aplicar en este caso una función sigmoide.
		
		Esta combinación es bastante típica, sobre todo cuando en las capas ocultas vamos aplicando rectificadores para eliminar lo que no nos interesa en cada una de las neuronas centrales.Y luego, emitimos a partir de allí la predicción de la capa oculta y en el nodo final, el valor de salida es transformado a una probabilidad a través de utilizar una función sigmoide. Esto es super tipico.
		
		
	- Como funcionan las redes neuronales?
		
		Veremos un ejemplo real de cómo se puede aplicar una red neuronal a un ejemplo clásico del mundo de las compras y ventas de casas. La tasación de casas. Tenemos un problema de evaluar propiedades. Tenemos una red neuronal que quiero que tenga en cuenta algunos campos, algunos parámetros de nuestra vivienda y el valor de venta de la misma. Usaremos un enfoque teorico. Entrenar la red es importante. Pero previamente, antes de entrenar a una red neuronal tengo que saber cómo montarla, qué estructura le tengo que dar, cuántos nodos de entrada, capas ocultas, funciones de activación, etc. Luego, se entrena y obtiene la prediccion final. 
		
		Bueno, pues digamos que para ello voy a necesitar una serie de parámetros de entrada. Podría ser, por ejemplo, el área en pies cuadrados de la propiedad, el número de dormitorios que tiene la propiedad, la distancia a la ciudad, suponemos que está en medio del campo o no tiene qué estar en el centro de la ciudad y la edad de la vivienda. Son cuatro variables que podrian conformar nuestra capa de entrada. Probablemente podrian haber mas parametros. Pero usaremos estos por simplicidad. Bueno, es una forma muy básica, pero la red neuronal tendría una capa de entrada con esta información y va a intentar aprender para devolvernos, para descubrirnos una capa de salida Para intentar sacar el valor de la casa que estoy intentando pronosticar.
		
		Si no hubiera ningún tipo de capa oculta, lo único que podría hacer era ponderar los datos de entrada con una serie de pesos W1 w2 w3 y w4, lo que hemos llamado sinapsis y se calcularía el resultado. De hecho, esto, el precio que calcularía sería básicamente esta ecuación: El precio final sería w uno por x1 más w dos por x dos w3 por x tres más w cuatro por x cuatro que no es ni más ni menos que una regresión lineal múltiple.
			
			precio final = w1 * x1 + w2 * x2 + w3 * x3 + w4 * x4

							xi: peso
							xi: valor de entrada

		No hay funcion de activacion. Si aplicáramos alguna función de activación, por ejemplo la logística, podríamos sacar una probabilidad, porque sería un número entre cero y uno. Podriamos utilizar cualquier otra funcion de activacion.
		
		La gran mayoría de algoritmos que os he enseñado durante este curso de machine learning se pueden generalizar a algoritmos de redes neuronales, de modo que la regresión lineal simple se puede ver como una red neuronal. La lineal múltiple se puede ver como una red neuronal. La logística se puede ver como una red neuronal. Y básicamente todos los algoritmos de predicción y clasificación que os he enseñado durante el curso se pueden ver como redes neuronales. La red neuronal es muy potente. Todos los algoritmos de machine learning que existen se pueden representar como neuronas operando entre ellas hasta que el nodo final devuelve o bien una categoría o bien un valor de predicción. Si nohay capa oculta no estamos en un problema de deep learning, se puede ver como un problema clasico de machine learning. La ventaja que tenemos y la flexibilidad, por supuesto, es poder colocar entre medias capas ocultas que se encarguen de aumentar la precisión de nuestra predicción. Entonces, alargaremos con una capa oculta intermedia, donde aparecerán una serie de neuronas entre medias. Y el poder de esta capa oculta es que cada una de esas neuronas podrá ver los datos a su modo, podrá interpretar los datos a su modo. Entonces, para ello vamos a hacer un ejemplo y suponer que realmente se ha entrenado esa red neuronal. Se supone que vamos a suministrar una serie de viviendas a la red neuronal y ésta va a ir aprendiendo los pesos que deben ocupar cada una de las uniones entre la capa de entrada de la capa oculta y lo mismo con la capa oculta y la capa de salida.

		Después de un par de iteraciones de la red neuronal, conectamos los cuatro datos de entrada, las cuatro variables de entrada con la primera neurona. Y aquí empieza a juntarse la información porque recuerda que sólo podemos tener un número cada vez. Como ya os enseñé anteriormente, la sinapsis es la fase de transmisión de la información desde una
		capa hasta la siguiente. En este sistema habrá una serie de pesos y según esos pesos, sean o no sean cero, las variables aportarán más o menos a cada neurona. Vamos a suponer que de las cuatro variables de entrada hay dos con pesos no nulos. Es decir que la primer neurona le importa dos variables. Sopongamos el area de la propiedad y la distancia. No le importa los dormitorios o cualquier otra variable (cant. dormitorios o edad). De acuerdo, entonces lo único que le importa a la primera neurona es saber cuál es el área, cuál es la distancia a la ciudad. Son los únicos 2 valores de sinapsis que llega a la primera neurona con valores no nulos. Pues esta neurona básicamente lo que hace es evaluar cómo de lejos está la ciudad. Por ejemplo, cuanto más lejos está de la ciudad, más barata se vuelve la propiedad.
		Y en términos de área, pues la idea es que cuanto más grande la propiedad, más cara es entonces el precio. Esta neurona lo entenderá como un factor proporcional al área e inversamente proporcional, por ejemplo, o con signo negativo en lo que es la distancia a la ciudad. De modo que esta neurona buscará casas grandes y lejos de la ciudad como forma de entender. La neurona será capaz de tasar las casas grandes que están lejos de la ciudad. Detecta una casa grande y lejos de la ciudad. Esta casa yo la se predecir, yo la se tasar y le voy a dar el resultado final a la capa de salida.
		Esta neurona se especializa en esto y por tanto la función de activación que dispara a esta neurona sólo se activará cuando se cumplan ciertos criterios con respecto a la distancia y al área de la casa. Sólo será una neurona activa que sólo hara una sinapsis si realmente los valores de esas dos variables de entrada son los adecuados. No le importa la edad o los domitorios. El poder de la red neuronal es que cada neurona entenderá los datos a su modo. 
		Bueno, pues la primera neurona es capaz de centrarse en el área y la distancia a la ciudad. Veamos la segunda, le interesa el área, el número de dormitorios y la edad. Por qué motivo podría interesarse por estas tres variables? Que ha encontrado que le permita jugar con estos datos? Bueno, recuerda que la red neuronal se entrena con todo el conjunto de datos que ya están escritos y se suministran a la red neuronal para que vaya aprendiendo. Y a través de pasar por todos los cientos de miles de ejemplos, cientos de miles de casas ha descubierto que combinando las variables de área, dormitorio y edad consigue algo interesante. El entrenamiento lo que hace es refinar el concepto que la red neuronal obtiene acerca de los datos de entrada. No le importa si esta cerca o lejos de la ciudad. Tan pronto como se cumplen los criterios de un número determinado de habitaciones, área y edad de la casa, se activa como neurona y le suministra el feedback a la capa de salida. Eesto aporta algo de información adicional a la capa de salida y permite que esas tres variables se combinen en un único atributo. Vemos la ultima neurona, sole le interesa la edad. Por que? El número de dormitorios, area o dónde esté ubicado le da bastante igual. Eso significa que es una casa antigua. Las casas antiguas normalmente se valoran por menos porque están más desgastadas. El valor de venta es menor o, puede tener valor historico y aumenta su valor. Esta neurona se especializó en esto, en ver las propiedades sólo en base a la antigüedad, de modo que casas con menos de 100 años, pues diría cuanto más vieja es y menos lujos tiene vamos a devaluarla pero, una casa histórica de mas de 100 años, incrementamos el precio hasta el punto de ser terriblemente cara una casa de hace 300 años, una casa histórica. En este ultimo caso, la funcion de activacion puede ser un rectificador  que cobraria sentido. Hasta cierto punto las casas no active esta neurona el componente de casa histórica y que de cierto punto en adelante empecemos a valorar la componente histórica del hogar de la casa. Entonces, hasta cierto punto, la neurona no se activa, por ejemplo hasta los 100 años, y después de los 100 años en adelante, cuanto más viejo se vuelve el edificio, más aumenta el valor total de la casa gracias a esta neurona. Entonces, fijaros que esto sería la idea global que la neurona habría ido recogiendo de todos los inputs, de todos los datos de entrada. A mayor neuronas mas flexibilidad, cada una de ellas aporta o quita al precio global de predicion. Cada una de las neuronas entiende la información a su propia manera. Se activan o no, respectivamente? Tiene más o menos pesos, más o menos parámetros.
		Y la respuesta final de la predicción una vez que hayamos mirado 1 millón de anuncios de valores de casas, podremos llevar a cabo una predicción final, ponderando los resultados de todas las neuronas intermedias. Por separado las neuronas no tendrian mucho poder pero en conjunto aumentan su poder. En la capa de salida todo se junta y devuelve un valor de prediccion final. 
		

	- Como aprenden las redes neuronales?
	
		Qué es lo que hace que un programa o una red neuronal entienda?
		
		Bueno, existen dos enfoques diferentes para lograr que un programa haga lo que vosotros queréis. Uno es programarlo directamente al programa. Se establecen una serie de reglas específicas que el programa sigue, ejecuta y que a lo largo de todo el camino es capaz de explotar. De modo que dentro del código, al final tenéis todas las opciones posibles con las que el programa tiene que lidiar.
		
		Y por otro lado, está el enfoque de las redes neuronales, que lo que hace es como instalar un programa limpio de cero, capaz de tomar decisiones. Y en base a esas decisiones, el programa es como si se fuera rellenando a base de proporcionarle entradas. El propio algoritmo es capaz de tomar valores en ciertos pesos y tomar las decisiones finales por sí mismo.
		
		Son dos enfoques totalmente diferentes. Nuestro objetivo es lograr. El segundo caso es que la red llegue a aprender por sí misma, no que yo le tenga que escribir las reglas  o las líneas de código, una por una.
		
		Por ejemplo, cómo distingue una red neuronal el pelo de un perro o de un gato en el lado de la izquierda? Se le programará. Busca las orejas. Tiene que ser de esta forma. En el caso de un gato busca bigotes, el tipo de nariz del perro es de esta forma. Las orejas suelen ir más caídas. Busca la forma de la cara. Fíjate en estos colores y demás.
		
		En el caso de la red neuronal no se le darían esas indicaciones al algoritmo sino que tendríamos una serie de carpetas con un montón de imágenes de perros y gatos correctamente etiquetadas y se las iría enseñando a la red neuronal. Dirías tú esto es un perro, esto es un gato, esto es un perro, esto es un perro, esto es un gato. Nosotros lo único que haríamos sería crear la arquitectura. Haríamos que la red neuronal tomara los datos de entrada. Típicamente, las imágenes que os acabo de decir y la red neuronal entendería por sí misma qué es lo que necesita buscar y qué es lo que necesita entender. De modo que después de haber pasado por todas las imágenes, cuando la entrenáis con una nueva imagen de un perro, de un gato, podrá entender de que se trata la propia red neuronal. Buscará las orejas, o buscarán los ojos, o buscará la nariz, o buscará lo que quiera buscar dentro de la imagen. Pero no se lo he indicado yo. Es la diferencia entre el enfoque clásico de programación y el enfoque de redes neuronales.
		
		Queremos ver que significa el concepto de aprender. Segundo enfoque.
		
		Partiríamos de una red neuronal muy básica, una sola capa con la red neuronal directa, se llama el Perception. Una red neuronal con una sola neurona es un perceptor. Entonces, antes de proceder, una cosa que necesitamos ajustar siempre es ese valor de salida que tenemos a la derecha. De modo que si el valor real será una y que yo no conozco, pero de momento hay que cambiarla por una y gorrito, que es el valor que va a generar la red neuronal. Por eso normalmente solemos separar del valor de salida de lo que llamamos el valor real o el valor actual(la y sin gorrito). Entonces, el valor de salida (y gorrito) tiene que parecerse  al valor actual, y a la hora de parecerse, es cuanto somos capaces de cuantificar qué tan lejos estamos de ese valor objetivo, del valor actual.
		
		El patrón se inventó por primera vez en 1957 por este señor que tenéis aquí, Franz Rosenblat. Y básicamente dedicó toda su vida a crear algo, una entidad, un objeto que fuera capaz de aprender y ajustarse para corregir sus propios errores.
		
		En base al perceptron dibujado, vemos cómo las ideas de Rosenfeld se aplicaron para medir la diferencia entre el valor actual y el valor de salida de la red neuronal salida de la red neuronal y que la propia neurona pueda corregirse a posteriori.
		
		Orden de ejecucion de la red neuronal.
		
		Lo primero es que los valores de entrada se suministran al perceptron. Básicamente, se multiplican esos valores con los pesos de la red neuronal. A continuación, se aplica una función de activación que me devuelve ese valor de salida. Esto nos dará un resultado (y gorrito) y lo que vamos a hacer es tratar de trazar un gráfico con ese resultado. Ahora lo que tengo que hacer es aprender y corregir lo que he devuelto mal, lo que la red neuronal no me ha podido devolver como debería. Es decir, qué tan lejos del valor real esta de mi prediccion. El valor real lo conozco y lo pinto junto al valor predecido y veo su diferencia. Esa diferencia la podemos cuantificar y se llama función de costes. La función de coste se define como la mitad de la diferencia al cuadrado entre el valor real y el valor de salida de la red neuronal.
		   

						  ^
			Coste (C) = ( y - y) ^ 2)/2
			
			Hay varias funciones de coste. La anterior es la mas comun. La funcion tiene que ser convexa. Que cuando se dibuje parezca una parabola. 
			
		El objetivo, pues, será intentar minimizar esta función de costes, porque cuanto menor sea la función de coste significa que más cerca está la predicción del valor actual. (y gorrito se acerca a y). Por ahora hay una neurona. Esto aumente con la gran cantidad de informacion. Básicamente, aquí lo que podéis hacer es pintar justo al lado el valor de la función de coste. Y a partir de aquí el objetivo intentar corregir donde me he equivocado. Para ello vamos a transmitir hacia atrás cuál es el error que he cometido. De modo que del valor actual, voy a reenviar hacia atrás la cantidad de error que me genera la función de costes. Esta es la información que se vuelve a introducir dentro de la red neuronal y está propagando hacia atrás, hacia el cuerpo de la neurona y a partir de ahí, hacia los pesos de la misma.
		
		Recordemos que la red neuronal estima los pesos. Por tanto, básicamente el único control que tenemos en la red neuronal es aumentar o disminuir cada uno de los pesos de las variables de entrada. Entonces, todo este camino hacia atrás es precisamente para intentar modificar un poquito los pesos y asi, minimizar los errores. 
		
		Ejemplo, en base a las horas de estudio, horas de sueño y la nota del test de mitad de examen, intento predecir la puntuación que obtendré a final del trimestre. En este caso sería un 9,3 o 93%, así que lo que tendríamos que hacer es alimentar estos tres valores a la red neuronal e intentar comparar el resultado que me devuelve la red neuronal con respecto a el resultado real.
		
		A medida que suministramos una y otra vez la misma informacion se va ajustamdo la diferencia en la prediccion, siguiendo el proceso anterior. Evidentemente nunca llegaremos a tener una función de costes igual a cero. Pero la idea es que en un momento dado, la predicción se acercará cuanto más sea posible, al valor real.
		
		Ajustar los pesos es lo único que sabe hacer la red neuronal. El objetivo es tratar de minimizar esa función de costes.
		
		Hasta ahora, he lidiado con una sola neurona, incluso con una sola línea. Sólo hay un estudiante del cual yo conozco sus datos. 
		
		Qué pasa cuando tengo muchos estudiantes? Pues evidentemente tengo las horas de estudio de los ocho estudiantes, las horas de sueño de los ocho estudiantes, la nota que sacaron, el parcial de los ocho estudiantes y su resultado final en el examen a final de trimestre.  Tengo ocho interpretaciones de la misma red neuronal, todas tienen el mismo parámetro. La neurona es única. No hay una neurona para cada individuo, simplemente la red neuronal trabaja en conjunto. Lo que hace es ajustar los pesos de la misma con respecto a toda la información. Bueno, realmente lo único que hay que hacer es pasarle todo el conjunto de datos a la red neuronal que ésta sea capaz de entrenar y ir corrigiendo, respectivamente.
		
		La red neuronal, con cada uno de los datos actua con los mismos pesos, siempre me da las predicciones respectivas. Una vez que les suministrado todo el dataset y con esos mismos pesos se ha hecho la predicción para cada uno de los individuos, se compara con los valores reales y se calcula la funcion de coste para cada prediccion y se suman cada uno de los resultados.

						  ^
			Coste (C) = Σ ( yi - yi) ^ 2)/2 = Coste Global.

		Luego, ajustamos los pesos con esta función de costes C y decidimos modificar los pesos da cada variable de entrada (W1, W2, W3) y así sucesivamente.
		
		Es importante, que los pesos se actualizan una vez que la red neuronal conoce acerca de todas las percepciones acerca de todos los individuos que hay. Se usa una funcion de coste global para actualizar los pesos. La funcion de coste global es la suma de las diferencias observadas con el valor real.
		
		Entonces, una vez que hemos ajustado los pesos acorde a esto, volvemos a empezar. Volvemos a ejecutar todo esto de nuevo y volvemos a suministrar todas las ocho filas de nuevo a la red neuronal y asi sucesivamente. El Objetivo es minimizar la funcion de coste.  Y tan pronto como tengamos un valor lo suficientemente pequeño, diremos que la red neuronal ha convergido o que tenemos un resultado correcto.
		
		Lo más normal es que la función de costes de una iteración a la siguiente vaya bajando de modo que los pesos se ajusten correctamente. Y cuando sea lo suficientemente pequeño, tendremos los pesos óptimos y la red neuronal estará listo para pasar a la fase de prueba y de testear con nuevos usuarios. Ver CrossValidated . Lista de funciones de coste usadas en la red neuronal.
		
		
	- El gradiente descendiente
	
		Veremos como se ajustan los pesos. Se usa la tecnica de gradiente descendente.  Y la idea es que yo quiero encontrar el mínimo de esa función de costes. 
		
		Entonces la mejor opción es colocarnos en algún lugar de esa función de coste. Evaluamos con una primera pasada los números y lo que vamos a hacer es medir el ángulo de inclinación de la función de costes justo en este punto. De modo que si yo detecto que tiene ese ángulo que véis ahí, eso es lo que se llama el gradiente, es la tangente, básicamente la dirección de máxima variación, de máxima caída y que por supuesto, hay que derivar para calcular un gradiente. El gradiente es una derivada. Se necesita derivar para averiguar cuál es la pendiente de la curva justo en ese punto y averiguar si la curva va para arriba o para abajo, si la pendiente es positiva o negativa. Para ello simplemente se traza una recta horizontal y se mide el ángulo de inclinación. Si la pendiente es negativa, como en nuestro caso, lo que ocurre es que vamos cuesta abajo y que para que para minimizar la función hay que ir hacia la derecha. En caso contrario hay que ir hacia la izquierda, por supuesto, en caso de que la pendiente sea positiva. Si hay que ir hacia la derecha, lo que significa es que yo ahora puedo moverme cierta cantidad para irme cuesta abajo, decido irme para allá y cuando lo hago la cosa cambia. De acuerdo, la siguiente iteración del algoritmo me da una función de coste menor que la anterior y puedo repetir el algoritmo. Puedo repetir el algoritmo. Le calculo la pendiente En este caso notamos que la pendiente es positiva. Si la escribe de izquierda a derecha va para arriba, lo cual significa que para minimizar la función de costes tengo que irme a la izquierda, de modo que elijo irme hacia la izquierda y la cosa sigue bajando. Evidentemente, mientras vamos repitiendo esta técnica, es de esperar que la función de costes será cada vez menor y que en un momento dado no bajará más. O lo que es lo mismo, el algoritmo habrá convergido hacia el valor final.
		
		Cada una de las bolas que tenéis sería una posible iteración de la fase de aprendizaje de la red neuronal.  La idea es que la bola no tiene que subir en ningún momento. Por supuesto que hay otros muchos más enfoques que no sólo este. En funcion a esta tecnica aprendemos cuales parametros ajustar y cuales son los pesos que deben cambiar de una fase a la siguiente. De todas las direcciones que pueden tomar los pesos, vamos a elegir aquellos que son capaces de forzar el descenso de la función de costes. Busco en que direccion ir para bajar de la montaña. Hacia que direccion esta la bajada. Esto hace el gradiente descendente. 
		
	- El gradiente estocastico.

		La palabra estocástico tiene que ver con la palabra aleatorio probabilístico estadístico.
		
		El gradiente descendente es un metod muy eficaz	 para minimizar la funcion de coste y corregir los errores de la red neuronal. Sin embargo, se puede mejorarlo. El gradiente descendente requiere una funcion convexa. 
		
		Bueno, pues el gradiente ascendente estocástico en esencia va a ser exactamente lo mismo con la pequeña diferencia. El gradiente descendente tiende a buscar el mínimo global, el que está más abajo de todos, si es que existe, lo encuentra.
		
		Que pasa si la funcion no es convexa o es mas irregular? Podemos elegir una funcion de coste que no sea convexa y al aplicar el gradiente descendente obtendriamos una red neuronal no optima. En ese caso hay que usar el gradiente descendente estocastico. Cuando el gradiente es estocástico no requiere, no tiene como premisa que la función de costes sea convexa.
		
		Entonces echemos un vistazo a las dos diferencias entre el descenso descendente normal y el gradiente descendente estocástico. En el gradiente normal, hemos tomado todos los datos de nuestro DataSet, todas las filas, y se los hemos suministrado a la red neuronal. Luego, calculado las predicciones para cada una de las filas de nuestro dataset, se calculan las diferencias (con la funcion de coste) y finalmente, se suman todas esas diferencias para obtener el resultado final de coste. Con este resultado se corrigen los pesos de las entradas. Corregidos los pesos se vuelve a pasar los datos a nuestra red neuronal. 
		
		En el gradiente descendente estocastico, vamos a tomar las filas una por una y se las paso a la red neuronal(se la voy a pasar a algoritmo). Le paso la primer fila a la red, me va a dar el error que cometo y voy a ajustar automáticamente los pesos por cada fila enviada a la red neural. Luego, paso la segunda fila y asi sucesivamente. básicamente, lo que estoy volviendo a hacer es ajustar en este caso los pesos después de que cualquier fila ha sido suministrada a la red neuronal. 
		Gradiente descendiente por bloque de datos. Gradiente descendiente estocastico por fila de datos. Gradiente descendiente estocastico mas costoso en calculos.
		
		Básicamente el gradiente ascendente estocástico ajusta los pesos cada vez que observa a un individuo y calcula su predicción. Mientras que el gradiente ascendente por bloques solo hace una corrección global de pesos una vez que se le ha suministrado toda la información. Gradiente descendiente evita de no converger al minimo global. Y es aconsejable en funciones no convexas. El coste es que es mucho mas lento por los calculos en cada iteracion de individuo observado.
		
		Ni uno ni otro se lleva el premio a los mejores algoritmos,uno por falta de convergencia en funcion de coste no convexa y el otro por exceso de coste computacional.  Entonces han surgido ideas como simplemente fusionarlo. Entonces, cómo se fusionan estos dos algoritmos? Tomando lo mejor de cada uno de ellos. Esto hace que si los combinamos, básicamente nos quedaremos con lo mejor de cada uno de ellos. Empaquetamos una serie de lotes, tal vez cinco usuarios de cinco en cinco días, de 100 en 100. La idea no muchas filas. Se pasan todas esas filas (bloque) a la red neuronal y se actualizan en los pesos en funcion de ese bloque de filas. Luego se saca otro pequeño bloque, se suministra la red neuronal y se actualizan los pesos (Gradiente descendente en mini bloques) Ver: 
				 - A neural network in 13 lines of python. Andrew trask. 2015 . Ver parte 2 - Alli se habla de gradiente descendente.
				 - Neural networks and deep learning. Michael Nielsen . 2015 . capitulo 2. Lectura pesada.


   - Propagacion hacia atras.
		
		Desde el momento en que calculamos los errores, y esos errores se transmiten hacia atrás para corregir los pesos. Estamos hablando de una propagación hacia atrás (back propagation). Entonces, esa es la parte del algoritmo que nos permite corregir nuestros errores y está sustentado, por unas matemáticas muy interesantes y sofisticadas que nos permite decidir el ajuste global de los pesos dentro de una red neuronal. Entonces, la clave es recordar que la propagación hacia atrás es una de las últimas fases de cada una de las iteraciones de la red neuronal, de modo que todos los pesos de la red neuronal se ajustan simultáneamente. L red neuronal se responsabiliza de corregir los errores causados por los pesos de la misma. Se ajusta todo a la vez, una vez que toda la información ha pasado por la red neuronal. Se podría corregir al propio paso, pero se decide dejarlo en la ultima etapa. 
		
		http://neuralnetworksanddeeplearning.com/ ver capitulo 2. 
		
		La propagación hacia atrás es corregir todos y cada uno de los pesos para ajustarlos a la vez e intentar minimizar el error.
		
				
	- Entrenar la red neuronal con el gradiente descendiente estocastico:
		
			Paso 1. 
				De inicializa los pesos aleatoriamente con valores pequeños cercanos a cero, no nulos, para poder empezar el proceso de ponderación de las variables de entrada. El propio proceso de propagación hacia atrás será capaz de ajustar estos pesos a los valores correctos.
				
			Paso 2. 
				A continuación se introduce la primera observación del dataset en la capa de entrada, de modo que cada una de las características, cada una de las variables independientes, es uno de los nodos de entrada.
				
			Paso 3.
				El paso número tres es la propagación hacia adelante. Las neuronas se activan. La activacion de cada una de ellas se limita por los pesos y la funcion de activacion elegida. La idea es que toda la información va pasando de una capa a la siguiente hasta obtener la predicción. 
				
			Paso 4. 
				Compara el valor de prediccion con el valor real. Se mide el error generado. 
				
			Paso 5.
				Propagacion hacia atras, propagando el error hacia atras y actualizando los pesos en funcion de cuan responsables del error son cada una de ellas.  Propagamos el error y actuamos para actualizar, cambiar los pesos que realmente sean los responsables del error cometido. El ratio de aprendizaje que gobierna cuanto deben actualizarse los pesos. 
				
			Paso 6.
			
				Si los pasos 1 a 5 los hacemos observacion por observacion (reinforcement learning)
			
				Si los pasos 1 a 5 los hacemos por bloque de observaciones (Batch learning)
			
			Paso 7.
				
				Una vez que todo el dataset sea suministrado a la red neuronal, se completa lo que llamamos un EPOCH.  A partir de aquí podemos repetir todo el proceso y hacer todos los EPOCs quequeráis. Cuanto más queréis entrenar, mucho mejor, por supuesto, hasta ajustarnos correctamente al cómputo final del error en base a estos pasos.
				
		Estos son los siete pasos que tenéis que seguir para construir una red neuronal artificial.
	
	
	- RNA en Python:
	
		Implementaremos RNA porque tenemos muchas variables independientes. Tí tenemos mucha información, algunas variables son categorías, otros son números, otros son booleanos.
		
		
		Vamos a construir un artefacto adecuado y decidir qué variables y qué factores influyen en el resultado final, en este caso, de abandonar o no el banco, que es una variable dependiente binaria.
		
		
		(1) Instalar Keras. Instalamos Keras, ya que el paquete de Keras instala Tensorflow.
		
			Usuarios de Linux y Max.
			
				sudo chown -R nelson /usr/local/software/anaconda3
				
				conda install -c conda-forge keras
				
				
			Usuarios de Windows.
			
				conda install -c conda-forge keras
		
		
		Las redes neuronales artificiales pueden hacer un excelente trabajo a la hora de hacer tanto predicciones como clasificaciones. Luego, a continuación, veremos otra rama que tiene que ver con las redes neuronales de convolución y que funciona mucho mejor para tareas de visión artificial, visión por computador y que nos permitirán jugar en ese caso con imágenes. 
		
		Vamos a construir nuestra primera red neuronal artificial para resolver el problema comercial.
		
		Vamos a usar python y a instalar las librerias necesarias. Son tres librerías, a pesar de que sólo veis dos fases de instalación aquí. Las librerias no son desarrolladas por python son de terceros. Muchas de ellas son optimizadas para el cálculo numérico, para ser ejecutados no solo en CPU, sino también en GPU. Las librerías permiten la opción si el PC está adaptado a ello, de no sólo utilizar la CPU, sino también la tarjeta gráfica a la hora de hacer el entrenamiento de la red neuronal en términos de potencia o en términos de eficiencia, pues la GPU es mucho más potente.
		
		Así que básicamente, en nuestro caso, la red neuronal es una forma de tener optimizado al máximo posible un proceso de clasificación o de predicción muy complejo, intentando que la caja negra de la red neuronal es quien vaya haciendo la tarea por nosotros.
		
		# Instalar Theano
		# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git o pip install Theano
		# Theano is a Python library that allows you to define, optimize, and efficiently evaluate mathematical expressions 
		# involving multi-dimensional array

		# Instalar Tensorflow y Keras - tensorflow.org - 
		# conda install -c conda-forge keras
		# Es importante saber para que versión de Python estas instalando TensorFlow.
		# Cuando instalo Keras automáticamente se descarga la propia versión de tensor flow necesaria para correr Keras.
		
		TensorFlow. Es una librería de fuente totalmente abierta que ejecuta en redes neuronales o permite crear redes neuronales
		de forma realmente sencilla.
		
		De modo que estas dos librerías básicamente se utilizan con fines de investigación y desarrollo y lo más importante, se han podido exportar para utilizarlos nosotros desde casa en el mundo del machine learning y poder construir nuestras redes neuronales profundas de forma automática. Las podemos utilizar como una especie de wrappers, como una especie de funciones de alto nivel a la que simplemente le voy a indicar, añádele una capa oculta o añade estafunción de activación rectificador lineal unitaria. Por último, Keras es una capa por encima de tensor flow, mantiene el espíritu de Tensor Flow y utiliza tensor flow por detrás, pero realmente hace que todavía a la hora de crear redes neuronales tengamos que bajar menos al nivel de capa por capa, de modo que nuestras redes neuronales todavía tendrán menos líneas de código gracias a Keras. Keras es una biblioteca basada en tensor flow que pudiera hacer lo mismo de tensor flow, pero accesible a más gente que no tuviera los conocimientos técnicos como para crear las capas una por una, o definir los nodos o los tamaños a mano, por así decir.
		
		Con Keras construiremos esos modelos de aprendizaje profundo de forma muy eficiente y utilizará Tensor flow y Theano sin ningún problema.
		
		Elaboremos la red neuronal para saber si todas las variables independientes tienen algún impacto, incluso si algunas son totalmente irrelevantes con respecto a la prediccion. La red neuronal artificial hara esto. Ella será capaz de detectar y ver perfectamente qué correlaciones existen entre las variables independientes y la variable dependiente. Por supuesto, la red neuronal dará pesos más grandes a aquellas variables que tengan mayor impacto dentro de la variable que quiero predecir.
		
		En el ejemplo me voy a sacar las tres primeras variables, voy a quedarme desde el Credit Score hasta la estimación salario y luego, la variable "exited" será la última,la que predice el modelo. Elimino las primeras tres porque considero que no tienen impacto en la prediccion. Luego ANN realizara el analisis para saber que peso otorgar a cada variable y si juega o no.
		
		Tenemos algunas variables categóricas dentro de nuestro dataset. Las variables categóricas no vienen representadas por un número, sino por una palabra o un string. Entonces necesitan ser codificadas. Recordar ver el pre-prosecado de datos para escalar y/o transformar variables categoricas a numeros. No hay un orden entre variables categoricas. No significa que Francia vaya después que España o antes de Alemania. Creamos variables dummy que separen las categorias en 1 o 0 respectivamente. Columnas dummy para cada valor  de las variables categoricas. Por ejemplo, se crean columnas para Francia, Germania, España y en funcion a su valor en la fila se asigna un 1 o 0 respectivamente al pais. Recorda evitar la trampa de las variables ficticias (dummy), no caigáis en esa trampa. Recordad que hay que colocar una columna para cada una de las categorías, salvo para una de ellas, para evitar la multicolinealidad. Se elimina una de las columnas dummy. 

		Es interesante que la ANN tenga muchos elementos para entrenar
		
		conda create --name ann
		
		conda activate ann

		conda install spyder
		conda install jupyter
		
		conda install <package>
		// package could be matplotlib, pandas, etc.	
		
		conda install conda-forge::tensorflow
		
		conda deactivate
		
		Luego, analizar si hace falta escalar las variable para ANN. En ANN es necesaria y obligatoria. Sino habra mucha confusion. Es sumamente necesario escalar por las operaciones matematicas que se realizan. Se agiliza muchísimo el resultado y hace que sea muchísimo más preciso. Si se toman todas las variables independientes y hacemos que ninguna domine sobre el resto.
		
		Y existen dos formas de inicializar una red neuronal. La primera es definir la secuencia de capas, o bien definir el grafo de cómo se van a relacionar entre sí las capas. Vamos a crear una red neuronal artificial con capas sucesivas.
		
		La propia red neuronal actuará con el papel de clasificador porque estamos en un problema de clasificación. Entonces, el modelo de red neuronal va a clasificar. 
		
		Vamos a agregar las capas una a una en forma secuencial.
		
		La cantidad de nodos de la capa de entrada coincide con la cantidad de variables independientes a analizar luego del escalado de variables y la conversion de variables categoricas a dummy. En el ejemplo son 11.
		
		VER: Entrenar la red neuronal con el gradiente descendiente estocastico.
		
		Recordad que tendremos que establecer el flujo de izquierda a derecha e incluso las funciones de activación que se encarguen de dar mayor o menor relevancia a la transmisión de la información, capa tras capa, de izquierda a derecha de nuestra red neuronal. Recordad que, en particular, la función de activación permite que la información que no tiene suficiente relevancia no pase adelante en función de cual elijáis y que la Los valores más grandes, los que cobran más relevancia dentro de la red neuronal, son los que se transmiten de una capa a la siguiente.
		Evidentemente podríamos hacer diferentes pruebas con diferentes funciones de activación y observar si el resultado nos gusta o no la función Sigma idea realmente es bastante interesante en este sentido, porque suaviza, nos da una probabilidad que cuadraría bastante bien con lo que quiero yo. Qué tan probable es que un cliente deje el banco?
		Y si quiero ser más estricto, pues la función escalón directamente me devolverá: Deja o no deja el banco. Ya dije que cuando hicimos el ejercicio era una forma bastante interesante de decidir si la variable es binaria cero uno utilizar la función escalón o bien la función sigma respectivamente. Si quiero dar directamente la categoría a la que pertenece, al cliente o al banco, o que tan probable es que deje el banco si lo quiero interpretar como una función de probabilidad, si lo quiero como función de probabilidad, puedo utilizar la función Sigma, de modo que cuanto más cercano a uno sea, más probable es que deje el banco. Y si queréis hacer una clasificación binaria utilizando la probabilidad misma, establece un punto de corte a partir del cual el cliente es más probable que abandone, que se quede o viceversa, para así estudiar los los clientes.
		
		Yo creo que el rectificador lineal unitario es bastante interesante para activar o no las capas intermedias. Si la información que llega desde la capa de entrada es suficientemente interesante, esa neurona central se activa si la información es relevante, sino esa neurona permanece dormida. En la capa de final, creo que la función sigmoide es util para devolver la probabilidad de que un cliente abandone el banco. 
		
		Fijaos que luego el paso cuatro es comparar la predicción con el resultado real. Evidentemente, la red neuronal devolverá una probabilidad o una categoría cero uno. Lo voy a contrastar con mi valor de verdad y voy a medir el error generado en base a ese error. Luego tendré que propagar el error hacia atrás en este caso, ya sea de derecha a izquierda y gracias a la gradiente descendente, corregir los pesos en base a intentar minimizar los errores. Recordad que había varias maneras de actualizar esos pesos y la manera en la que se defina la forma de actualizar los condicionara. Con respecto a si utilizas gradiente descendente o gradiente descendente estocástico respectivamente.
		
		
		Y finalmente, una vez hayamos finalizado en el paso número seis, se repite el paso 1, 2, 3, 4 y 5 para cada observación o cada lote de observaciones (cada diez observaciones, cada 15 observaciones o cada 30 observaciones).
		
		Y finalmente, en el paso número siete, una vez se ha completado toda una iteración sobre todo el conjunto, lo que hacemos es volver a entrenar haciendo otro EPOC, y otro. Asi sucesivamente. Cada EPOC es una iteracion completa. Por ejemplo, todos los datos del set de datos.
		
		Al final lo que hacemos es pasarle la misma información a la red neuronal una y otra vez hasta completar todas las iteraciones que nos interese.
		
		Para añadir una capa usamos "Dense". Dense tiene muchos más parámetros. En primer lugar, esta Units. Units es simplemente el número de nodos que queréis agregar a la capa oculta. Dense será la zona de la sinapsis. La zona que tendrá que especificar el tamaño de entrada y el tamaño de salida. Lo que hace precisamente la función Dense no es agregar la capa en sí, sino es la conexión entre capas, por así decir, la zona de las sinapsis, o incluso preparar los datos para la capa oculta que vendrá a continuación.
		
		En particular, el número de unidades son cuántos nodos va a tener la primera capa oculta. Cuántos nodos tiene que tener la capa oculta? Porque la capa de entrada que tiene 11 son las 11 variables independientes de mi dataset. Pero y la capa oculta? Cuántos nodos tiene que tener? Cinco o 20? Un consejo es: Esta fase se trata de experimentar. No es teoria. Una que utilizamos los datos hoy en día bastante correcta o bastante asentada en estos últimos años.
		Se puede experimentar para analizar los resultados. Una regla que se usa bastante, no esta definida como principio sino es basada la experiencia, es colocar la media entre los nodos de la capa de entrada y los nodos de la capa de salida. En particular en la capa de entrada tengo 11 nodos y en la de salida un nodo(Se queda o no). Entonces la media es 6 nodos para cada capa oculta. Aunque el numero puede cambiar en funcion de pruebas y el rendimiento de la red neuronal. El siguiente de los parametros es elegir como inicializar esos esos pesos. La asignacion de los pesos debe ser aleatoria. El parametro kernel_initializer me permite elegir la destribucion para inicializar los pesos en el algoritmo.  Con una distribución uniforme nos aseguramos de que los pesos en primer lugar son pequeños y en segundo son cercanos a cero. Elegimos "uniform". Luego elegimos la funcion de activacion, paramnetro "activation". Ya os he dicho que en la capa oculta quería utilizar el rectificador lineal unitario, por ejemplo, pero podríamos también aplicar la sigmoide o lo que fuera. Seleccionamos "relu" rectificador lineal unitario. Finalmente, los nodos de entrada (o capa actual) que son 11 con el parametro imput_dim. 
		
			Nota: existe imput_shape para indicar la capa de entrada. Con la única diferencia de que es un tensor  que tenéis que especificar el tamaño de la muestra que le vais a suministrar. Si va a ser de fila en fila, de dos en dos, de tres en tres o de 50 en 50. En caso de ser desconocido, podéis dejarlo por un espacio en blanco y será variable. Y la dimensión (input_dim, parametro de imput_shape) de cada dato en este caso sería nuestro 11.

	   Cuántas capas ocultas habrá que hacer? Será variable y vosotros mismos podéis experimentar. La vamos mantener sencilla.
		
	   Añadimos la segunda capa. Hay que especificar en general las capas de entrada y salida para que la red neuronal sepa como conectar los datos. La diferencia está que la primera que he creado, la red neuronal, no conocía nada acerca del parámetro input parámetro de entrada. Sin embargo, ahora si yo añado otra capa (segunda en este caso) a la red neuronal, la red neuronal ya sabe que viene de un espacio dimensional de tamaño seis, porque así he dejado preparado la capa anterior. Eso significa que la nueva capa oculta ya sabe lo que espera de la capa anterior. EL parametro input_dim no es necesario, porque la segunda capa oculta ya sabe que tiene que conectarse, que enganchar con la primera capa oculta y la primera capa oculta tiene 6 nodos. Elijo 6 porque es el promedio de nodos de la capa de entrada y la capa de salida. Se puede cambiar la cantidad de nodos, activacion y forma de iniciar los pesos. Dejamos todo igual a la capa primera. 
	   
	   Es bastante interesante para ser configurada el rectificador en las capas ocultas y sigmoide en la capa de salida.
	   
	   Vamos a definir solamente dos capas ocultas. 
	   
	   Finalmente, la capa de salida. No necesitamos el párametro input_dim a la funcion Dense y hay que indicar que solamente va haber un nodo de salida. Cambiamos la funcion de activacion por sigmoide (sigmoid) porque quiero conseguir probabilidades que un cliente se vaya del banco o no. Y esto servirá siempre y cuando queráis clasificar en dos categorías si se quiere clasificar en tres categorías. Por ejemplo, una persona activa, pasiva o neutra dentro del banco, realmente tendríamos que cambiar varias cosas. Lo primero es que en la capa de salida ya no habría una unidad, ya no sólo habría un nodo, sino que tendríamos exactamente el número de categorias. Por tanto, la variable dependiente tendría que tener más categorías, en este caso tres posibles categorías.  Evidentemente la función sigmoide aplicada a tres categorías no sería la más adecuada. Tal vez tendríamos que reemplazar por el escalón o otra vez por un rectificador lineal unitario para ver cuál de las tres es la que debe activarse. Podriamos usar una sigmoide con dos , tres o cuatro categorias pero probablemente se necesita una funcion softmax para que todas las probabilidades sumaran uno. Precisamente para volver a tener que la suma de todas las probabilidades posibles vuelva a dar uno. Un 80% de una categoría o un diez de la otra, un diez de la otra, para tres categorias. Usamos en el ejemplo la sigmoide porque tenemos dos categorias. 
	   
	   Luego, de construir la red la compilamos. Usamos la funcion classifier.compile(), parametros:
		
			optimizer = "adam"
			El algoritmo que se utiliza para encontrar el conjunto óptimo de soluciones. El conjunto óptimo de pesos dentro de la red neuronal. Eligo uno que me permita llegar al optimo de los pesos. Puede ser: El gradiente, El gradiente descendente estocastico o adam. "adam" es el recomendado.
			
			loss = "binary_crossentropy"
			Funcion de perdida, se corresponde dentro del gradiente descendente estocástico o no, como esa función que permitía ser minimizada, la que minimizaba el error entre la predicción real y el dato real. Entonces, elegir una función de pérdida u otra puede ser interesante para optimizar más o menos en función del algoritmo de optimización que hayáis elegido. Y algunos de los ejemplos más interesantes podrían ser la minimización de las diferencias de los cuadrados. Es esa suma de los cuadrados ordinarios. Es una de las funciones que podemos utilizar. DEbemos medir el error de algun modo.  Aqui usamos, binary_crossentropy que básicamente lo que hará será hacer la diferencia y aplicar un logaritmo al resultado para transformar las categorías directamente en números. 
			
			metrics = ["accuracy"])
			
			Lista de metricas a evaluar en nuestro modelo.  Luego de varias observaciones de muchas iteraciones, el algoritmo utiliza este criterio de precisión para mejorar el rendimiento global del modelo. Entonces, si yo le pido esa métrica directamente, el modelo la va a evaluar y va a ayudar a que durante la fase de entrenamiento la precisión intente aumentar de una iteración a la siguiente. Entonces ahí pasará que eligiendo la métrica de la precisión obtendremos unos resultados bastante interesantes. Puede ser una lista de metricas con corchetes. 
			
			Entrenamos la red con .fit(), parametros
							X_train
								variables independiente
							y_train
								variables dependientes
							batch_size
								tamaño del lote
								Se puede cambiar y experimentar. Se selecciono 10. Procesa 10 elementos y luego corrige los pesos y asi sucesivamente hasta terminar el set de datos. Es mas optimo que procesar un elemento y corregir los pesos. Y es mejor que procesar todo el dataset y corregir los pesos.
								
							epochs
								Numero de iteraciones globales - pasadas sobre todos los datos del data set.
								Por lo general, cuantas más veces pase la red neuronal sobre el conjunto de datos, más aprenderá sobre él. Hay que ir con cuidado porque basarnos en el número de veces significará muchas probabilidades de sufrir over, fitting o sobre ajuste y no dara un buen resultado. Es un momento para experimentar. Se puede cambiar.
	   
	    Obtenemos un 84% de efectividad con los datos actuales y la configuracion de la red neuronal.
		
		
----------------------------------
Redes neuronales convolucionales
----------------------------------

Rama de las redes neuronales. Trabajar con imagenes.


	- Que son las Redes neuronales convolucionales?
	
		Dotan al ordenador de procesamiento y clasificacion de imagenes. Geoffrey Hinton un de los padres de las redes neuronales. Yann LeCun padre de las redes convolucionales.
		
		La arquitectura de una red neuronal. Se tiene una capa de entrada, tiende a ser una imagen, pasa a través de la red neuronal. De momento, es una caja oscura, a la que le suministramos la imagen y me devuelve una etiqueta con la que clasificar a dicha imagen. Tenemos que entrenar la Red neuronal con imagenes etiquetadas.  Por ejemplo, le paso expresiones faciales y le digo: Mira, esto es una cara de una persona que está feliz. Esto es una persona feliz. La red neuronal convolucional intenta sacar rasgos de ahí dentro de una persona feliz. Otro ejemplo, esta imagen es una persona que está triste. Evidentemente tiene el ceño fruncido. La red neuronal convulcional, puede detectar esas emociones, puede detectar rasgos dentro de una imagen, mirando por ejemplo la forma de la boca. Evidentemente, no nos va a decir, cuando le suministra una nueva imagen, si es feliz o triste con un 100%, sino que me da cierta probabilidad, un 80 o 90 % de probabilidad. Lo cual es bueno. A base de entender las características de las imágenes, podrá llegar a hacer este tipo de clasificaciones.
		
		Como lo puede hacer? Cómo puede reconocer esas características sin que nosotros le digamos nada? Pues todo comienza en un nivel básico muy sencillo, dependiendo de si tengamos una imagen en color o en blanco y negro. Básicamente imagina que tenemos una imagen pequeña de dos píxeles por dos píxeles. Lo que hace la red neuronal es aprovechar el hecho de que cada píxel viene dado por un número o bien una colección de números. De modo que, a pesar de que yo tenga una imagen de cuatro píxeles en blanco y negro, el ordenador lo transforma, lo mapea a un array bidimensional organizado en filas y columnas, donde en cada una de las posiciones aparece el valor de la escala de grises. En este caso, por forma simple, los pinto en filas y columnas de valores de cuatro píxeles con cantidad de información entre cero y 255. En ocho bits se almacena un número de ocho bits para cada una de las posiciones, indicando la intensidad de color cero, totalmente negro y blanco completamente a uno. Bueno, pues esa sería la forma en la que se traduce la imagen a una estructura que el ordenador es capaz de analizar y entender. Este es el punto de partida de cualquier imagen, como se digitaliza una imagen o cómo se transforma a ordenadores.  Con los colores no es del todo asi, porque para cada color necesitamos información de cada uno de los canales de color, de modo que en este caso se transforma a tres arrays bidimensionales y en este caso se almacena la cantidad de rojo, verde y azul, respectivamente, de cada píxel. Tambien, en un numero de ocho bits. De modo que tenemos tres arrays bidimensionales colocados uno encima de otro, o, lo que es lo mismo, un array tridimensional. Cada uno de esos tres colores tendrá su propia intensidad. Entonces, cada píxel tiene tres valores asignados cantidad de rojo, cantidad de verde y cantidad de azul, cantidades que se mueven siempre entre cero y 255, o bien si se escalan valores entre cero y uno respectivamente. Entonces, en base de la cantidad de rojo, de verde y de azul que tiene cada píxel, nos da el valor final, la luminosidad final de cada uno de los píxeles de una imagen. A base de combinar los valores de ese array, tenemos la representacion de la imagen. 
		
		Echemos un vistazo a un ejemplo muy trivial de una cara sonriente. Una vez que tenemos esta carita sonriente en blanco y negro para suministrarle al ordenador, el ordenador lo que hace es pasarlo a escala de grises. Simplemente lo transforma a una matriz bidimensional donde cada uno de los píxeles está pintado de un color u otro. Técnicamente, aquí cada píxel podría tener un valor entre cero y 255. Para simplificar mejor las cosas y entender mejor los conceptos, supongamos que el cero representa el blanco y el uno representa el negro respectivamente. Entonces, simplificando, podéis ver respectivamente que ahora tendríamos una matriz donde en cada posición tendríamos un número.Esos números,  corresponderían al rango de valores entre cero y 255. Matriz ya forma la parte de la entrada de nuestro algoritmo y los pasos que tendremos que seguir para entrenar nuestra red. Los pasos son:
		
				1.- Convolucion
				2.- Max Pooling
				3.- Flattening
				4.- Full Conection.
				
				
		Ver paper de tectnicas para reconocer imagenes:
				Gradient-Based learning applid to Document Recognition (1998) Yann Lecun
				http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf
				
				
	- Convolucion:
	
		Es una operacion matematica. Una convolución es una integral. Buscar luego la formula. Por tanto, la convolución es como una operación que, dada una función, está modificada a la otra. Donde se requiere procesamiento de señales uno se enfrenta a la operacion de convolucion.  Ver paper: Introduction to Convolutional Neural networks. Jianxin Wu (2017)
		Para ello, aquí tenéis un ejemplo vale, en términos intuitivos, tenéis una imagen cualquiera de entrada, suponemos que es en escala de grises y de hecho sólo con blanco y negro. La imagen ya esta representada en un vector 2D con unos y ceros por simplicidad. A partir de esa imagen de entrada tenemos un detector de rasgos o un detector de características.
		Esto es una matriz más pequeña. Podes usar dectectores de rasgos mas grande 7 x 7 en lugar de 3 x3 como en el ejemplo. Usamos 3 x 3 por simplicidad y para seguir el ejemplo. A los detectores de rasgos se le llaman nucleos de convolucion o filtros.  Entonces nosotros iremos viendo que algunos filtros tienen nombres y que algunos de ellos permiten detectar los bordes, difuminar una imagen o conseguir detectar ciertos rasgos en el interior de una imagen. Cuando juntamos la imgen de entrda (con representacion de array) con la operacion cruz con el detector de rasgos. Lo que sucede a nivel intuitivo es que se promedian la imagen grande con el filtro o el detector de características. Y esta operación precisamente es la convolución. Lo que hacemos es tomar el filtro de características, ponerlo arriba a la izquierda de la imagen y respetar que cada una de las celdas del detector de rasgos coincide con alguna celda de la imagen de entrada
		Hacemos que en la posición uno o dos o de la imagen coincida con el uno o dos del detector de rasgos y así sucesivamente.
		Lo que conseguimos es que los nueve primeros píxeles de la esquina superior izquierda de la imagen correspondan con las 9 posiciones respectivas del detector de rasgos (3 x 3). Pues una vez que hemos colocado el detector de rasgos encima de la imagen de entrada, de entrada lo que hacemos es multiplicar de forma inteligente cada posición de la imagen con cada posición del detector de rasgos, y habiendo multiplicado los nueve números, sumamos los resultados. En este caso, fijaros que tendríamos que hacer la operación de cero por 0 nada todavía. Cero por uno nada bajo la segunda fila sería cero por uno. Nada uno por 0 nada. Cero por uno nada. Y cero por uno tampoco nada.  De acuerdo, entonces fijaos que cuando yo multiplico cada una de las posiciones de la imagen por el detector de rasgos, todos son ceros y cuando da cero, por ejemplo, cuando muevo el detector de rasgo a la siguiente columna, obtengo un uno. Todos estos resultados de se agregan a la matriz de caracteristica resultante. Entonces el filtro es capaz de medir y ponderar, por así decir, sumar los píxeles de las posiciones donde no hay ceros. Fijaros, hay un dos cosa interesante en el centro izquierda y abajo a la derecha tenemos dos, uno más uno es dos.
		El resultado es la matriz de características y lo que me permite es obtener una versión simplificada de la información de la imagen original despues de aplicar el filtro. Resumen la iagen original y reduce el tamaño de la imagen original a 5 x 5, por tener un filtro de 3 x 3. Filtros mas grandes no dan mapa de caracteristicas mas pequeños.  El filtro de la imagen reduce el tamaño y extrae los rasgos dados por la forma que tiene el detector de rasgos. Evidentemente nosotros no aplicaremos este filtro a imágenes 7X7. Si os imagináis una imagen de 256 píxeles por 256 píxeles, pues un filtro de tamaño de tres por tres solo nos bajaría 254 por 254. Entonces ahí cobra sentido tener detectores de rasgos de tamaño diez por diez o 11 por 11. Mejor es que sea impar. Entonces, para imágenes grandes cobra sentido tener detectores de rasgos bastante más grandes. La pregunta es, al reducir informacion o pixeles, no pierdo informacion?
		Si se pierde porque la matris es mas pequeña y tengo menos valores. El objetivo del filtro o detector de caracteristicas es detectar ciertas formas o patrones de la imagen global. Se extrae las caracteristicas interesantes para dicho filtro. No nos importa el valor de cada píxel por separado, sino que lo que me importa en una imagen es el valor ponderado de todos los vecinos de un determinado píxel. Si tenemos que distinguir entre un tren bala y un tren normal, probablemente tenga que buscar mapas de características que me ayuden a preservar la forma de uno o de otro, respectivamente. Y esto es lo que nos permite sacar no sólo un mapa de características como este que tengo aquí, sino
		muchos mapas de características, los unos y tras otros, respectivamente, eligiendo diferentes detectores de rasgos. Simplifica la informacion. De modo que si aplicamos muchos mapas de características diferentes, podemos tener versiones evolucionadas de la imagen original a través de usar diferentes filtros y toda una colección de mapa de características.
	    Esta es la capa de convolucion. Eso es lo que se conoce con el nombre de la capa de convolución, donde evidentemente cada filtro aplicado a la imagen buscará ciertas características diferentes y básicamente la red neuronal se encargará de
		utilizarlas para poder hacer una operación que las junte todas ellas en la siguiente capa. Entonces algunas buscarán formas de bigotes, otras formas de orejas, otras formas de bocas, otras diferentes tipos de forma, algunos filtros que permitan pasar ciertos ciertas características, ciertos colores difuminados de la imagen, las manchas que tiene un guepardo. Cada uno de estos mapas de características que veis aquí en la pantalla sería como una forma de detectar,
		de filtrar partes de una imagen. El propósito del filtro que usamos en el ejemplo es tal vez buscar el inicio de la comisura de una boca. En el caso, cuando vamos juntando más y más y más y más filtros, es que cada uno de ellos se encargue de detectar características diferentes de nuestra imagen. Y esto que nos permite, nos permite tener un resumen general de todas las características que se pueden encontrar presentes dentro de una imagen. Y esto es bastante interesante porque en función del la del tipo de detector de rasgos que utilicéis, la imagen filtrada dará lugar a un mapa de características u otro.

		https://docs.gimp.org/
		https://www.gimp.org/downloads/
		https://docs.gimp.org/2.8/es/plug-in-convmatrix.html
		
		Gimp. Ver filtros de convolucion. Aqui, alugnos:

		Filtro Serpent. Es el filtro que permite afinar, por así decir, los rasgos, los contornos, que permite agudizar la presencia de objetos en la imagen. En este caso es bastante intuitivo. Fijaros lo que hace la convolución toma el valor central del píxel central en esta matriz cinco por cinco, lo multiplica por cinco y le resta cada uno de los cuatro vecinos al norte, sur, este y oeste del píxel central. Al final es como hacer una media de multiplicar tu píxel por cinco y restarle los píxeles vecinos, de modo que y mejoro la resolución del píxel central eliminando el ruido o la perturbación que tiene con los vecinos cercanos.
		
		Blur. Eso lo hace el filtro de blur o de difuminado, el cual lo que hace es promediar todos los pixeles vecinos a un pixel dado. Al hacer una una media de los números podéis ver que se obtiene un borde borroso.

		Entonces fijaos que utilizamos diferentes filtros para sacar diferentes versiones de la misma imagen. Y al final, en términos de redes de convolución, cada uno de estos filtros permitiría extraer información (formas) que la red neuronal sea capaz de buscar. Entonces fijaros que al final la idea es que la red neuronal en función del mapa de características
		que aplique, se va a encargar de detectar o explicar una parte u otra de la imagen.
		Ejemplo,
		
		Mi imagen tiene muchas manchas circulares, probablemente sea gepardo. Tiene líneas alargadas en forma de bigote, tal vez sea un gato o tiene orejas caídas, tl vez sea un perro.
		
		Entonces fijaos que al final nosotros en la red neuronal tendremos una ventaja,no le tendremos que decir exactamente qué filtro tiene que utilizar, sino que la propia red neuronal irá ajustando los valores de los pesos que tiene que tener el mapa de características, formas y valores del interior para ser capaces de detectar lo que realmente considere interesante.
		
		Convolucion. Encontrar las características de una imagen utilizando el detector de características. Un filtro para, en el futuro tener toda una colección de mapas de características que podamos pasar a la siguiente fase, habiendo localizado en cada uno de ellos una serie de patrones. Hemos descompuesto la imagen, la hemos dejado repartida, promediado con diferentes filtros y ahora tendremos una serie de características que poder analizar y combinar.
		
		
	- Capa ReLU (capa rectificadora lineal unitaria.)
	
		Una parte importante no es que sea un paso separado, sino que es un paso adicional que se aplica justo al final de la capa de convolución. En la clase anterior había dejado la imagen como esto que veis aquí. Nosotros habíamos empezado con una imagen, habíamos aplicado diferentes filtros de convolución y habíamos obtenido toda una colección de mapas de características en lo que hemos llamado la capa de convolución.
		
		A continuación, lo que voy a hacer es aplicar una función rectificadora lineal unitaria que ya conoceréis de la sección anterior. La funcion es el máximo entre x y cero y normalmente veréis que en los libros los autores tienden a
		separar la capa rectificadora lineal unitaria de la capa de convolución, como si fueran dos pasos totalmente separados.
		Yo tiendo a no separarlos porque casi siempre que hacéis una convolucion va pegada automáticamente una capa de rectificación lineal unitaria.
		
		Y la razón por la que estamos aplicando el rectificador lineal unitario es porque nosotros tendemos a aplicar un filtro y después de aplicar ese filtro hemos obtenido un resumen de nuestra imagen. Pero el filtro al final no es más que una operación lineal, una operación lineal de sumas y multiplicaciones, cuando en la vida real no todo es lineal.
		
		La razón de aplicar el rectificador lineal unitario es separar, romper, aumentar todo lo que no sea la linealidad dentro de nuestra red neuronal. Porque en las imágenes, los rasgos que aparecen en ellas son altamente no lineales, especialmente si queremos reconocer objetos diferentes pero cercanos al lado el uno del otro y separar entre el objeto
		y el fondo. Muchas veces la imagen va a tener muchos contrastes, muchos bordes, muchos límites, muchos elementos no lineales, y la transición entre los pixeles adyacentes en general, tiende a ser una operación no lineal. Es por eso que para separar, hacer divergentes las fronteras entre elementos cercanos dentro de la imagen, podemos aplicar ese rectificador lineal unitario. Y el problema es ese que la operación matemática de la convolución, como os la he enseñado y le hemos ejecutado para calcular los mapas de características que os he enseñado en la clase anterior, corremos el riesgo de como usamos una operación lineal, que el resultado sea algo lineal. Cuando a mi lo que me interesa es separar, romper esa linealidad y acentuar más los detalles que no son lineales.
		
		Al aplicar el filtro de detección de características a esa imagen, obtendríamos en general algo similar a esto.
		Fijaros que es una imagen emborronada, una imagen difuminada, y se puede observar que hay elementos en blanco, elementos en negro y que las fronteras han perdido un poco la razón de ser, al aplicar un detector de características a una imagen que no necesariamente solo tenga ceros y unos.  Puede ocurrir que los valores entre medios tampoco tengan ceros y unos pueden tener valores positivos, incluso pueden salir valores negativos en función del filtro que utilicéis. A veces saldrán valores negativos y esos son los negros excesivos de píxeles dentro del interior de la imagen. El rectificador lineal unitario lo que hace es eliminar todos los valores por debajo de cero, por debajo del negro y transformarlos automáticamente en un color negro. A la imagen filtrada y luego pasada por el filtro rectificador lineal unitario, hemos eliminado la linealidad. Solo quedan valors positivos en los pixels. La imagen resultante, luego de aplicar ReLU, resalta las formas de las figuras en la imagen.
			
		Ver:
			Understanding Redes Neurolales de Convolucion with a mathematical model. Jay Kuo (2016)
			Delving Deep into Rectifiers: Surpassing Human-level Performanceon ImageNet. Kaiming He et al. (2015)
			
			
	- Max Pooling.
	
		
